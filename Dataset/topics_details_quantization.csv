id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
57360,About the quantization category,2019-10-02T21:54:28.644Z,0,318,"<div class=""post"" itemprop=""articleBody""><NewLine><p>This category is for questions, discussion and issues related to PyTorch’s quantization feature.</p><NewLine><p>For more information, see: <a href=""https://github.com/pytorch/pytorch/issues/18318"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/18318</a></p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"October 2, 2019,  9:55pm",,,,,
96590,Is there a way to quantize conv_transpose2d layer?,2020-09-17T07:32:45.932Z,4,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Following is my error message:</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “pose_estimation/test_on_single_image_quant_ver.py”, line 119, in <br/><NewLine>main()<br/><NewLine>File “pose_estimation/test_on_single_image_quant_ver.py”, line 92, in main<br/><NewLine>output = quantized_model(input)<br/><NewLine>File “/usr/lib/python3.8/site-packages/torch/nn/modules/module.py”, line 722, in _call_impl<br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>RuntimeError: The following operation failed in the TorchScript interpreter.<br/><NewLine>Traceback of TorchScript, serialized code (most recent call last):<br/><NewLine>File “code/<strong>torch</strong>/models/pose_mobilenet.py”, line 17, in forward<br/><NewLine>x0 = (self.quant).forward(x, )<br/><NewLine>x1 = (self.features).forward(x0, )<br/><NewLine>x2 = (self.conv_transpose_layers).forward(x1, )<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>x3 = (self.final_layer).forward(x2, )<br/><NewLine>return (self.dequant).forward(x3, )<br/><NewLine>File “code/<strong>torch</strong>/torch/nn/modules/container/___torch_mangle_4.py”, line 26, in forward<br/><NewLine>_8 = getattr(self, “8”)<br/><NewLine>input0 = (_0).forward(input, None, )<br/><NewLine>input1 = (_1).forward(input0, )<br/><NewLine>~~~~~~~~~~~ &lt;— HERE<br/><NewLine>input2 = (_2).forward(input1, )<br/><NewLine>input3 = (_3).forward(input2, None, )<br/><NewLine>File “code/<strong>torch</strong>/torch/nn/modules/container/___torch_mangle_4.py”, line 25, in forward<br/><NewLine>_7 = getattr(self, “7”)<br/><NewLine>_8 = getattr(self, “8”)<br/><NewLine>input0 = (_0).forward(input, None, )<br/><NewLine>~~~~~~~~~~~ &lt;— HERE<br/><NewLine>input1 = (_1).forward(input0, )<br/><NewLine>input2 = (_2).forward(input1, )<br/><NewLine>File “code/<strong>torch</strong>/torch/nn/modules/conv.py”, line 22, in forward<br/><NewLine>output_size: Optional[List[int]]=None) -&gt; Tensor:<br/><NewLine>output_padding = (self)._output_padding(input, output_size, [2, 2], [1, 1], [4, 4], )<br/><NewLine>_0 = torch.conv_transpose2d(input, self.weight, self.bias, [2, 2], [1, 1], output_padding, 1, [1, 1])<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>return _0<br/><NewLine>def _output_padding(self: <strong>torch</strong>.torch.nn.modules.conv.ConvTranspose2d,</p><NewLine><p>Traceback of TorchScript, original code (most recent call last):<br/><NewLine>File “/usr/lib/python3.8/site-packages/torch/nn/modules/container.py”, line 117, in forward<br/><NewLine>def forward(self, input):<br/><NewLine>for module in self:<br/><NewLine>input = module(input)<br/><NewLine>~~~~~~ &lt;— HERE<br/><NewLine>return input<br/><NewLine>File “/usr/lib/python3.8/site-packages/torch/nn/modules/container.py”, line 117, in forward<br/><NewLine>def forward(self, input):<br/><NewLine>for module in self:<br/><NewLine>input = module(input)<br/><NewLine>~~~~~~ &lt;— HERE<br/><NewLine>return input<br/><NewLine>File “/usr/lib/python3.8/site-packages/torch/nn/modules/conv.py”, line 905, in forward<br/><NewLine>output_padding = self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size)</p><NewLine><pre><code>    return F.conv_transpose2d(<NewLine>           ~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        input, self.weight, self.bias, self.stride, self.padding,<NewLine>        output_padding, self.groups, self.dilation)<NewLine></code></pre><NewLine><p>RuntimeError: Could not run ‘aten::slow_conv_transpose2d’ with arguments from the ‘QuantizedCPU’ backend. ‘aten::slow_conv_transpose2d’ is only available for these backends: [CPU, CUDA, Autograd, Profiler, Tracer].</p><NewLine></blockquote><NewLine><p>I’m new to this, I try to find something like torch.nn.quantized.conv_transpose2d but I can’t find it or is there any other ways?<br/><NewLine>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/ruka,,ruka,"September 17, 2020,  8:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/ruka"">@ruka</a>, we landed support for quantized conv transpose recently (<a href=""https://github.com/pytorch/pytorch/pull/40371"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/40371</a> and the preceding PRs).  It is not in v1.6, but you can try it out in the nightly!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much! I will try it. <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/> <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a>   I updated my pytorch to nightly(Version: 1.7.0a0+60665ac).<br/><NewLine>But when I try to convert my model, I get error:</p><NewLine><blockquote><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “pose_estimation/quantized.py”, line 67, in <br/><NewLine>main()<br/><NewLine>File “pose_estimation/quantized.py”, line 61, in main<br/><NewLine>torch.quantization.convert(model, inplace = True)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/quantization/quantize.py”, line 414, in convert<br/><NewLine>_convert(module, mapping, inplace=True)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/quantization/quantize.py”, line 458, in _convert<br/><NewLine>_convert(mod, mapping, inplace=True)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/quantization/quantize.py”, line 459, in _convert<br/><NewLine>reassign[name] = swap_module(mod, mapping)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/quantization/quantize.py”, line 485, in swap_module<br/><NewLine>new_mod = mapping[type(mod)].from_float(mod)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py”, line 507, in from_float<br/><NewLine>qconv = cls(mod.in_channels, mod.out_channels, mod.kernel_size,<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py”, line 641, in <strong>init</strong><br/><NewLine>super(ConvTranspose2d, self).<strong>init</strong>(<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py”, line 476, in <strong>init</strong><br/><NewLine>super(_ConvTransposeNd, self).<strong>init</strong>(<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py”, line 53, in <strong>init</strong><br/><NewLine>self.set_weight_bias(qweight, bias_float)<br/><NewLine>File “/home/yjwen/local/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py”, line 650, in set_weight_bias<br/><NewLine>self._packed_params = torch.ops.quantized.conv_transpose2d_prepack(<br/><NewLine>RuntimeError: FBGEMM doesn’t support transpose packing yet!</p><NewLine></blockquote><NewLine><p>Did I miss anything(maybe some special that I need to do before quantized conv transpose) or this is a bug?<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Currently, the ConvTranspose is only supported using the QNNPACK. The FBGEMM version is planned, but there is no specific date specified for it. Meanwhile, you have two options for the eager mode: replace the ConvTranspose: 1) Replace the instances of the ConvTranspose with <code>dequant-&gt;ConvTranspose-&gt;quant</code> construct 2) Set the <code>torch.backends.quantized.engine = 'qnnpack'</code> before running your model. You also might need to set the <code>qconfig = torch.quantization.get_default_qconfig('qnnpack')</code> or <code>qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')</code></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zafar"">@Zafar</a>  Thank you so much for the reply!!  I successfully converted the model by setting <code>quantized.engine = 'qnnpack'</code>  and <code>get_default_qconfig('qnnpack')</code><br/><NewLine>But the quantized model predicts a totally wrong result(my original model works fine)<br/><NewLine>I carefully studied the official tutorial <a class=""inline-onebox-loading"" href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture</a><br/><NewLine>It seems that nothing special needs to be done when using a quantized model, just the usual way</p><NewLine><pre><code class=""lang-auto"">model.eval()<NewLine>with torch.no_grad():<NewLine>    output = model(image)<NewLine></code></pre><NewLine><p>Any hints?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you elaborate on the wrong result, please – I wonder if it is within the quantization error.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ruka; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ruka; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/ruka; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: September 18, 2020,  4:19pm; <NewLine> REPLY_DATE 2: September 21, 2020,  1:42am; <NewLine> REPLY_DATE 3: September 25, 2020,  8:25am; <NewLine> REPLY_DATE 4: September 25, 2020,  6:43pm; <NewLine> REPLY_DATE 5: September 27, 2020,  8:54am; <NewLine> REPLY_DATE 6: September 28, 2020,  6:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95589,Is there any way I could get bit value of my weight,2020-09-08T17:50:00.181Z,1,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I quantize my model and I want the bit value of all weights in my model. But I don’t how to do it</p><NewLine></div>",https://discuss.pytorch.org/u/sunyoung,(sunyoung),sunyoung,"September 8, 2020,  5:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you elaborate on what you mean by bit value of the weights?<br/><NewLine>If you would like to access quantized weights you can try &lt;quantized_module&gt;.weight(). This returns a quantized tensor of weights and you can use int_repr() to get the int8_t values of weights.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.<br/><NewLine>Based on your advice, when I try to print the weight of the quantized model, I get FP32 weights, scaling, and zero_point, but I can’t get the int weight?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111179; <NewLine> ,"REPLY_DATE 1: September 9, 2020,  7:17pm; <NewLine> REPLY_DATE 2: September 26, 2020,  7:53am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
97459,Best practice or suggestion for QAT?,2020-09-25T07:39:51.315Z,1,35,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am curious about <code>disable_observer</code> and <code>freeze_bn_stats</code> in quantization aware training. I don’t know when should I apply them. I have tried different combinations of two parameters. It seems that has a big impact on accuracy. Is there any best practice for quantization aware training? Like should I disable observer first and when should I disable it, train from scratch or fine-tune a trained model?</p><NewLine></div>",https://discuss.pytorch.org/u/eleflea,(Eleflea),eleflea,"September 25, 2020,  7:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/eleflea"">@eleflea</a>, check out <a href=""https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py</a> for one example.  One approach which has proven to work well is:</p><NewLine><ul><NewLine><li>start QAT training from a floating point pre-trained model and with observers and fake_quant enabled</li><NewLine><li>after a couple of epochs, freeze the BN stats if your network has any BNs (epoch == 3 in the example)</li><NewLine><li>after a couple of epochs, disable observers (epoch == 4 in the example)</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I’ll try it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eleflea; <NewLine> ,"REPLY_DATE 1: September 25, 2020,  3:45pm; <NewLine> REPLY_DATE 2: September 26, 2020,  1:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
97177,PyTorch quantization resnet50 model,2020-09-22T22:03:30.509Z,1,48,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I am trying the resnet50 model quantization with PyTorch and I tried these 3 lines of code :<br/><NewLine>the import, model=qn.resnet50(pretrain=true), and model.state_dict()), and why the coefficients being shown are all float values if this is the quarantined version of the model?<br/><NewLine>Noticed this while trying to figure out how to save/load the coefficients for a quantized model, and is there anything special you need to do convert model coefficients between float and int8.</p><NewLine><p>Please let me know. Appreciate any help/suggestions , Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/samhithaaaa,(samhitha mamindla),samhithaaaa,"September 22, 2020, 10:03pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone help me with this please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>check out <a href=""https://pytorch.org/docs/stable/quantization.html#quantized-torch-tensor-operations"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#quantized-torch-tensor-operations</a>, in partucular the “int_repr” function.  By default, if you print out a quantized tensor you will see the dequantized values that the tensor represents.  To see the raw int8 values, you can use <code>x.int_repr()</code>.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you so much for your response,I will check it.Thanks again.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/samhithaaaa; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/samhithaaaa; <NewLine> ,"REPLY_DATE 1: September 23, 2020,  3:20am; <NewLine> REPLY_DATE 2: September 23, 2020,  5:54pm; <NewLine> REPLY_DATE 3: September 24, 2020,  5:16pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
97325,How to extract the quantized weight of quantized NN model,2020-09-24T04:29:46.744Z,0,26,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using Post Training Quantization and try to extract the quantized weight for inference phase, but I failed.<br/><NewLine>I try to directly use</p><NewLine><p>for weight in quantized_model.state_dict():<br/><NewLine>np.set_printoptions(suppress=True)<br/><NewLine>print(weight, “\n”, quantized_model.state_dict()[weight].detach().cpu().clone().numpy())</p><NewLine><p>get “TypeError: NumPy conversion for QuantizedCPUQInt8Type is not supported”</p><NewLine><p>Could give me any advice for extracting the quantized weight from the quantized model?<br/><NewLine>Thank you very much!!</p><NewLine></div>",https://discuss.pytorch.org/u/111179,(mmmm),111179,"September 24, 2020,  4:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>check out <a href=""https://pytorch.org/docs/stable/quantization.html#quantized-torch-tensor-operations"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#quantized-torch-tensor-operations</a>.  Some options:</p><NewLine><ol><NewLine><li>convert your quantized tensor to floating point with <code>x.dequantize()</code><NewLine></li><NewLine><li>get the raw integer values with <code>x.int_repr()</code>, this should be used together with <code>x.q_scale()</code> and <code>x.q_zero_point</code>.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: September 24, 2020,  3:53pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
97368,How pytorch simulates bias during quantization aware training,2020-09-24T12:57:47.119Z,0,24,"<div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that pytorch qat doesn’t simulate bias quantization error during qat. And I found that <code>qat.Conv2d</code> only fake-quantize <code>weight</code> and <code>activation</code>. So pytorch’s quantization strategy does not quantize the <code>bias</code>, right?</p><NewLine></div>",https://discuss.pytorch.org/u/Jonson,(叶俊贤),Jonson,"September 24, 2020, 12:57pm",,,,,
97016,Qnnpack vs. fbgemm,2020-09-21T15:07:04.265Z,3,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!<br/><NewLine>I am trying to implement quantization in my model.<br/><NewLine>In the case of Post Static Quantization some interesting detail came across:</p><NewLine><pre><code class=""lang-auto"">quantized_model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine># torch.backends.quantized.engine = 'qnnpack' # gives error<NewLine></code></pre><NewLine><p>works nearly perfect according to performance numbers. However, <code>qnnpack</code> is not available as an engine on my machine.</p><NewLine><p>Trying to use</p><NewLine><pre><code class=""lang-auto"">quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine></code></pre><NewLine><p>led to much worser performance numbers.</p><NewLine><p>Also, in my opinion this should not work, but does perform very good:</p><NewLine><pre><code class=""lang-auto"">quantized_model.qconfig = torch.quantization.get_default_qconfig('qnnpack') <NewLine>torch.backends.quantized.engine = 'fbgemm'<NewLine></code></pre><NewLine><p>Is this a bug? Shouldn’t <code>fbgemm</code> outperform <code>qnnpack</code> an a x86 system?</p><NewLine></div>",https://discuss.pytorch.org/u/pintonos,(Pintonos),pintonos,"September 21, 2020,  3:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97016"" data-username=""pintonos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pintonos/40/28210_2.png"" width=""20""/> pintonos:</div><NewLine><blockquote><NewLine><p>Shouldn’t <code>fbgemm</code> outperform <code>qnnpack</code> an a x86 system?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, that would be expected.  Does your system have AVX and AVX2 capabilities?  Those are needed for the fast paths of the fbgemm kernels.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""97016"" data-username=""pintonos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pintonos/40/28210_2.png"" width=""20""/> pintonos:</div><NewLine><blockquote><NewLine><p>Is this a bug? Shouldn’t <code>fbgemm</code> outperform <code>qnnpack</code> an a x86 system?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, sounds like it could be a bug.  Would you be able to share the per-op profiling results for the model you are seeing this for using <a href=""https://pytorch.org/docs/stable/autograd.html#profiler"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/autograd.html#profiler</a> on both fbgemm and qnnpack on your machine?  Qnnpack only has fast kernels on ARM, on x86 it is taking the slow fallback path.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Profile for <code>fbgemm</code> for evaluation:</p><NewLine><pre><code class=""lang-auto"">------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>mul                                   64.88%           3.584s           65.20%           3.602s           13.341ms         270              <NewLine>sum                                   13.79%           761.666ms        15.01%           829.509ms        1.097ms          756              <NewLine>quantized::linear                     12.68%           700.596ms        12.68%           700.596ms        19.461ms         36               <NewLine>_cat                                  3.06%            168.962ms        3.14%            173.683ms        6.433ms          27               <NewLine>relu                                  1.32%            73.125ms         1.34%            73.805ms         2.734ms          27               <NewLine>fill_                                 1.17%            64.873ms         1.17%            64.876ms         82.855us         783              <NewLine>index_select                          0.73%            40.152ms         1.22%            67.359ms         95.953us         702              <NewLine>copy_                                 0.42%            23.189ms         0.42%            23.197ms         44.438us         522              <NewLine>empty                                 0.39%            21.815ms         0.39%            21.815ms         9.696us          2250             <NewLine>quantize_per_tensor                   0.38%            20.759ms         0.38%            20.771ms         2.308ms          9                <NewLine>cat                                   0.16%            9.051ms          3.31%            182.734ms        6.768ms          27               <NewLine>embedding                             0.15%            8.441ms          2.80%            154.721ms        110.200us        1404  <NewLine>...<NewLine></code></pre><NewLine><p>Metrics:</p><NewLine><pre><code class=""lang-auto"">Size (MB): 3.466263<NewLine>Loss: 1.093 (not good)<NewLine>Acc: 0.622<NewLine>Elapsed time (seconds): 7.084<NewLine>Avg execution time per forward(ms): 0.00363<NewLine></code></pre><NewLine><p>Profile for <code>qnnpack</code> for evaluation:</p><NewLine><pre><code class=""lang-auto"">------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>Name                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  <NewLine>------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  <NewLine>mul                                   66.18%           3.379s           66.49%           3.395s           12.573ms         270              <NewLine>sum                                   12.98%           662.933ms        14.21%           725.287ms        959.374us        756              <NewLine>quantized::linear                     12.45%           635.799ms        12.45%           635.799ms        17.661ms         36               <NewLine>_cat                                  3.14%            160.059ms        3.23%            164.724ms        6.101ms          27               <NewLine>relu                                  1.33%            67.692ms         1.34%            68.278ms         2.529ms          27               <NewLine>fill_                                 1.17%            59.914ms         1.17%            59.917ms         76.522us         783              <NewLine>index_select                          0.68%            34.661ms         1.11%            56.808ms         80.923us         702              <NewLine>empty                                 0.38%            19.191ms         0.38%            19.191ms         8.529us          2250             <NewLine>quantize_per_tensor                   0.37%            18.920ms         0.37%            18.930ms         2.103ms          9                <NewLine>copy_                                 0.35%            17.947ms         0.35%            17.954ms         34.394us         522              <NewLine>embedding                             0.14%            7.034ms          2.52%            128.492ms        91.519us         1404             <NewLine>...<NewLine></code></pre><NewLine><p>Metrics:</p><NewLine><pre><code class=""lang-auto"">Size (MB): 3.443591<NewLine>Loss: 0.580 (very good)<NewLine>Acc: 0.720<NewLine>Elapsed time (seconds): 6.978<NewLine>Avg execution time per forward(ms): 0.00427<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hmm, one hypothesis that would fit this data is that fbgemm is not enabled, and both fbgemm and qnnpack are taking the fallback paths.</p><NewLine><p>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> , any tips?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/pintonos; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: September 22, 2020,  3:21pm; <NewLine> REPLY_DATE 2: September 23, 2020,  3:23am; <NewLine> REPLY_DATE 3: September 23, 2020,  6:42am; <NewLine> REPLY_DATE 4: September 23, 2020,  4:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
97170,PyTorch Quantization,2020-09-22T20:58:17.189Z,0,25,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,<br/><NewLine>I am trying the resnet50 model quantization with PyTorch and I tried these 3 lines of code :<br/><NewLine>the import, model=qn.resnet50(pretrain=true), and model.state_dict()), and why the coefficients being shown are all float values if this is the quarantined version of the model?<br/><NewLine>Noticed this while trying to figure out how to save/load the coefficients for a quantized model, and is there anything special you need to do convert model coefficients between float and int8.</p><NewLine><p>Please let me know. Appreciate any help/suggestions , Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/samhithaaaa,(samhitha mamindla),samhithaaaa,"September 22, 2020,  8:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You need to set quantized=True when you load the model. i.e do model =qn.resnet(pretrained=True, quantized=True)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: September 28, 2020,  5:37pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96488,How does pytorch determine which backend a function supports,2020-09-16T11:31:08.960Z,1,50,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I wrote a c++ function that receives a quantized tensor and outputs a fp32 tensor. everything compiled without errors and the python binding also works but when i try to use the function i get the following error:</p><NewLine><p><code>RuntimeError: Could not run 'quantized::linear_my' with arguments from the 'QuantizedCPU' backend. 'quantized::linear_my' is only available for these backends: [].</code></p><NewLine><p>so my question is how does pytorch determine which backends does my function support?<br/><NewLine>how can i fix this state where my function doesnt support any backend?</p><NewLine><p>Thanks,<br/><NewLine>Ofir</p><NewLine></div>",https://discuss.pytorch.org/u/Ofir_Zafrir,,Ofir_Zafrir,"September 16, 2020, 11:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using <a href=""https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html"" rel=""nofollow noopener"">TorchBind</a> rather than PyBind11?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am not sure, I used the same method that was used in the torch library, I modified code inside the library.<br/><NewLine>I resolved the problem in the end, it was a compilation problem, cleaning the project resolved it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ofir_Zafrir; <NewLine> ,"REPLY_DATE 1: September 16, 2020,  1:00pm; <NewLine> REPLY_DATE 2: September 21, 2020, 10:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85902,How to avoid Quantization warning: &ldquo;Must run observer before calling calculate_qparams.&rdquo;?,2020-06-18T10:56:23.331Z,5,333,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I worked with the pytorch tutorial for <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">static quantization</a> and when running the line:</p><NewLine><pre><code class=""lang-auto"">torch.quantization.convert(per_channel_quantized_model, inplace=True)<NewLine></code></pre><NewLine><p>I receive the following warning:</p><NewLine><pre><code class=""lang-auto"">.../torch/quantization/observer.py:845: <NewLine>UserWarning: must run observer before calling calculate_qparams. Returning default scale and zero point <NewLine></code></pre><NewLine><p>I call the <code>convert</code> function within the following lines of code:</p><NewLine><pre><code class=""lang-auto"">per_channel_quantized_model = load_model(..)<NewLine>per_channel_quantized_model.eval()<NewLine>per_channel_quantized_model.fuse_model()<NewLine>per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>print(per_channel_quantized_model.qconfig)<NewLine>torch.quantization.prepare(per_channel_quantized_model, inplace=True)<NewLine>evaluate(per_channel_quantized_model, ...)<NewLine>torch.quantization.convert(per_channel_quantized_model, inplace=True)<NewLine></code></pre><NewLine><p>Does somebody have an idea what the warning means and how I can avoid that? I appreciate any hints and suggestions!</p><NewLine></div>",https://discuss.pytorch.org/u/FabianSchuetze,(Fabian Schuetze),FabianSchuetze,"June 18, 2020, 11:33am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Facing the same issue. <code>torch.quantization.convert</code> is supposed to run the observers, right. This warning does not make sense.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The prepare script inserts the observers. After that when model forward is run it also runs the observers.<br/><NewLine>If you call <code>convert</code> without calling prepare then can complain about not running observers.</p><NewLine><p>Which model are you running this on? We can take a look if there is a repro.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your replies, <a class=""mention"" href=""/u/khizar-anjum"">@khizar-anjum</a> and <a class=""mention"" href=""/u/supriyar"">@supriyar</a>!</p><NewLine><p>After <a class=""mention"" href=""/u/khizar-anjum"">@khizar-anjum</a> comments, I also filed a <a href=""https://github.com/pytorch/pytorch/issues/40278"" rel=""nofollow noopener"">issue</a>  on github. The warning is thrown when running the static quantization tutorial. I also received the warning in a SSD-type model I wrote. The quantization lead to a low accuracy and I began asking myself if it was caused by the improper quantization the observer warns against.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I also encountered this problem, is there any latest solution, thank you!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>see <a href=""https://github.com/pytorch/pytorch/issues/40278"" rel=""nofollow noopener"">here</a> for a solution.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Replace self.skip_add.add with torch.add</p><NewLine><p>class InvertedResidual(nn.Module):<br/><NewLine>def <strong>init</strong>(self,in_channel,out_channel,stride,expand_ratio):<br/><NewLine>super(InvertedResidual, self).<strong>init</strong>()<br/><NewLine>hidden_channel=int(round(in_channel*expand_ratio))<br/><NewLine>self.shortcut=stride==1 and in_channel==out_channel</p><NewLine><pre><code>    layers=[]<NewLine>    if expand_ratio!=1:<NewLine>        #1x1 pointwise conv<NewLine>        layers.append(ConvBnRelu(in_channel,hidden_channel,kersize=1))<NewLine>    layers.extend([<NewLine>        # 3x3 depthwise conv<NewLine>        ConvBnRelu(hidden_channel,hidden_channel,stride=stride,groups=hidden_channel),<NewLine>        nn.Conv2d(hidden_channel,out_channel,kernel_size=1,bias=False),<NewLine>        nn.BatchNorm2d(out_channel),<NewLine>    ])<NewLine><NewLine>    self.conv=nn.Sequential(*layers)<NewLine>    #self.skip_add = nn.quantized.FloatFunctional()<NewLine><NewLine>def forward(self,x):<NewLine>    if self.shortcut:<NewLine>        #return self.skip_add.add(x,self.conv(x))<NewLine>        return torch.add(x,self.conv(x))<NewLine>    else:<NewLine>        return self.conv(x)<NewLine></code></pre><NewLine><p>RuntimeError: Could not run ‘aten::add.Tensor’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::add.Tensor’ is only available for these backends: [CPUTensorId, MkldnnCPUTensorId, SparseCPUTensorId, VariableTensorId].</p><NewLine><p>Where did I write it wrong? thanks!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am still facing the same issue even after following the instructions present in the link you shared. Are there any further updates to it?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/khizar-anjum; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Ashar_Ali; <NewLine> ,"REPLY_DATE 1: June 18, 2020,  3:08pm; <NewLine> REPLY_DATE 2: June 19, 2020,  9:11pm; <NewLine> REPLY_DATE 3: June 20, 2020,  7:41am; <NewLine> REPLY_DATE 4: July 6, 2020,  6:59am; <NewLine> REPLY_DATE 5: July 6, 2020,  3:05pm; <NewLine> REPLY_DATE 6: July 7, 2020,  8:56am; <NewLine> REPLY_DATE 7: September 21, 2020,  7:42pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
82405,Am I correct in concluding that resnet that comes with pytorch can&rsquo;t be quantized by pytorch?,2020-05-22T01:00:41.404Z,2,308,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Greetings. I have gone through two quantization attempts for resnet50 that comes with pytorch and had mixed results:</p><NewLine><ol><NewLine><li><NewLine><p><strong>dynamic quantization</strong> works but is limited to the only <em>Linear</em> layer used in ResNet, thus the resulting improvements in model size and inference latency are just a few percent.</p><NewLine></li><NewLine><li><NewLine><p><strong>static quantization</strong> nominally succeeds, but at runtime the new model throws the exception described in <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/supported-quantized-tensor-operations/71688"">Supported quantized tensor operations</a>, which I presume is caused by the “+” operation used to implement skip connections. It doesn’t seem feasible to exclude those as they repeat throughout the entire depth of the model. Am I correct in deducing then that the resnet implementation that ships with pytorch cannot be (correctly) statically quantized by the current API?</p><NewLine></li><NewLine></ol><NewLine><p>I understand that quantization support is marked experimental – I’d like to confirm that the limitations I am seeing are expected at this stage.</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/vladium,,vladium,"May 22, 2020,  1:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Incidentally, I can reproduce the issue with a tiny test model: adding a <code>+=</code> step to <code>forward()</code> makes it non-quantizable.</p><NewLine><p>(BTW, I am aware of <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.FloatFunctional"" rel=""nofollow noopener""> <code>torch.nn.quantized.FloatFunctional</code> </a> – my use case prevents such intrusive model modifications)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>At this point, eager mode quantization might require changes to the model in order to make it work. Here is an example of how resnet50 is quantized in pytorch - <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py</a></p><NewLine><p>Going forward we are planning on graph mode quantization where such invasive model changes won’t be required.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I wanted to make sure I wasn’t missing anything obvious. The pre-quantized model works because of the changes including</p><NewLine><pre><code class=""lang-auto"">def __init__(self, *args, **kwargs):<NewLine>    ...<NewLine>    **self.add_relu = torch.nn.quantized.FloatFunctional()**<NewLine>...<NewLine>    def forward(self, x):<NewLine>        identity = x<NewLine>        out = self.conv1(x)<NewLine>        ...<NewLine>        **out = self.add_relu.add_relu(out, identity)**<NewLine><NewLine>        return out<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,I keep getting this error :</p><NewLine><p>RuntimeError: Could not run ‘quantized::conv2d’ with arguments from the ‘CPUTensorId’ backend. ‘quantized::conv2d’ is only available for these backends: [QuantizedCPUTensorId].</p><NewLine><p>Can someone help me with this?<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Any update on graph mode quantization? Facing similar issues quantizing ResNet.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/bryan_wang"">@Bryan_Wang</a>, we cannot commit to a timeline yet but we are hoping to release it as a prototype this year.  You are welcome to check out the test cases demonstrating the current API in <a href=""https://github.com/pytorch/pytorch/blob/master/test/quantization/test_quantize_fx.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/test/quantization/test_quantize_fx.py</a>, although it will be in flux for the near future and we don’t have documentation just yet.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vladium; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/vladium; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/samhithaaaa; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Bryan_Wang; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: May 22, 2020,  3:18pm; <NewLine> REPLY_DATE 2: June 2, 2020,  3:32am; <NewLine> REPLY_DATE 3: May 28, 2020,  6:54pm; <NewLine> REPLY_DATE 4: September 15, 2020, 11:58pm; <NewLine> REPLY_DATE 5: September 17, 2020,  6:33pm; <NewLine> REPLY_DATE 6: September 18, 2020,  5:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
96553,torch.nn.MaxPool2d does not need &ldquo;input&rdquo; argument run but quantized function torch.nn.quantized.functional.max_pool2d must have it,2020-09-16T21:30:31.619Z,0,37,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to quantize a salient object detection model.<br/><NewLine>Originally, my ResNet class would look like:</p><NewLine><pre><code class=""lang-auto"">import torch.nn as nn<NewLine>import torch.nn.quantized as nnq<NewLine>import torch.nn.functional as F<NewLine>import torch.nn.quantized.functional as qF<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">class ResNet(nn.Module):<NewLine>    def __init__(self, block, layers):<NewLine>        self.inplanes = 64<NewLine>        super(ResNet, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64,affine = affine_par)<NewLine>        for i in self.bn1.parameters():<NewLine>            i.requires_grad = False<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # changed to Quanti<NewLine><NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation__ = 2)<NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels<NewLine>                m.weight.data.normal_(0, 0.01)<NewLine>            elif isinstance(m, nn.BatchNorm2d):<NewLine>                m.weight.data.fill_(1)<NewLine>                m.bias.data.zero_()<NewLine>    def _make_layer(self, block, planes, blocks, stride=1,dilation__ = 1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion,affine = affine_par),<NewLine><NewLine>            )<NewLine>        for i in downsample._modules['1'].parameters():<NewLine>            i.requires_grad = False<NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride,dilation_=dilation__, downsample = downsample ))<NewLine>        self.inplanes = planes * block.expansion<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes,dilation_=dilation__))<NewLine>        return nn.Sequential(*layers)<NewLine>    def forward(self, x):<NewLine>        tmp_x = []<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.maxpool(x)<NewLine>        x = self.layer1(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer2(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer3(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer4(x)<NewLine>        tmp_x.append(x)<NewLine>        return tmp_x<NewLine></code></pre><NewLine><p>And it works just fine. But if I replace everything with quantized functions like:</p><NewLine><pre><code class=""lang-auto""><NewLine>class ResNet(nn.Module):<NewLine>    def __init__(self, block, layers):<NewLine>        self.inplanes = 64<NewLine>        super(ResNet, self).__init__()<NewLine>        self.conv1 = nnq.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)<NewLine>        self.bn1 = nnq.BatchNorm2d(64) #,affine = affine_par<NewLine>        for i in self.bn1.parameters():<NewLine>            i.requires_grad = False<NewLine>        self.relu = nnq.ReLU(inplace=False)<NewLine><NewLine>        #self.maxpool = F.nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change <NewLine><NewLine>        self.maxpool = qF.max_pool2d(x = ??? ,kernel_size=3, stride=2, padding=1, ceil_mode=True)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation__ = 2)<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels<NewLine>                m.weight.data.normal_(0, 0.01)<NewLine>            elif isinstance(m, nn.BatchNorm2d):<NewLine>                m.weight.data.fill_(1)<NewLine>                m.bias.data.zero_()<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1,dilation__ = 1):<NewLine><NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:<NewLine>            downsample = nn.Sequential(<NewLine>                nnq.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nnq.BatchNorm2d(planes * block.expansion), #,affine = affine_par<NewLine>            )<NewLine><NewLine>        for i in downsample._modules['1'].parameters():<NewLine>            i.requires_grad = False<NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride,dilation_=dilation__, downsample = downsample ))<NewLine>        self.inplanes = planes * block.expansion<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes,dilation_=dilation__))<NewLine>        return nn.Sequential(*layers)<NewLine>    def forward(self, x):<NewLine>        tmp_x = []<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        tmp_x.append(x)<NewLine><NewLine>        x = qF.max_pool2d(x,kernel_size=3, stride=2, padding=1, ceil_mode=True)  #this certainly will not create a MaxPool layer.<NewLine><NewLine>        #x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer2(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer3(x)<NewLine>        tmp_x.append(x)<NewLine>        x = self.layer4(x)<NewLine>        tmp_x.append(x)<NewLine>        return tmp_x<NewLine></code></pre><NewLine><p>Error:<br/><NewLine><code>TypeError: max_pool2d() missing 1 required positional argument: 'input'</code></p><NewLine><p>So, I think the problem is in torch.nn.MaxPool2d does not need any input argument but torch.nn.quantized.functional.max_pool2d needs an input argument. DOES ANYONE KNOW ANY WAY ROUND? How can I successfully quantize like this one other custom classes?</p><NewLine></div>",https://discuss.pytorch.org/u/parth15041995,,parth15041995,"September 16, 2020,  9:36pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://discuss.pytorch.org/t/quantized-maxpool2d-and-adaptiveavgpool2d/83314"">quantized maxpool2d and adaptiveavgpool2d</a>  should not be defined in the quantizable version.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/parth15041995; <NewLine> ,"REPLY_DATE 1: September 18, 2020,  2:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
91555,The results of torch.profiler() and time.time() do not match,2020-08-04T03:40:26.989Z,6,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>When training a 3D CNN model on a 4-tesla-v100 node with mixed precision, I got a wired result:</strong></p><NewLine><p>The mixed-precision (O1 &amp;&amp; O3) result is slower than the FP32 result when batch size is 1 using time.time() for recording the execution time.</p><NewLine><p>Using the torch.profiler(), it displays that the mixed-precision indeed speeds up the training in terms of the CPU time and the CUDA time.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/40a4db2e2ec2e62afd8f78374c15f66b637dab22"" href=""https://discuss.pytorch.org/uploads/default/original/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22.png"" title=""image""><img alt=""image"" data-base62-sha1=""9dRLaDqaLPeCicY3xehi6QPIVa2"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22_2_10x10.png"" height=""58"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22_2_690x58.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22_2_690x58.png, https://discuss.pytorch.org/uploads/default/optimized/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22_2_1035x87.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/4/0/40a4db2e2ec2e62afd8f78374c15f66b637dab22_2_1380x116.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1652×140 6.83 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>Notably, the problem only exists when batch size is equal to 1 (batch size = 4 is accelerated as predicted) and I tried two scales of the 3D CNN models. (The large model can only be trained with batch size =1 for it is too large.)</p><NewLine><p><strong>Question:</strong><br/><NewLine>It seems that there exists a large portion of the execution time that is not related to computing.<br/><NewLine>Do you have any idea about it?<br/><NewLine>Why the total execution of 3D CNN in mixed precision is slower than the FP32 when batch size =1？</p><NewLine><p><strong>Env:</strong><br/><NewLine>apex:0.1<br/><NewLine>pytorch:1.5 &amp;&amp; 1.3<br/><NewLine>hardware: DGX</p><NewLine></div>",https://discuss.pytorch.org/u/dujiangsu,(DouJS),dujiangsu,"August 4, 2020,  3:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you are manually timing CUDA operations, you would need to synchronize the code before starting and stopping the timer via <code>torch.cuda.synchronize()</code>.<br/><NewLine>Also, the first CUDA operation will create the CUDA context etc. and will be slower than the following calls, so you should add some warmup iterations.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks.<br/><NewLine>I have used torch.cuda.synchronize(), and there is about hundreds of iterations in one epoch with the same problem.<br/><NewLine>I will provide you the detailed results soon.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>FP32 training without apex:</strong><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/fc469724c578d76593b024001f912aff9204c9f7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7.png"" title=""image""><img alt=""image"" data-base62-sha1=""zZJyPycfmn3cDeiLljr6xpTrKiH"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7_2_10x10.png"" height=""488"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7_2_690x488.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7_2_690x488.png, https://discuss.pytorch.org/uploads/default/optimized/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7_2_1035x732.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/f/c/fc469724c578d76593b024001f912aff9204c9f7_2_1380x976.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2358×1671 619 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p><strong>FP32 training with apex O0:</strong><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/feb24d5e87f48d28dfac05a286ade69408310c4f"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f.png"" title=""image""><img alt=""image"" data-base62-sha1=""Al9hS1cOJJwRkn3UjDGeYrqE795"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f_2_10x10.png"" height=""378"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f_2_690x378.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f_2_690x378.png, https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f_2_1035x567.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/f/e/feb24d5e87f48d28dfac05a286ade69408310c4f_2_1380x756.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1684×924 149 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/809603cdb00a85fa9382a45864d87e3997ad1644"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644.png"" title=""image""><img alt=""image"" data-base62-sha1=""ilwwkU09QRbJ3v9lNeLu7wu4aj2"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644_2_10x10.png"" height=""348"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644_2_690x348.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644_2_690x348.png, https://discuss.pytorch.org/uploads/default/optimized/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644_2_1035x522.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/8/0/809603cdb00a85fa9382a45864d87e3997ad1644_2_1380x696.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2383×1204 377 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p><strong>mixed precision training with apex O1:</strong><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/b7d64eca98541903d429bb32419b516dc43f737f"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f.png"" title=""image""><img alt=""image"" data-base62-sha1=""qeiwOYE7CufbHOKSDVQMS0om9oX"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f_2_10x10.png"" height=""444"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f_2_690x444.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f_2_690x444.png, https://discuss.pytorch.org/uploads/default/optimized/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f_2_1035x666.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/b/7/b7d64eca98541903d429bb32419b516dc43f737f_2_1380x888.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1695×1093 190 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/96e7417b0fc3f302ca9a1f763dadd06cdf6106da"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da.png"" title=""image""><img alt=""image"" data-base62-sha1=""lwX5c7MBORV7Ns01Mfytsq9KViW"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da_2_10x10.png"" height=""395"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da_2_690x395.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da_2_690x395.png, https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da_2_1035x592.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/9/6/96e7417b0fc3f302ca9a1f763dadd06cdf6106da_2_1380x790.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">2385×1368 483 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Both the forward and backward passes seem to see a speedup between <code>O0</code> and <code>O1</code>.<br/><NewLine>Are you seeing that the complete training time in <code>O1</code> is still higher than in <code>O0</code>?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>this is not related to quantization, can we have a mixed recision/amp tag?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>That sounds like a good idea!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>The main concern is that these two kinds of profiling methods produce very different results.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dujiangsu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dujiangsu; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dujiangsu; <NewLine> ,"REPLY_DATE 1: August 4, 2020, 10:31am; <NewLine> REPLY_DATE 2: August 4, 2020, 11:23am; <NewLine> REPLY_DATE 3: August 4, 2020,  1:23pm; <NewLine> REPLY_DATE 4: August 5, 2020,  7:14am; <NewLine> REPLY_DATE 5: August 21, 2020, 10:21pm; <NewLine> REPLY_DATE 6: August 21, 2020, 10:23pm; <NewLine> REPLY_DATE 7: September 18, 2020,  3:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 2 Likes; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> 
96496,"Why program exit after the backward funtion ,please help",2020-09-16T12:11:21.651Z,0,24,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I download the code and try to run it, but it exit after the backward funtion,can anyone tell me why and how to solve it,the project is <a href=""https://github.com/ice-tong/pytorch-captcha"" rel=""nofollow noopener"">https://github.com/ice-tong/pytorch-captcha</a>, thank u<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c3d9b63d76ab248b6f14d3f75b004751d833fad2"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/3/c3d9b63d76ab248b6f14d3f75b004751d833fad2.png"" title=""图片""><img alt=""图片"" data-base62-sha1=""rWzyeXCuaBWWNe9Y15VMQfMoSeS"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/3/c3d9b63d76ab248b6f14d3f75b004751d833fad2_2_10x10.png"" height=""265"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/3/c3d9b63d76ab248b6f14d3f75b004751d833fad2.png"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">图片</span><span class=""informations"">1147×441 31.3 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/Disp41r_QAQ,(LuYiren),Disp41r_QAQ,"September 16, 2020, 12:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>i think it may be my cuda’s problem,i use the cpu to train it,and it works</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Disp41r_QAQ; <NewLine> ,"REPLY_DATE 1: September 16, 2020, 12:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
96260,Bfloat16 + transformers,2020-09-14T19:03:35.209Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><h3>Context</h3><NewLine><p>In huggingface transformers, the pegasus and t5 models overflow during beam search in half precision.</p><NewLine><p>Models that were originally trained in fairseq work well in half precision, which leads to be believe that  models trained in bfloat16 (on TPUS with tensorflow)  will often fail to generate with less dynamic range.</p><NewLine><p>I was considering starting a project to further train the models with a penalty for having large activations (to discourage overflow in fp16), but was wondering whether this is duplicative with the pytorch team’s current efforts.</p><NewLine><h3>Specific Questions</h3><NewLine><ul><NewLine><li>(a) is the snippet below likely to work in a current nightly version?</li><NewLine><li>(b) are the various kernel implementations “in the works” (and my proposed project won’t be useful in a few months)?</li><NewLine><li>© Is bfloat16 + cuda a possibility?</li><NewLine></ul><NewLine><h3>Failing Snippet</h3><NewLine><p>The following snippet with tries to run a transformer forward pass in bfloat16:</p><NewLine><pre><code class=""lang-auto"">from transformers import BartForConditionalGeneration<NewLine>import torch<NewLine>model = BartForConditionalGeneration.from_pretrained(""sshleifer/distilbart-xsum-12-3"")<NewLine>model = model.to(torch.bfloat16)<NewLine>input_ids = torch.tensor([[0, 31414, 232, 328, 740, 1140, 12695, 69, 46078, 1588, 2]], dtype=torch.long)<NewLine>model(input_ids)<NewLine># RuntimeError: ""LayerNormKernelImpl"" not implemented for 'BFloat16'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Sam_Shleifer,(Sam Shleifer),Sam_Shleifer,"September 15, 2020,  4:11am",,,,,
58580,PyTorch 1.3 wheels for Raspberry Pi (Python 3.7),2019-10-18T14:59:24.512Z,6,2838,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve made it available here: <a href=""https://wintics-opensource.s3.eu-west-3.amazonaws.com/torch-1.3.0a0%2Bdeadc27-cp37-cp37m-linux_armv7l.whl"" rel=""nofollow noopener"">https://wintics-opensource.s3.eu-west-3.amazonaws.com/torch-1.3.0a0%2Bdeadc27-cp37-cp37m-linux_armv7l.whl</a>.</p><NewLine><p>Have fun !</p><NewLine></div>",https://discuss.pytorch.org/u/LeviViana,(Levi Viana),LeviViana,"October 18, 2019,  3:40pm",8 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s works, thanks you</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It works! Thank you <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I run <code>pip3 install *.whl</code> of your wheels ,but when I <code>import torch</code> in my code,it says <code>ImportError: No module named torch</code></p><NewLine><p>How Can I solve this problem?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh I should run python3 thanks it works</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Torch 1.4 wheels available here -&gt; <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/installing-pytorch-on-raspberry-pi-3/25215/14"">Installing pytorch on raspberry pi 3</a> !</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, do you have wheels for Python 3.8 or could you explain me how you compiled it?</p><NewLine><p>I’m using a Rockchip processor with Manjaro. <a href=""https://github.com/pytorch/pytorch/issues/35049"" rel=""nofollow noopener"">PyTorch doesn’t compile for me</a> and it is really a pain to downgrade to python 3.7.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the moment, I don’t have access to the RPI where I’ve made the source code modifications (I’m under lockdown and far from my office). But as far as I can remember, I just changed the commit of <code>third_party/protobuf</code> to something more recent. I did this because I noticed an error coming from atomic types, and I saw an issue in the protobuf repo (I can’t remember which one) related to the same kind of problem. So naturally I hooked PyTorch source code to a commit fixing the issue.</p><NewLine><p>Just to make things clear, I don’t suffer from any kind of memory loss desease (at least I can’t remember being diagnosed with it <img alt="":smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smile.png?v=9"" title="":smile:""/>). If I don’t remember anything it is because I actually did it in September 2019 (for compiling torch 1.3), and I just kind of did the same steps some weeks ago without thinking too much, and it just worked.</p><NewLine><p>BTW, the binaries generated by my compilation aren’t perfect, for instance, quantization isn’t working. But the main torch functionalities are working just fine, so I think it is useful making the binaries available for everyone to use.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank it successfully works , any wheel files for torchvision sir? <a class=""mention"" href=""/u/leviviana"">@LeviViana</a></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/sundar_krishna"">@Sundar_Krishna</a>, nope I  didn’t compile it. I guess it should be easy to compile it from source, just cloning the repo and running <code>python3 setup.py install</code>.</p><NewLine><p>If it doesn’t work, I’d suggest you to post an issue at the repo, or contacting <a class=""mention"" href=""/u/fmassa"">@fmassa</a> <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually I went to piwheels website to download torchvision , pip installed it and was successful! Anyways thank you so much sir! <a class=""mention"" href=""/u/leviviana"">@LeviViana</a></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I tried to perform the installation on a RPI4 with the armv7l but it doesn’t work. On which system did you perform it?</p><NewLine><p>Sincerely</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>For <a href=""https://downloads.raspberrypi.org/raspios_lite_arm64/images/raspios_lite_arm64-2020-08-24/"" rel=""nofollow noopener"">Raspberry Pi OS 64bit</a>, <a href=""http://mathinf.com/pytorch/arm64/"" rel=""nofollow noopener"">I put up wheels of PyTorch 1.6</a>.</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, here’s my wheel for Pytorch 1.6.0 build on Raspberry Pi 4 (should work for Pi 3 too I assume, have not verified.)</p><NewLine><p><a href=""https://drive.google.com/file/d/1U9xwHsECjatOkQASQza9I8muQmOUAF9G/view?usp=sharing"" rel=""nofollow noopener"">Link to Download</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Peerawit_Naprae; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lucam; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/skyline; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/skyline; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/louisabraham; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Sundar_Krishna; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/LeviViana; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Sundar_Krishna; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Kevin_DESCHARRIERES; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/suhridh; <NewLine> ,"REPLY_DATE 1: October 30, 2019,  9:59am; <NewLine> REPLY_DATE 2: December 11, 2019,  7:05pm; <NewLine> REPLY_DATE 3: January 26, 2020, 11:32am; <NewLine> REPLY_DATE 4: January 26, 2020, 11:35am; <NewLine> REPLY_DATE 5: January 31, 2020, 10:15pm; <NewLine> REPLY_DATE 6: March 20, 2020,  2:34pm; <NewLine> REPLY_DATE 7: March 22, 2020,  9:43pm; <NewLine> REPLY_DATE 8: May 10, 2020,  6:13am; <NewLine> REPLY_DATE 9: May 11, 2020, 12:45pm; <NewLine> REPLY_DATE 10: May 11, 2020,  3:05pm; <NewLine> REPLY_DATE 11: August 26, 2020,  1:35pm; <NewLine> REPLY_DATE 12: September 7, 2020,  1:38pm; <NewLine> REPLY_DATE 13: September 14, 2020,  8:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
94165,Object Detection Quantization in PyTorch,2020-08-26T10:05:39.152Z,2,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How quantization for object detection models varies from that of classification models?<br/><NewLine>Since detection models need to handle the bbox coordinates(multiple objects in an input), there must be some scaling trick in quantization.<br/><NewLine>Is there any implementation sources?</p><NewLine></div>",https://discuss.pytorch.org/u/Nilakshan_Kunanantha,(Nilakshan Kunananthaseelan),Nilakshan_Kunanantha,"August 26, 2020, 10:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have usually quantized the backbone part for detection models while leaving the rest in fp32 and gotten good speedups. For the other part, <a class=""mention"" href=""/u/zafar"">@Zafar</a> has tried quantizing but the accuracy is usually bad.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>If we employ MinMax observer for calibrating the floating model for quantization,how are the bounding box coordinates quantized? Does it follow same way as of feature extraction?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, MinMax observer will operate the same way if you’re using it for bounding box co-ordinates. It calculates the scale and zero-point of the tensor based on min and max values.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Nilakshan_Kunanantha; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  6:04pm; <NewLine> REPLY_DATE 2: September 7, 2020,  8:37am; <NewLine> REPLY_DATE 3: September 12, 2020, 12:05am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
74439,Got slow speed on quantized model with fbgemm on X86,2020-03-26T11:23:21.741Z,2,330,"<div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch/Libtorch 1.4.0</p><NewLine><p>ABOUT CNN:</p><NewLine><ol><NewLine><li>Make a model just like MobileNetV3</li><NewLine><li>Do post-training static quantization with fbgemm</li><NewLine><li>The model size is reduced to a quarter of the original, the inferring speed is reduce to a half of the original, and the CPU usage is about 2400%, that means the default OMP_NUM_THREADS is 24</li><NewLine><li>Do “export OMP_NUM_THREADS=1”, the inferring speed is increased to 3 times the original</li><NewLine><li>Do “export OMP_NUM_THREADS=6”, the inferring speed is closed to the original</li><NewLine></ol><NewLine><p>After more testing, I found that the problem is in depth-wise conv where groups is not 1.<br/><NewLine>My question is “Is this normal?”</p><NewLine><p>ABOUT RNN:</p><NewLine><ol><NewLine><li>Make a model with 2 LSTMs</li><NewLine><li>Do post-training dynamic quantization</li><NewLine><li>The model size is reduced to a quarter of the original, the inferring speed is no significantly changed</li><NewLine></ol><NewLine><p>My question is “Is this normal?”</p><NewLine></div>",https://discuss.pytorch.org/u/wizardk,,wizardk,"March 27, 2020,  2:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> <a class=""mention"" href=""/u/zafar"">@Zafar</a> …</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/wizardk"">@wizardk</a>,</p><NewLine><p>Is the original running with a single thread?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>,<br/><NewLine>I had tested it in 1 and 10 threads. Let’s just make it simple, test it in 1 thread and limit the OMP with 1. Here are the details of the experiment.</p><NewLine><p>1.Install Pytorch 1.4, download Libtorch 1.4</p><NewLine><p>2.Prepare JIT model in Pytorch</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>import torch.quantization as Q<NewLine><NewLine>class TestConv(nn.Module):<NewLine>    def __init__(self, q, dw, i_c, o_c):<NewLine>        super(TestConv, self).__init__()<NewLine>        self.lyr = nn.Sequential(<NewLine>            nn.Conv2d(in_channels=i_c, out_channels=i_c, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=i_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>            nn.Conv2d(in_channels=i_c, out_channels=o_c, kernel_size=3, stride=1, padding=1, dilation=1, groups=i_c if dw else 1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=o_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>            nn.Conv2d(in_channels=o_c, out_channels=o_c, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=o_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.lyr(x)<NewLine><NewLine>class TestCNN(nn.Module):<NewLine>    def __init__(self, q, dw):<NewLine>        super(TestCNN, self).__init__()<NewLine>        self.q = q<NewLine>        self.quant = Q.QuantStub()<NewLine>        self.dequant = Q.DeQuantStub()<NewLine>        i_c = 1<NewLine>        self.cnn = []<NewLine>        for _ in range(8):<NewLine>            self.cnn.append(TestConv(q=q, dw=dw, i_c=i_c, o_c=i_c*2))<NewLine>            i_c *= 2<NewLine>        self.cnn = nn.Sequential(*self.cnn)<NewLine><NewLine>    def fuse_model(self):<NewLine>        for m in self.modules():<NewLine>            if type(m) == TestConv:<NewLine>                Q.fuse_modules(m.lyr, ['0', '1', '2'], inplace=True)<NewLine>                Q.fuse_modules(m.lyr, ['3', '4', '5'], inplace=True)<NewLine>                Q.fuse_modules(m.lyr, ['6', '7', '8'], inplace=True)<NewLine><NewLine>    def forward(self, x):<NewLine>        if self.q:<NewLine>            x = self.quant(x)<NewLine>        x = self.cnn(x)<NewLine>        if self.q:<NewLine>            x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>def q_test(dw):<NewLine>    def _eval(m):<NewLine>        m.eval()<NewLine>        with torch.no_grad():<NewLine>            for batch_idx in range(10):<NewLine>                x = torch.randn(10, 1, 100, 100)<NewLine>                y = m(x)<NewLine><NewLine>    print('\nno quantization\n')<NewLine>    fm = TestCNN(q=False, dw=dw)<NewLine>    torch.save(fm.state_dict(), 'float.{}.pt'.format('dw' if dw else 'cmn'))<NewLine>    torch.jit.save(torch.jit.script(fm), 'jit.f.{}.pt'.format('dw' if dw else 'cmn'))<NewLine><NewLine>    print('\npost-training static quantization\n')<NewLine>    qm = TestCNN(q=True, dw=dw)<NewLine>    qm.load_state_dict(torch.load('float.{}.pt'.format('dw' if dw else 'cmn'), map_location='cpu'))<NewLine>    qm.eval()<NewLine>    qm.fuse_model()<NewLine>    qm.qconfig = Q.get_default_qconfig('fbgemm')<NewLine>    Q.prepare(qm, inplace=True)<NewLine>    _eval(qm)  # calibration<NewLine>    Q.convert(qm, inplace=True)<NewLine>    torch.jit.save(torch.jit.script(qm), 'jit.q.{}.pt'.format('dw' if dw else 'cmn'))<NewLine><NewLine>q_test(dw=False)  # dump float and quant model without depthwise<NewLine>q_test(dw=True)  # dump float and quant model with depthwise<NewLine></code></pre><NewLine><p>3.Run JIT model in Libtorch</p><NewLine><pre><code class=""lang-auto"">#include &lt;torch/script.h&gt;<NewLine>#include &lt;torch/torch.h&gt;<NewLine>#include &lt;pthread.h&gt;<NewLine>#include &lt;omp.h&gt;<NewLine>#include &lt;algorithm&gt;<NewLine>#include &lt;iostream&gt;<NewLine>#include &lt;chrono&gt;<NewLine>#include &lt;vector&gt;<NewLine>#include &lt;numeric&gt;<NewLine><NewLine>typedef struct t_s_param {<NewLine>    torch::jit::script::Module * sess;<NewLine>    int loop_cnt;<NewLine>    int * ms, * min_ms, * max_ms;<NewLine>} s_param;<NewLine><NewLine>torch::TensorOptions g_options = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false).device(torch::kCPU);<NewLine><NewLine>torch::jit::script::Module load(const char * model_file_name)<NewLine>{<NewLine>    torch::NoGradGuard no_guard;<NewLine><NewLine>    torch::jit::script::Module module = torch::jit::load(model_file_name);<NewLine>    module.to(torch::kCPU);<NewLine>    module.eval();<NewLine><NewLine>    torch::Tensor x = torch::randn({ 1, 1, 32, 100 }, g_options);<NewLine>    std::chrono::system_clock::time_point start = std::chrono::system_clock::now();<NewLine>    torch::Tensor y = module.forward({x}).toTensor();<NewLine>    std::chrono::milliseconds elapsed = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(std::chrono::system_clock::now() - start);<NewLine>    std::cout &lt;&lt; ""warmup "" &lt;&lt; elapsed.count() &lt;&lt; std::endl;<NewLine><NewLine>    return module;<NewLine>}<NewLine><NewLine>void * working_thread(void * param)<NewLine>{<NewLine>    torch::init_num_threads();<NewLine><NewLine>    int * ms = ((s_param *)param)-&gt;ms;<NewLine>    int * min_ms = ((s_param *)param)-&gt;min_ms;<NewLine>    int * max_ms = ((s_param *)param)-&gt;max_ms;<NewLine>    for (int idx = 0; idx &lt; ((s_param *)param)-&gt;loop_cnt; ++idx) {<NewLine>        torch::NoGradGuard no_guard;<NewLine>        torch::Tensor x = torch::randn({ 1, 1, 32, 1000 }, g_options);<NewLine>        std::chrono::system_clock::time_point start = std::chrono::system_clock::now();<NewLine>        torch::Tensor y = ((s_param *)param)-&gt;sess-&gt;get_method(""forward"")({x}).toTensor();<NewLine>        std::chrono::milliseconds elapsed = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(std::chrono::system_clock::now() - start);<NewLine>        int elapsed_ms = elapsed.count();<NewLine>        *ms += elapsed_ms;<NewLine>        if (*min_ms == 0 || *min_ms &gt; elapsed_ms) { *min_ms = elapsed_ms; }<NewLine>        if (*max_ms == 0 || *max_ms &lt; elapsed_ms) { *max_ms = elapsed_ms; }<NewLine>    }<NewLine>    *ms /= ((s_param *)param)-&gt;loop_cnt;<NewLine>    std::cout &lt;&lt; ""thread quit"" &lt;&lt; std::endl;<NewLine>    return 0;<NewLine>}<NewLine><NewLine>int main(int argc, char ** argv)<NewLine>{<NewLine>    if (argc != 2) { return 0; }<NewLine><NewLine>    omp_set_num_threads(1);<NewLine>    torch::set_num_threads(1);<NewLine>    torch::set_num_interop_threads(1);<NewLine><NewLine>    torch::jit::script::Module module = load(argv[1]);<NewLine><NewLine>    // create thread<NewLine>    std::vector&lt;int&gt; ms(thread_cnt, 0);<NewLine>    std::vector&lt;int&gt; min_ms(thread_cnt, 0);<NewLine>    std::vector&lt;int&gt; max_ms(thread_cnt, 0);<NewLine>    std::vector&lt;s_param&gt; param(thread_cnt);<NewLine>    std::vector&lt;pthread_t&gt; thread_handle;<NewLine>    for (int idx = 0; idx &lt; thread_cnt; ++idx) {<NewLine>        param[idx].sess = &amp;module;<NewLine>        param[idx].op_thread_cnt = op_thread_cnt;<NewLine>        param[idx].loop_cnt = loop_cnt;<NewLine>        param[idx].ms = &amp;ms[idx];<NewLine>        param[idx].min_ms = &amp;min_ms[idx];<NewLine>        param[idx].max_ms = &amp;max_ms[idx];<NewLine>        pthread_t sub_handle;<NewLine>        pthread_create(&amp;sub_handle, 0, working_thread, &amp;param[idx]);<NewLine>        thread_handle.push_back(sub_handle);<NewLine>    }<NewLine>    for (int idx = 0; idx &lt; thread_cnt; ++idx) {<NewLine>        pthread_join(thread_handle[idx], 0);<NewLine>    }<NewLine>    float mean_time = std::accumulate(ms.begin(), ms.end(), 0) / ms.size();<NewLine>    float min_time = *std::min_element(min_ms.begin(), min_ms.end());<NewLine>    float max_time = *std::max_element(max_ms.begin(), max_ms.end());<NewLine>    std::cout &lt;&lt; ""mean time : "" &lt;&lt; mean_time &lt;&lt; std::endl;<NewLine>    std::cout &lt;&lt; ""min  time : "" &lt;&lt; min_time &lt;&lt; std::endl;<NewLine>    std::cout &lt;&lt; ""max  time : "" &lt;&lt; max_time &lt;&lt; std::endl;<NewLine>    <NewLine>    return 0;<NewLine>}<NewLine></code></pre><NewLine><p>4.Experiment result</p><NewLine><p>Run float model without depthwise:<br/><NewLine>mean time : 648<br/><NewLine>min  time : 642<br/><NewLine>max  time : 805</p><NewLine><p>Run quant model without depthwise:<br/><NewLine>mean time : 478<br/><NewLine>min  time : 474<br/><NewLine>max  time : 533</p><NewLine><p>Run float model with depthwise:<br/><NewLine>mean time : 422<br/><NewLine>min  time : 376<br/><NewLine>max  time : 608</p><NewLine><p>Run quant model with depthwise:<br/><NewLine>mean time : 1731<br/><NewLine>min  time : 1725<br/><NewLine>max  time : 1828</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/wizardk"">@wizardk</a>: Thanks for reporting it. It’s an issue in our backend library, FBGEMM, for certain specific depthwise shapes (i_c != o_c ). In your case, depthwise convolution goes through a slower path. <a href=""https://github.com/pytorch/FBGEMM/issues/347"" rel=""nofollow noopener"">https://github.com/pytorch/FBGEMM/issues/347</a> is tracking the progress on improving performance for such cases. If your use case doesn’t need i_c != o_c, please proceed with using i_c == o_c for depthwise convolutions.</p><NewLine><p>Meanwhile I see the following results for your 4 cases, if I make depthwise to have the same i_c and o_c.<br/><NewLine>Self CPU time total: 64.826ms<br/><NewLine>Self CPU time total: 31.913ms<br/><NewLine>Self CPU time total: 50.317ms<br/><NewLine>Self CPU time total: 17.530ms</p><NewLine><p>The following is the code I used for benchmarking.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>import torch.quantization as Q<NewLine><NewLine>torch.set_num_threads(1)<NewLine><NewLine>class TestConv(nn.Module):<NewLine>    def __init__(self, q, dw, i_c, o_c):<NewLine>        super(TestConv, self).__init__()<NewLine>        self.lyr = nn.Sequential(<NewLine>            nn.Conv2d(in_channels=i_c, out_channels=i_c, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=i_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>            nn.Conv2d(in_channels=i_c, out_channels=o_c, kernel_size=3, stride=1, padding=1, dilation=1, groups=i_c if dw else 1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=o_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>            nn.Conv2d(in_channels=o_c, out_channels=2*o_c, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False),<NewLine>            nn.BatchNorm2d(num_features=2*o_c),<NewLine>            nn.ReLU(inplace=False) if q else nn.ReLU6(inplace=True),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.lyr(x)<NewLine><NewLine>class TestCNN(nn.Module):<NewLine>    def __init__(self, q, dw):<NewLine>        super(TestCNN, self).__init__()<NewLine>        self.q = q<NewLine>        self.quant = Q.QuantStub()<NewLine>        self.dequant = Q.DeQuantStub()<NewLine>        i_c = 1<NewLine>        self.cnn = []<NewLine>        for _ in range(8):<NewLine>            self.cnn.append(TestConv(q=q, dw=dw, i_c=i_c, o_c=i_c))<NewLine>            i_c *= 2<NewLine>        self.cnn = nn.Sequential(*self.cnn)<NewLine><NewLine>    def fuse_model(self):<NewLine>        for m in self.modules():<NewLine>            if type(m) == TestConv:<NewLine>                Q.fuse_modules(m.lyr, ['0', '1', '2'], inplace=True)<NewLine>                Q.fuse_modules(m.lyr, ['3', '4', '5'], inplace=True)<NewLine>                Q.fuse_modules(m.lyr, ['6', '7', '8'], inplace=True)<NewLine><NewLine>    def forward(self, x):<NewLine>        if self.q:<NewLine>            x = self.quant(x)<NewLine>        x = self.cnn(x)<NewLine>        if self.q:<NewLine>            x = self.dequant(x)<NewLine>        return x<NewLine>def q_test(dw):<NewLine>    def _eval(m):<NewLine>        m.eval()<NewLine>        with torch.no_grad():<NewLine>            for batch_idx in range(10):<NewLine>                x = torch.randn(10, 1, 100, 100)<NewLine>                y = m(x)<NewLine><NewLine>    print('\nno quantization\n')<NewLine>    fm = TestCNN(q=False, dw=dw)<NewLine>    fm.eval()<NewLine>    torch.save(fm.state_dict(), 'float.{}.pt'.format('dw' if dw else 'cmn'))<NewLine>    scriptModel = torch.jit.script(fm)<NewLine>    x = torch.randn(1, 1, 32, 100)<NewLine>    with torch.autograd.profiler.profile(record_shapes=True) as prof:<NewLine>        scriptModel(x)<NewLine>    print(""autograd prof:\n {} \n"".format(prof.key_averages(group_by_input_shape=False)))<NewLine>    #print(""autograd prof table:\n {} \n"".format(prof.table(row_limit=-1)))<NewLine><NewLine>    torch.jit.save(scriptModel, 'jit.f.{}.pt'.format('dw' if dw else 'cmn'))<NewLine><NewLine>    print('\npost-training static quantization\n')<NewLine>    qm = TestCNN(q=True, dw=dw)<NewLine>    #print(qm)<NewLine>    qm.load_state_dict(torch.load('float.{}.pt'.format('dw' if dw else 'cmn'), map_location='cpu'))<NewLine>    qm.eval()<NewLine>    qm.fuse_model()<NewLine>    qm.qconfig = Q.get_default_qconfig('fbgemm')<NewLine>    Q.prepare(qm, inplace=True)<NewLine>    _eval(qm)  # calibration<NewLine>    Q.convert(qm, inplace=True)<NewLine>    qscriptModel = torch.jit.script(qm)<NewLine>    with torch.autograd.profiler.profile(record_shapes=True) as prof:<NewLine>        qscriptModel(x)<NewLine>    print(""autograd prof:\n {} \n"".format(prof.key_averages(group_by_input_shape=False)))<NewLine>    #print(""autograd prof table:\n {} \n"".format(prof.table(row_limit=-1)))<NewLine>    torch.jit.save(qscriptModel, 'jit.q.{}.pt'.format('dw' if dw else 'cmn'))<NewLine><NewLine>q_test(dw=False)  # dump float and quant model without depthwise<NewLine>q_test(dw=True)  # dump float and quant model with depthwise<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> You explained it very clearly and helpful,  thanks a lot.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I confronted similar slow speed incident but this time it happens when I’m using depth-wise convolution with non-squared kernel (e.g, 3X1).<br/><NewLine>When I change the filtersize to (3,1) using the script above, it shows me the speed as follows,<br/><NewLine>Self CPU time total: 48.013ms<br/><NewLine>Self CPU time total: 29.187ms<br/><NewLine>Self CPU time total: 24.026ms<br/><NewLine>Self CPU time total: 85.271ms</p><NewLine><p>Int8 operation with non-squared depth-wise convolution significantly increases the elapsed time.<br/><NewLine>Is there anyway I can deal with this issue?</p><NewLine><p>Thank you.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wizardk; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wizardk; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kekepa15; <NewLine> ,"REPLY_DATE 1: March 27, 2020,  8:37pm; <NewLine> REPLY_DATE 2: March 27, 2020, 10:55pm; <NewLine> REPLY_DATE 3: March 29, 2020,  4:20am; <NewLine> REPLY_DATE 4: April 14, 2020,  1:07pm; <NewLine> REPLY_DATE 5: April 14, 2020,  1:00pm; <NewLine> REPLY_DATE 6: September 11, 2020,  1:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
95734,How to ensure the repeatability of the results of the Pytorch model across devices？,2020-09-10T04:15:12.674Z,0,36,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am a little bit confused about the randomness of the pytorch model.</p><NewLine><p>I used the following code to fix the random seed so that the training results of the model can be repeated on the same device：</p><NewLine><pre><code class=""lang-auto"">def seed_torch(seed=2020):<NewLine>    random.seed(seed)<NewLine>    os.environ[""PYTHONHASHSEED""] = str(seed)<NewLine>    np.random.seed(seed)<NewLine>    torch.manual_seed(seed)<NewLine>    torch.cuda.manual_seed(seed)<NewLine>    torch.backends.cudnn.deterministic = True<NewLine>    torch.backends.cudnn.benchmark = False<NewLine></code></pre><NewLine><p>But what is confusing is that when the code is run on another device with the same hardware configuration and software environment, it will produce different results.</p><NewLine><p>I reinstalled the virtual environment to ensure that the versions of the libraries are consistent, but this problem still puzzles me.</p><NewLine><p>Feel free to ask if more code is needed to explain the problem.</p><NewLine></div>",https://discuss.pytorch.org/u/dirk,,dirk,"September 10, 2020,  4:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Refer to <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/reproducibility-over-different-machines/63047/2"">Reproducibility over Different Machines</a> for the same discussion!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/briankosw; <NewLine> ,"REPLY_DATE 1: September 10, 2020,  5:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88063,Understand the usage of quantized weights from quantized model,2020-07-06T04:03:40.710Z,6,361,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry if the question has been answered somewhere, I couldn’t find similar question across the forum so I would want to put my question here, and hope for your answer.</p><NewLine><p>So we have a simple trained model, and applied the static quantization to get quantized model using ‘fbgemm’ as qconfig:<br/><NewLine><code>myModel.qconfig = torch.quantization.get_default_qconfig('fbgemm')</code></p><NewLine><p>After this, we have quantized model with weights (int_repr()) exported.<br/><NewLine>I expect if I create a similar architecture, and import the int represented weight in, I can generate same result per layer as quantized model, but turn out the results are different.</p><NewLine><p>Below is detailed flows:<br/><NewLine><span class=""hashtag"">#Notes:</span> x_batch and x_quant were exported previously with quant model eval to pickle file and reload here for comparison</p><NewLine><pre><code class=""lang-auto"">    #Flow 1<NewLine>    #using x as input, calculate results through loaded quantized model <NewLine>    #forward: x--&gt; x_quant = self.quant(x) --&gt; f = self.featExt(x_quant)<NewLine>    # featExt definition: self.featExt = nn.Sequential(nn.Conv2d(1, 8, <NewLine>    #                   kernel_size=5, stride=5, bias=False), nn.ReLU())<NewLine>    x_quant_new, f, x_conv, y_hat = quant_net.forward(x_batch[0])<NewLine>    print('using saved quantized model: ')<NewLine>    print('x_quant to compare(int): ', x_quant_new.int_repr())<NewLine>    print('filter to compare(int): ', quant_net.featExt[0].weight().int_repr())<NewLine>    print('output to compare(int): ', f.int_repr())<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">    #Flow 2<NewLine>    #using x_quant as input, calculate conv 2d using pytorch function<NewLine>    conv2d = nn.Conv2d(1, 8, kernel_size=5, stride=5, bias=False)<NewLine>    conv2d.weight.data = my_debug_net.featConv.weight.data<NewLine>    with torch.no_grad():<NewLine>        conv2d.eval()<NewLine>        res1 = conv2d(x_quant[0].type(torch.CharTensor)) <NewLine>    print('*********using F.conv2d***********')<NewLine>    print('x_quant: ', x_quant[0])<NewLine>    print('filter: ', conv2d.weight.data)<NewLine>    print('F.conv2d Output ', res1)<NewLine>    print('F.relu Output ', F.relu(res1))<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Giang_Dang,(Giang Dang),Giang_Dang,"July 6, 2020,  4:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88063"" data-username=""Giang_Dang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/giang_dang/40/26354_2.png"" width=""20""/> Giang_Dang:</div><NewLine><blockquote><NewLine><p>I expect if I create a similar architecture, and import the int represented weight in, I can generate same result per layer as quantized model, but turn out the results are different.</p><NewLine></blockquote><NewLine></aside><NewLine><p>This should be possible, if the weights are copied correctly.  Would you have a reproducible toy example of this behavior?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for confirming the thinking. I can’t upload the quantized model and architecture we are currently working here but for the purpose of demonstrating, I will create a toy example to share for the investigation.<br/><NewLine>For now I can share the log from the 2 flows I put in my question, that is to prove the weights are the same. Perhaps with this log you will find something that I had missed.<br/><NewLine>I added the log here to avoid messing-up the conversation: <a href=""https://drive.google.com/drive/folders/1O7A96jJIWbqS_5uYL1tmp__N6LJHMh9k?usp=sharing"" rel=""nofollow noopener"">https://drive.google.com/drive/folders/1O7A96jJIWbqS_5uYL1tmp__N6LJHMh9k?usp=sharing</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/giang_dang"">@Giang_Dang</a>,</p><NewLine><p>Unfortunately it’s hard to spot what could be missing in your code without seeing it.  Here is a toy example representing the expected behavior:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine># toy model<NewLine><NewLine>class M(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.fc = nn.Linear(2, 2)<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.fc(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>m1 = M()<NewLine>m2 = M()<NewLine><NewLine>def static_quant(m):<NewLine>    m.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(m, inplace=True)<NewLine>    # toy calibration<NewLine>    data = torch.rand(4, 2)<NewLine>    m(data)<NewLine>    torch.quantization.convert(m, inplace=True)<NewLine>    <NewLine>static_quant(m1)<NewLine>static_quant(m2)<NewLine># m1 and m2 now have different weights, because of different<NewLine># initialization, and different calibration data<NewLine><NewLine># verify that same inputs do not lead to same outputs<NewLine>data = torch.rand(16, 2)<NewLine>print('outputs match', torch.allclose(m1(data), m2(data)))<NewLine><NewLine># set m2's weights to be equal to m1's weights<NewLine>m2.quant.load_state_dict(m1.quant.state_dict())<NewLine>m2.fc.load_state_dict(m1.fc.state_dict())<NewLine><NewLine># verify that same inputs lead to same outputs<NewLine>data = torch.rand(16, 2)<NewLine>print('outputs match', torch.allclose(m1(data), m2(data)))<NewLine></code></pre><NewLine><p>One thing you could try is to use the state dict to transfer weights between modules of the same type, instead of manually copying attributes over.  However, if you manually transfer all the attributes correctly, it should work as well.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a>: thank you for taking the time to create the toy example.<br/><NewLine>The approach to save state_dict and reload the state_dict with the same architecture as you described would work as expected, and I don’t have issue with that.<br/><NewLine>To clarify, my purpose is to have: trained pytorch model (M) -&gt; quantized trained pytorch model(M1) -&gt; port to run on ARM cortex-M4 with CMSIS-NN (M3).<br/><NewLine>In order to do so, I am doing the intermediate steps:<br/><NewLine>quantized trained pytorch model(M2) -&gt; export weights param in integers -&gt; load to a brand new Pytorch architecture without quantized info(M2_int) -&gt; this model will be close to what is developed in embedded device (M3).<br/><NewLine>I will update your example to show the above steps. What I am not clear is some normalization steps done in pytorch internal functions, that would be different between quantized and non-quantized model.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The state dicts don’t have to be used on the whole model, you can do it module by module, something like <code>model2.conv3.load_state_dict(model1.conv3.state_dict())</code>.  But in any case, loading a state dict is the same thing as transferring all the attributes manually, it’s just easier.</p><NewLine><blockquote><NewLine><p>load to a brand new Pytorch architecture without quantized info(M2_int)</p><NewLine></blockquote><NewLine><p>If you are still seeing different results after transferring the weights, there could be other differences.  Some things to debug would be:</p><NewLine><ul><NewLine><li>are the other parameters you need to transfer (conv bias, etc)</li><NewLine><li>is the input data coming in exactly the same (you are modeling quant/dequant correctly, etc)</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88063"" data-username=""Giang_Dang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/giang_dang/40/26354_2.png"" width=""20""/> Giang_Dang:</div><NewLine><blockquote><NewLine><p>I expect if I create a similar architecture, and import the int represented weight in, I can generate same result per layer as quantized model</p><NewLine></blockquote><NewLine></aside><NewLine><p>Unless the two architectures are the <strong>same</strong>, you can not expect to get the same same result as your network output. You are guaranteed to get the same result for the very same layers, with  the <strong>same input</strong>, but anything other than that will cause the result to change.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""88063"" data-username=""Vasiliy_Kuznetsov""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/vasiliy_kuznetsov/40/24530_2.png"" width=""20""/> Vasiliy_Kuznetsov:</div><NewLine><blockquote><NewLine><p>If you are still seeing different results after transferring the weights, there could be other differences. Some things to debug would be:</p><NewLine><ul><NewLine><li>are the other parameters you need to transfer (conv bias, etc)</li><NewLine><li>is the input data coming in exactly the same (you are modeling quant/dequant correctly, etc)</li><NewLine></ul><NewLine></blockquote><NewLine></aside><NewLine><p>Hi Both,<br/><NewLine>I am thankful for your time to look into the issue.<br/><NewLine>I totally agree with you both on the logic. I modified the program from <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> to demonstrate what I am trying to achieve. Would this be explained, I am thankful for that, since this is an essential step to convert pytorch model to C model:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine># toy model<NewLine><NewLine>class M(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.conv = nn.Conv2d(1,1,kernel_size=2,stride=2,padding=0,bias=False)<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x_quant = self.quant(x)<NewLine>        x_conv = self.conv(x_quant)<NewLine>        y = self.dequant(x_conv)<NewLine>        return x_quant, x_conv, y<NewLine><NewLine>class M_int(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.conv = nn.Conv2d(1,1,kernel_size=2,stride=2,padding=0,bias=False)<NewLine>    <NewLine>    def forward(self, x):<NewLine>        # get x_quant as input<NewLine>        x_conv = self.conv(x)<NewLine>        return x_conv<NewLine><NewLine><NewLine>m1 = M()<NewLine>m2 = M()<NewLine><NewLine>def static_quant(m):<NewLine>    m.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(m, inplace=True)<NewLine>    # toy calibration<NewLine>    data = torch.rand(4, 1, 2, 2)<NewLine>    m(data)<NewLine>    torch.quantization.convert(m, inplace=True)<NewLine>    <NewLine>static_quant(m1)<NewLine>static_quant(m2)<NewLine># m1 and m2 now have different weights, because of different<NewLine># initialization, and different calibration data<NewLine><NewLine># verify that same inputs do not lead to same outputs<NewLine>data = torch.rand(4, 1, 2, 2)<NewLine>print('outputs match', torch.allclose(m1(data)[2], m2(data)[2]))<NewLine><NewLine># set m2's weights to be equal to m1's weights<NewLine>m2.quant.load_state_dict(m1.quant.state_dict())<NewLine>m2.conv.load_state_dict(m1.conv.state_dict())<NewLine><NewLine># verify that same inputs lead to same outputs<NewLine>data = torch.rand(4, 1, 2, 2)<NewLine>print('outputs match', torch.allclose(m1(data)[2], m2(data)[2]))<NewLine><NewLine>m3 = M_int()<NewLine>with torch.no_grad():<NewLine>    m3.conv.weight.data = m1.conv.state_dict()['weight'].int_repr().type(torch.ByteTensor)<NewLine>    m3.eval()<NewLine>    data = torch.rand(4, 1, 2, 2)<NewLine>    x_quant, x_conv, y = m1(data)<NewLine>    x_conv3 = m3(x_quant.int_repr().type(torch.ByteTensor))<NewLine>print('weight match', torch.allclose(m1.conv.state_dict()['weight'].int_repr().type(torch.ByteTensor), m3.conv.weight.data))<NewLine>print('outputs match', torch.allclose(x_conv.int_repr(), x_conv3))<NewLine><NewLine></code></pre><NewLine><p>M_int model is the fresh model with integer weight loaded-in.<br/><NewLine>I expect to have the result after conv layer to be the same for m1 and m3.<br/><NewLine>I changed from linear to conv just because I am debugging for convolution2D currently.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/giang_dang"">@Giang_Dang</a>,</p><NewLine><p>I’m not sure if it makes sense conceptually to try to put weights from a quantized layer directly into a floating point layer.  Consider the translation between the quantized and floating point domain:</p><NewLine><pre><code class=""lang-auto"">x_quant = round(x_fp / scale + zero_point)<NewLine>x_fp = (x_quant - zero_point) * scale<NewLine></code></pre><NewLine><p>For the weights of the quantized conv, even though they are stored in the quantized domain, they represent the floating point domain.  To use them in non-quantized layers you’d need to convert back to the floating point domain.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a>,</p><NewLine><p>For quant() layer yes I managed to figure out the formula and it is fine to apply.<br/><NewLine>For the weights of convolution layer, it goes the same formula to calculate int_repr() values from float with scale and zero_point.<br/><NewLine>The purpose of quantization is to have parameters in integer and hence reduce computation cost during convolution. If we couldn’t produce the same result with plain network with these weights, it seems the task to port successfully to C model is not feasible, or at least, not well-supported by Pytorch currently.</p><NewLine><p>Cheers,<br/><NewLine>Giang</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">with torch.no_grad():<NewLine>    m3.conv.weight.data = m1.conv.state_dict()['weight'].int_repr().type(torch.ByteTensor)<NewLine></code></pre><NewLine><p>This line doesn’t seem to be applying the dequantization.  If you want m3.conv to match m1.conv when m3 is floating point and m1 is quantized, you would need to convert the weights back to floating point.  Int_repr() returns the integer weights but it does not dequantize them.</p><NewLine><p>One other thing you could consider is to run quantization on m3 directly.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi, have you solve your problem? I have the same question <img alt="":thinking:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/thinking.png?v=9"" title="":thinking:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Giang_Dang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Giang_Dang; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Giang_Dang; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Giang_Dang; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/111179; <NewLine> ,"REPLY_DATE 1: July 6, 2020,  9:41pm; <NewLine> REPLY_DATE 2: July 7, 2020,  4:18am; <NewLine> REPLY_DATE 3: July 15, 2020,  4:54pm; <NewLine> REPLY_DATE 4: July 16, 2020,  7:02am; <NewLine> REPLY_DATE 5: July 16, 2020,  4:10pm; <NewLine> REPLY_DATE 6: July 18, 2020,  8:07am; <NewLine> REPLY_DATE 7: July 21, 2020, 10:33am; <NewLine> REPLY_DATE 8: July 21, 2020,  4:27pm; <NewLine> REPLY_DATE 9: July 22, 2020,  6:40am; <NewLine> REPLY_DATE 10: July 22, 2020,  3:38pm; <NewLine> REPLY_DATE 11: September 6, 2020,  1:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
94946,Can nn.quantized.FloatFunctional().cat() be used multiple times in one module?,2020-09-02T12:35:57.466Z,0,61,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I was reading the source code of torchvision.models.quantization.inception_v3, I found self.myop.cat is used 3 times in QuantizableInceptionE, so when I finished the training, there is only one group of quantization params (min_val/max_val/scale/zeros_point). If I understand correctly, we need 3 different group of quantization params for each concat operation.<br/><NewLine>Can any one help to explain whether it’s a bug here or I misunderstood it?</p><NewLine></div>",https://discuss.pytorch.org/u/chuanqi305,(Chuanqi),chuanqi305,"September 2, 2020, 12:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right, looks like we need 3 different self.myop.cat. Could you file an issue for it? We will take a look.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks! I saw it has been solved in the newest version by this <a href=""https://github.com/pytorch/vision/commit/ae4012e27287466c6250235daa5ac3b74c26fbf2#diff-8309d86ffb3a63402883ab29f4c6b0f2"" rel=""nofollow noopener"">commit</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chuanqi305; <NewLine> ,"REPLY_DATE 1: September 12, 2020, 12:06am; <NewLine> REPLY_DATE 2: September 4, 2020,  8:43am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
94961,Permutation not working in Quantized Model,2020-09-02T14:37:15.242Z,2,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model that contains a custom permutation that I need to apply to the second dimension of a Tensor, this is implemented as</p><NewLine><pre><code class=""lang-auto"">def forward(self, x: torch.Tensor):<NewLine>    return x[:, self.permutation]<NewLine></code></pre><NewLine><p>where <code>self.permutation</code> is a LongTensor.<br/><NewLine>When the model is not quantized (x is a FloatTensor) everything works correctly, when I quantize the model I get the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'aten::empty.memory_format' with arguments from the 'QuantizedCPU' backend. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Autograd, Profiler, Tracer]<NewLine></code></pre><NewLine><p>It seems that the operation is not implemented, I’m using PyTorch 1.6.0.<br/><NewLine>Is there any alternative permutation operation that I can use?</p><NewLine><p>Thanks,<br/><NewLine>Matteo</p><NewLine></div>",https://discuss.pytorch.org/u/matteo-ronchetti,(Matteo Ronchetti),matteo-ronchetti,"September 2, 2020,  2:37pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Maybe you can work around it by adding dequantize before this module and quantize after so that it’s not quantized?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes it is a possible workaround but, because I need the permutation in many layers (I’m using a variant of ShuffleNet), I think the performance will suffer.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> do you know if this is supported in PyTorch quantization?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/matteo-ronchetti; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: September 3, 2020,  6:42am; <NewLine> REPLY_DATE 2: September 3, 2020,  8:25am; <NewLine> REPLY_DATE 3: September 3, 2020, 11:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
94303,How to use quantize_per_tensor,2020-08-27T12:30:00.338Z,2,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model which is trained in Kaldi and I’m able to load the model parameters in PyTorch as tensors.<br/><NewLine>I  am trying to perform post-quantization of the weight matrices and I’ve tried to use the quantize_per_tensor function.<br/><NewLine>For. ex:</p><NewLine><pre><code class=""lang-auto"">a = torch.rand(10)<NewLine>b = torch.rand(10)<NewLine>scale_a = (max_a - min_a) / (qmax - qmin)<NewLine>zpt_a = qmin - min_a / scale_a<NewLine>scale_b = (max_b - min_b) / (qmax - qmin)<NewLine>zpt_b = qmin - min_b / scale_b<NewLine>a_quant = torch.quantize_per_tensor(a, scale_a, -127, torch.qint8)<NewLine>b_quant = torch.quantize_per_tensor(b, scale_b, -127, torch.qint8)<NewLine>a_quant + b_quant<NewLine></code></pre><NewLine><p>When I add the 2 quantized tensors, I get the below error</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>RuntimeError: Could not run 'aten::add.Tensor' with arguments from the 'QuantizedCPU' backend. 'aten::add.Tensor' is only available for these backends: [CPU, CUDA, MkldnnCPU, SparseCPU, SparseCUDA, Meta, Named, Autograd, Profiler, Tracer].<NewLine></code></pre><NewLine><p>It seems that I can convert fp32 to int8 but not perform any integer arithmetic .<br/><NewLine>Any help as to how to use this will be appreciated.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/aprasad,(Amrutha),aprasad,"August 27, 2020, 12:30pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/aprasad"">@aprasad</a>: If you are just looking to quantize weights only and want to keep activations in fp32, please look into dynamic quantization. It does exactly that and you it will calculate scale/zero_points automatically for you as well.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>BTW if you want to use add (or other such operations) on quantized tensor you can use in the following way.</p><NewLine><p>qfn = torch.nn.quantized.QFunctional()<br/><NewLine>qfn.add(a_quant, b_quant)</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/quantization.html#qfunctional"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/quantization.html#qfunctional</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>: Thank you for the reply. The quantized functional works.<br/><NewLine>I want to quantize both the weights  and the activations and run inference in Pytorch with my custom class.</p><NewLine><p>Is there a function that calculates the scale/zero_points automatically for that?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>: I tried using the qfn to multiply 2 matrices and I get the below error.</p><NewLine><p>For <code>a</code> of shape (1,10) and <code>b</code> of shape (10,20)  if i do</p><NewLine><pre><code class=""lang-auto"">qfn.mul(a_quant, b_quant)<NewLine></code></pre><NewLine><p>I get,</p><NewLine><pre><code class=""lang-auto"">r = ops.quantized.mul(x, y, scale=self.scale, zero_point=self.zero_point)<NewLine>RuntimeError: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 1<NewLine></code></pre><NewLine><p>Is there any function for matrix multiplication of quantized tensors?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/aprasad; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/aprasad; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  4:39pm; <NewLine> REPLY_DATE 2: August 27, 2020,  5:03pm; <NewLine> REPLY_DATE 3: August 28, 2020,  9:09am; <NewLine> REPLY_DATE 4: September 2, 2020,  4:05pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
83294,GPU support for quantization,2020-05-29T00:01:55.162Z,2,130,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Will quantization be supported for GPUs anytime soon? I have a project where evaluation speed is a very major concern and would love to use quantization to speed it up.</p><NewLine><p>I see the CPU quantization tutorial on the docs was written about 6 months ago, so I am really just curious if this is on the developers’ radar at all and if we can expect this <em>eventually</em> or in the near future.</p><NewLine></div>",https://discuss.pytorch.org/u/Haxxardoux,(Will Tepe),Haxxardoux,"May 29, 2020, 12:01am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We will be considering the GPU option in the second half of the year, but I think it probably won’t be a high priority item.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>is there a particular reason it is not a high priority? i am still a student but was under the impression that inference with large models was typically done on GPUs, and quantization would be very beneficial</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>this depends on our internal customers, we haven’t decided yet.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure if there is a voting process but we (as a company) are using pytorch in our production process and inference speed of our custom BERT model is critical for us. In my opinion to get more adoption of pytorch in production and commercial applications inference speed is going to be critical and this feature would be a huge step forward for that. My two cents.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Haxxardoux; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/KevinPD66; <NewLine> ,"REPLY_DATE 1: May 29, 2020,  5:53pm; <NewLine> REPLY_DATE 2: June 1, 2020,  4:42pm; <NewLine> REPLY_DATE 3: June 1, 2020,  5:12pm; <NewLine> REPLY_DATE 4: September 1, 2020, 12:39pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
94144,FwFM Quantization,2020-08-26T08:11:43.197Z,1,79,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to perform dynamic quantization on a FwFM model (<a href=""https://arxiv.org/pdf/1806.03514.pdf"" rel=""nofollow noopener"">paper</a>). Unfortunately, i did not manage to make it work!</p><NewLine><p>I get following <strong>error</strong>:</p><NewLine><pre><code class=""lang-auto"">AttributeError: 'function' object has no attribute 't'<NewLine></code></pre><NewLine><p>corresponding to the line in my <strong>forward</strong> function:</p><NewLine><pre><code class=""lang-auto"">outer_fwfm = torch.einsum('klij,kl-&gt;klij', outer_fm,<NewLine>                                              (self.field_cov.weight.t() + self.field_cov.weight) * 0.5)<NewLine></code></pre><NewLine><p>Corresponding model layer <strong>before</strong> quantization:</p><NewLine><pre><code class=""lang-auto"">(field_cov): Linear(in_features=39, out_features=39, bias=False)<NewLine></code></pre><NewLine><p><strong>After</strong> quantization:</p><NewLine><pre><code class=""lang-auto"">(field_cov): DynamicQuantizedLinear(in_features=39, out_features=39, dtype=torch.qint8, qscheme=torch.per_tensor_affine)<NewLine></code></pre><NewLine><p>Quantization line:</p><NewLine><pre><code class=""lang-auto"">quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)<NewLine></code></pre><NewLine><p>Am I missing something out here? Thanks for your help!<br/><NewLine><a href=""https://github.com/pintonos/xsDeepFwFM"" rel=""nofollow noopener"">GitHub Code</a></p><NewLine></div>",https://discuss.pytorch.org/u/pintonos,(Pintonos),pintonos,"August 26, 2020,  8:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that <code>DynamicQuantizedLinear</code> replaces the <code>weight</code> attribute with a method:</p><NewLine><pre><code class=""lang-python"">lin = torch.nn.quantized.dynamic.Linear(1, 1)<NewLine>print(lin.weight())<NewLine></code></pre><NewLine><p>So you might need to call <code>self.field_cov.weight().t() + self.field_cov.weight()</code>.</p><NewLine><p>Note that, while this might work functionality-wise, I’m not familiar enough with your use case or the dynamic quantization to claim it’s the right approach to use when quantization is applied.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks!</p><NewLine><p>Code now looks like this:</p><NewLine><pre><code class=""lang-auto""> if self.dynamic_quantization or self.static_quantization or self.quantization_aware:<NewLine>                        q_func = QFunctional()<NewLine>                        q_add = q_func.add(self.field_cov.weight().t(), self.field_cov.weight())<NewLine>                        q_add_mul = q_func.mul_scalar(q_add, 0.5)<NewLine>                        outer_fwfm = torch.einsum('klij,kl-&gt;klij', outer_fm, q_add_mul)<NewLine></code></pre><NewLine><p><strong>Error</strong> Traceback:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>return _VF.einsum(equation, operands)<NewLine>RuntimeError: Could not run 'aten::mul.Tensor' with arguments from the 'QuantizedCPU' backend. 'aten::mul.Tensor' is only available for these backends: [CPU, CUDA, MkldnnCPU, SparseCPU, SparseCUDA, Named, Autograd, Profiler, Tracer, Batched].<NewLine></code></pre><NewLine><p>Can <code>torch.einsum(...)</code> be quantized? Would there be a workaround since it consists of mulitplication and addition?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, I’m not experienced enough using the quantization package, so we would need to wait for an expert. <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""94144"" data-username=""pintonos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/pintonos/40/28210_2.png"" width=""20""/> pintonos:</div><NewLine><blockquote><NewLine><p><code>einsum</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>hi <a class=""mention"" href=""/u/pintonos"">@pintonos</a>, currently we don’t have a quantized kernel for <code>einsum</code>, we would be happy to review a PR if someone is interested in implementing.  In the meanwhile, a workaround could be to dequantize -&gt; floating point einsum -&gt; quantize.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pintonos; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: August 27, 2020,  8:00am; <NewLine> REPLY_DATE 2: August 27, 2020,  8:47am; <NewLine> REPLY_DATE 3: August 28, 2020,  8:53am; <NewLine> REPLY_DATE 4: August 28, 2020,  5:41pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
93844,Incorrect results after loading saved quantized model,2020-08-24T02:42:16.082Z,0,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have quantized and saved the model using this code -</p><NewLine><pre><code class=""lang-auto"">model = Net()<NewLine>model.load_state_dict(torch.load('model.pth', map_location=torch.device('cpu')))                                <NewLine>model.qconfig = torch.quantization.default_qconfig              <NewLine>model.eval()<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine>evaluate(model)<NewLine>torch.quantization.convert(model, inplace=True)<NewLine>model.eval()<NewLine>x = evaluate(model)<NewLine>torch.save(model.state_dict(), 'model_q.pth')<NewLine></code></pre><NewLine><p>and loading the model like this -</p><NewLine><pre><code class=""lang-auto"">model2 = Net()<NewLine>torch.qconfig = torch.quantization.default_qconfig              <NewLine>model2.eval()<NewLine>torch.quantization.prepare(model2, inplace=True)<NewLine>torch.quantization.convert(model2, inplace=True)<NewLine>model2.eval()<NewLine>model2.load_state_dict(torch.load('model_q.pth'))<NewLine>xQ = evaluate(model2)<NewLine></code></pre><NewLine><p>Now x and xQ are different. I checked the parameters of both ‘model’ and ‘model2’. Parameters are same.</p><NewLine><pre><code class=""lang-auto"">for i in range(len(list(model.parameters()))):<NewLine>    print(np.equal(list(model.parameters())[i].detach().numpy(), list(model2.parameters())[i].detach().numpy()))<NewLine></code></pre><NewLine><p>All parameters are equal.</p><NewLine><p>Is there anything incorrect with my method of loading or saving? or some bug in PyTorch?</p><NewLine></div>",https://discuss.pytorch.org/u/deepak_mangla,(Deepak Mangla),deepak_mangla,"August 24, 2020,  2:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I  compared output of all layers of original model and loaded  model. I found output of BatchNorm2d was different.</p><NewLine><p>The problems is Pytorch wasn’t saving ‘scale’ and ‘zero_point’ of unfused QuantizedBatchNorm in checkpoints. Two solutions -</p><NewLine><ul><NewLine><li>Save these values as pickle when saving model. While loading model, load pickle and add scale and zero point to  QuantizedBatchNorm layers.</li><NewLine><li>Fuse BatchNorm with Convolution.</li><NewLine></ul><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/deepak_mangla"">@deepak_mangla</a>, thanks for the report.  I created <a href=""https://github.com/pytorch/pytorch/issues/43774"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/43774</a> to verify correct behavior.  Please let us know if you have a repro on a toy model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/deepak_mangla; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: August 25, 2020,  1:18am; <NewLine> REPLY_DATE 2: August 28, 2020,  4:40pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
90990,Performance drop when quantizing Efficientnet,2020-07-29T16:46:29.722Z,9,293,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m trying to quantize a trained model of Efficientnet-Lite0, following the architectural changes detailed in this <a href=""https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html"" rel=""nofollow noopener"">blog post</a>.<br/><NewLine>I’m using the implementation from <a href=""https://github.com/rwightman/pytorch-image-models"" rel=""nofollow noopener"">this repo</a> and I get a significant accuracy drop (5-10%) after quantizing the model.<br/><NewLine>The full model after converting to 8-bit is:</p><NewLine><pre><code class=""lang-auto""> EfficientNet(<NewLine>  (conv_stem): ConvReLU6(<NewLine>    (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.36273476481437683, zero_point=57, padding=(1, 1))<NewLine>    (1): QuantizedReLU6(inplace=True)<NewLine>  )<NewLine>  (bn1): Identity()<NewLine>  (act1): Identity()<NewLine>  (blocks): Sequential(<NewLine>    (0): Sequential(<NewLine>      (0): DepthwiseSeparableConv(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.6822086572647095, zero_point=56, padding=(1, 1), groups=32)<NewLine>          (1): QuantizedReLU6(inplace=True)<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_pw): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.7673127055168152, zero_point=65)<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>      )<NewLine>    )<NewLine>    (1): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.5392391085624695, zero_point=60)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.322853684425354, zero_point=57, padding=(1, 1), groups=96)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.7627326250076294, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (1): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.8407724499702454, zero_point=62<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.3213047683238983, zero_point=63)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.267162948846817, zero_point=67, padding=(1, 1), groups=144)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.6916980743408203, zero_point=53)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>    (2): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.30310994386672974, zero_point=62)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), scale=0.20994137227535248, zero_point=61, padding=(2, 2), groups=144)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.6519036889076233, zero_point=65)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (1): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.7288376092910767, zero_point=63<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.20947812497615814, zero_point=52)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=0.24765455722808838, zero_point=83, padding=(2, 2), groups=240)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.4334663450717926, zero_point=61)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>    (3): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.20177333056926727, zero_point=56)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), scale=0.22160769999027252, zero_point=61, padding=(1, 1), groups=240)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.5097917914390564, zero_point=64)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (1): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.514493465423584, zero_point=64<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.15477867424488068, zero_point=47)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.19667555391788483, zero_point=82, padding=(1, 1), groups=480)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.2826884686946869, zero_point=64)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (2): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.5448680520057678, zero_point=65<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.12001236528158188, zero_point=67)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.1878129243850708, zero_point=79, padding=(1, 1), groups=480)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.23110872507095337, zero_point=61)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>    (4): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.20795781910419464, zero_point=51)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), scale=0.2575533390045166, zero_point=81, padding=(2, 2), groups=480)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), scale=0.5269572138786316, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (1): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.5629716515541077, zero_point=65<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.16619464755058289, zero_point=58)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=0.2228115200996399, zero_point=69, padding=(2, 2), groups=672)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=0.3241402208805084, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (2): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.642544686794281, zero_point=67<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.13504581153392792, zero_point=60)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=0.2062821239233017, zero_point=73, padding=(2, 2), groups=672)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=0.25870615243911743, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>    (5): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.16723443567752838, zero_point=66)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), scale=0.22132091224193573, zero_point=61, padding=(2, 2), groups=672)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.4806938171386719, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (1): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.49192753434181213, zero_point=64<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.1888679713010788, zero_point=51)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.2976231873035431, zero_point=83, padding=(2, 2), groups=1152)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.34456929564476013, zero_point=60)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (2): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.5567103624343872, zero_point=62<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.19077259302139282, zero_point=47)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.38248512148857117, zero_point=91, padding=(2, 2), groups=1152)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.2738204598426819, zero_point=65)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>      (3): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=0.6205083727836609, zero_point=62<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.15164275467395782, zero_point=59)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.29384535551071167, zero_point=80, padding=(2, 2), groups=1152)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.24689887464046478, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>    (6): Sequential(<NewLine>      (0): InvertedResidual(<NewLine>        (skip_add): QFunctional(<NewLine>          scale=1.0, zero_point=0<NewLine>          (activation_post_process): Identity()<NewLine>        )<NewLine>        (conv_pw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.20717555284500122, zero_point=64)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (act1): Identity()<NewLine>        (conv_dw): ConvReLU6(<NewLine>          (0): QuantizedConv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), scale=0.3554805517196655, zero_point=68, padding=(1, 1), groups=1152)<NewLine>          (1): QuantizedReLU6()<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (act2): Identity()<NewLine>        (conv_pwl): QuantizedConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.2588821351528168, zero_point=63)<NewLine>        (bn3): Identity()<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (conv_head): ConvReLU6(<NewLine>    (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.2839420437812805, zero_point=80)<NewLine>    (1): QuantizedReLU6(inplace=True)<NewLine>  )<NewLine>  (bn2): Identity()<NewLine>  (act2): Identity()<NewLine>  (global_pool): SelectAdaptivePool2d (output_size=1, pool_type=avg)<NewLine>  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)<NewLine>  (dequant): DeQuantize()<NewLine>  (classifier): QuantizedLinear(in_features=1280, out_features=1000, scale=0.14930474758148193, zero_point=34, qscheme=torch.per_channel_affine)<NewLine>)<NewLine></code></pre><NewLine><p>Is there anything I’m missing? I can provide the conversion code and other information if needed.</p><NewLine><p>Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/kfir_goldberg,(kfir goldberg),kfir_goldberg,"July 29, 2020,  4:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We just released Numeric Suite as prototype feature in PyTorch 1.6 to support quantization debugging, you can try it out to see which layer is problematic. The tutorial can be found at: <a href=""https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks!<br/><NewLine>I tried checking the quantization error for each of the layers and got the following:</p><NewLine><pre><code class=""lang-auto"">conv_stem.0.weight tensor(44.4819)<NewLine>blocks.0.0.conv_dw.0.weight tensor(45.0884)<NewLine>blocks.0.0.conv_pw.weight tensor(42.8196)<NewLine>blocks.1.0.conv_pw.0.weight tensor(43.1310)<NewLine>blocks.1.0.conv_dw.0.weight tensor(46.9183)<NewLine>blocks.1.0.conv_pwl.weight tensor(43.1703)<NewLine>blocks.1.1.conv_pw.0.weight tensor(44.4646)<NewLine>blocks.1.1.conv_dw.0.weight tensor(45.7783)<NewLine>blocks.1.1.conv_pwl.weight tensor(39.9211)<NewLine>blocks.2.0.conv_pw.0.weight tensor(44.0625)<NewLine>blocks.2.0.conv_dw.0.weight tensor(45.3749)<NewLine>blocks.2.0.conv_pwl.weight tensor(41.9430)<NewLine>blocks.2.1.conv_pw.0.weight tensor(43.8883)<NewLine>blocks.2.1.conv_dw.0.weight tensor(42.4965)<NewLine>blocks.2.1.conv_pwl.weight tensor(40.5602)<NewLine>blocks.3.0.conv_pw.0.weight tensor(43.9803)<NewLine>blocks.3.0.conv_dw.0.weight tensor(47.7440)<NewLine>blocks.3.0.conv_pwl.weight tensor(41.9959)<NewLine>blocks.3.1.conv_pw.0.weight tensor(43.2630)<NewLine>blocks.3.1.conv_dw.0.weight tensor(45.7537)<NewLine>blocks.3.1.conv_pwl.weight tensor(41.7492)<NewLine>blocks.3.2.conv_pw.0.weight tensor(43.5795)<NewLine>blocks.3.2.conv_dw.0.weight tensor(45.5840)<NewLine>blocks.3.2.conv_pwl.weight tensor(41.2215)<NewLine>blocks.4.0.conv_pw.0.weight tensor(42.7768)<NewLine>blocks.4.0.conv_dw.0.weight tensor(41.5424)<NewLine>blocks.4.0.conv_pwl.weight tensor(41.2056)<NewLine>blocks.4.1.conv_pw.0.weight tensor(43.2486)<NewLine>blocks.4.1.conv_dw.0.weight tensor(43.3677)<NewLine>blocks.4.1.conv_pwl.weight tensor(41.5483)<NewLine>blocks.4.2.conv_pw.0.weight tensor(43.2695)<NewLine>blocks.4.2.conv_dw.0.weight tensor(43.2045)<NewLine>blocks.4.2.conv_pwl.weight tensor(41.8538)<NewLine>blocks.5.0.conv_pw.0.weight tensor(42.5763)<NewLine>blocks.5.0.conv_dw.0.weight tensor(46.0717)<NewLine>blocks.5.0.conv_pwl.weight tensor(41.6060)<NewLine>blocks.5.1.conv_pw.0.weight tensor(42.4102)<NewLine>blocks.5.1.conv_dw.0.weight tensor(44.6428)<NewLine>blocks.5.1.conv_pwl.weight tensor(40.9154)<NewLine>blocks.5.2.conv_pw.0.weight tensor(42.4992)<NewLine>blocks.5.2.conv_dw.0.weight tensor(44.1465)<NewLine>blocks.5.2.conv_pwl.weight tensor(40.3739)<NewLine>blocks.5.3.conv_pw.0.weight tensor(42.2826)<NewLine>blocks.5.3.conv_dw.0.weight tensor(44.1184)<NewLine>blocks.5.3.conv_pwl.weight tensor(40.7068)<NewLine>blocks.6.0.conv_pw.0.weight tensor(42.2656)<NewLine>blocks.6.0.conv_dw.0.weight tensor(47.4642)<NewLine>blocks.6.0.conv_pwl.weight tensor(41.3921)<NewLine>conv_head.0.weight tensor(42.7725)<NewLine>classifier._packed_params._packed_params tensor(39.3391)<NewLine><NewLine></code></pre><NewLine><p>I’m not sure if these are large values or standard for this type of quantization, but it doesn’t seem that a specific layer is significantly worse than others.<br/><NewLine>Maybe it is something regarding the AdaptiveAvgPool2d?<br/><NewLine>I saw there were changes to it in the release notes.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I just ran the activation comparison suggested in the guide provided, and got:</p><NewLine><pre><code class=""lang-auto"">conv_stem.0.stats tensor(28.3666)<NewLine>conv_stem.1.stats tensor(28.3666)<NewLine>blocks.0.0.conv_dw.0.stats tensor(16.1361)<NewLine>blocks.0.0.conv_dw.1.stats tensor(16.1361)<NewLine>blocks.0.0.conv_pw.stats tensor(8.5438)<NewLine>blocks.1.0.conv_pw.0.stats tensor(7.0812)<NewLine>blocks.1.0.conv_pw.1.stats tensor(10.7929)<NewLine>blocks.1.0.conv_dw.0.stats tensor(10.3284)<NewLine>blocks.1.0.conv_dw.1.stats tensor(11.8796)<NewLine>blocks.1.0.conv_pwl.stats tensor(6.0492)<NewLine>blocks.1.1.conv_pw.0.stats tensor(9.7360)<NewLine>blocks.1.1.conv_pw.1.stats tensor(11.2618)<NewLine>blocks.1.1.conv_dw.0.stats tensor(8.9654)<NewLine>blocks.1.1.conv_dw.1.stats tensor(9.1349)<NewLine>blocks.1.1.conv_pwl.stats tensor(5.3888)<NewLine>blocks.2.0.conv_pw.0.stats tensor(8.9415)<NewLine>blocks.2.0.conv_pw.1.stats tensor(10.1787)<NewLine>blocks.2.0.conv_dw.0.stats tensor(12.5325)<NewLine>blocks.2.0.conv_dw.1.stats tensor(14.5331)<NewLine>blocks.2.0.conv_pwl.stats tensor(5.8452)<NewLine>blocks.2.1.conv_pw.0.stats tensor(10.9424)<NewLine>blocks.2.1.conv_pw.1.stats tensor(11.9166)<NewLine>blocks.2.1.conv_dw.0.stats tensor(10.7086)<NewLine>blocks.2.1.conv_dw.1.stats tensor(11.7042)<NewLine>blocks.2.1.conv_pwl.stats tensor(3.9516)<NewLine>blocks.3.0.conv_pw.0.stats tensor(7.9058)<NewLine>blocks.3.0.conv_pw.1.stats tensor(8.7798)<NewLine>blocks.3.0.conv_dw.0.stats tensor(13.6778)<NewLine>blocks.3.0.conv_dw.1.stats tensor(15.0221)<NewLine>blocks.3.0.conv_pwl.stats tensor(7.0661)<NewLine>blocks.3.1.conv_pw.0.stats tensor(11.2245)<NewLine>blocks.3.1.conv_pw.1.stats tensor(12.1855)<NewLine>blocks.3.1.conv_dw.0.stats tensor(10.3169)<NewLine>blocks.3.1.conv_dw.1.stats tensor(7.3186)<NewLine>blocks.3.1.conv_pwl.stats tensor(5.9016)<NewLine>blocks.3.2.conv_pw.0.stats tensor(10.9814)<NewLine>blocks.3.2.conv_pw.1.stats tensor(12.2782)<NewLine>blocks.3.2.conv_dw.0.stats tensor(11.5729)<NewLine>blocks.3.2.conv_dw.1.stats tensor(6.8540)<NewLine>blocks.3.2.conv_pwl.stats tensor(4.0227)<NewLine>blocks.4.0.conv_pw.0.stats tensor(9.5918)<NewLine>blocks.4.0.conv_pw.1.stats tensor(10.4552)<NewLine>blocks.4.0.conv_dw.0.stats tensor(11.8454)<NewLine>blocks.4.0.conv_dw.1.stats tensor(12.2951)<NewLine>blocks.4.0.conv_pwl.stats tensor(4.5780)<NewLine>blocks.4.1.conv_pw.0.stats tensor(9.8242)<NewLine>blocks.4.1.conv_pw.1.stats tensor(9.5439)<NewLine>blocks.4.1.conv_dw.0.stats tensor(12.6775)<NewLine>blocks.4.1.conv_dw.1.stats tensor(10.9211)<NewLine>blocks.4.1.conv_pwl.stats tensor(2.9198)<NewLine>blocks.4.2.conv_pw.0.stats tensor(9.9729)<NewLine>blocks.4.2.conv_pw.1.stats tensor(9.4751)<NewLine>blocks.4.2.conv_dw.0.stats tensor(14.5569)<NewLine>blocks.4.2.conv_dw.1.stats tensor(12.2109)<NewLine>blocks.4.2.conv_pwl.stats tensor(3.3256)<NewLine>blocks.5.0.conv_pw.0.stats tensor(10.7336)<NewLine>blocks.5.0.conv_pw.1.stats tensor(9.2929)<NewLine>blocks.5.0.conv_dw.0.stats tensor(19.4747)<NewLine>blocks.5.0.conv_dw.1.stats tensor(21.1074)<NewLine>blocks.5.0.conv_pwl.stats tensor(8.3158)<NewLine>blocks.5.1.conv_pw.0.stats tensor(12.8702)<NewLine>blocks.5.1.conv_pw.1.stats tensor(12.2446)<NewLine>blocks.5.1.conv_dw.0.stats tensor(14.1980)<NewLine>blocks.5.1.conv_dw.1.stats tensor(12.0078)<NewLine>blocks.5.1.conv_pwl.stats tensor(7.1764)<NewLine>blocks.5.2.conv_pw.0.stats tensor(13.4789)<NewLine>blocks.5.2.conv_pw.1.stats tensor(12.8941)<NewLine>blocks.5.2.conv_dw.0.stats tensor(15.1403)<NewLine>blocks.5.2.conv_dw.1.stats tensor(13.3021)<NewLine>blocks.5.2.conv_pwl.stats tensor(6.3677)<NewLine>blocks.5.3.conv_pw.0.stats tensor(13.3304)<NewLine>blocks.5.3.conv_pw.1.stats tensor(13.2739)<NewLine>blocks.5.3.conv_dw.0.stats tensor(16.0722)<NewLine>blocks.5.3.conv_dw.1.stats tensor(14.6379)<NewLine>blocks.5.3.conv_pwl.stats tensor(8.0309)<NewLine>blocks.6.0.conv_pw.0.stats tensor(12.9786)<NewLine>blocks.6.0.conv_pw.1.stats tensor(13.6662)<NewLine>blocks.6.0.conv_dw.0.stats tensor(16.3897)<NewLine>blocks.6.0.conv_dw.1.stats tensor(17.3638)<NewLine>blocks.6.0.conv_pwl.stats tensor(6.5583)<NewLine>conv_head.0.stats tensor(3.8746)<NewLine>conv_head.1.stats tensor(3.8746)<NewLine>quant.stats tensor(34.5170)<NewLine>classifier.stats tensor(6.9768)<NewLine></code></pre><NewLine><p>Is there anything suspicious here?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also calculated the cosine similarity of the activations in each of the layers:</p><NewLine><pre><code class=""lang-auto"">conv_stem.0.stats tensor(0.9992)<NewLine>conv_stem.1.stats tensor(0.9992)<NewLine>blocks.0.0.conv_dw.0.stats tensor(0.9882)<NewLine>blocks.0.0.conv_dw.1.stats tensor(0.9882)<NewLine>blocks.0.0.conv_pw.stats tensor(0.9376)<NewLine>blocks.1.0.conv_pw.0.stats tensor(0.9126)<NewLine>blocks.1.0.conv_pw.1.stats tensor(0.9569)<NewLine>blocks.1.0.conv_dw.0.stats tensor(0.9549)<NewLine>blocks.1.0.conv_dw.1.stats tensor(0.9677)<NewLine>blocks.1.0.conv_pwl.stats tensor(0.8856)<NewLine>blocks.1.1.conv_pw.0.stats tensor(0.9488)<NewLine>blocks.1.1.conv_pw.1.stats tensor(0.9625)<NewLine>blocks.1.1.conv_dw.0.stats tensor(0.9364)<NewLine>blocks.1.1.conv_dw.1.stats tensor(0.9385)<NewLine>blocks.1.1.conv_pwl.stats tensor(0.8623)<NewLine>blocks.2.0.conv_pw.0.stats tensor(0.9364)<NewLine>blocks.2.0.conv_pw.1.stats tensor(0.9518)<NewLine>blocks.2.0.conv_dw.0.stats tensor(0.9711)<NewLine>blocks.2.0.conv_dw.1.stats tensor(0.9819)<NewLine>blocks.2.0.conv_pwl.stats tensor(0.8685)<NewLine>blocks.2.1.conv_pw.0.stats tensor(0.9585)<NewLine>blocks.2.1.conv_pw.1.stats tensor(0.9671)<NewLine>blocks.2.1.conv_dw.0.stats tensor(0.9565)<NewLine>blocks.2.1.conv_dw.1.stats tensor(0.9647)<NewLine>blocks.2.1.conv_pwl.stats tensor(0.7922)<NewLine>blocks.3.0.conv_pw.0.stats tensor(0.9168)<NewLine>blocks.3.0.conv_pw.1.stats tensor(0.9344)<NewLine>blocks.3.0.conv_dw.0.stats tensor(0.9773)<NewLine>blocks.3.0.conv_dw.1.stats tensor(0.9831)<NewLine>blocks.3.0.conv_pwl.stats tensor(0.8967)<NewLine>blocks.3.1.conv_pw.0.stats tensor(0.9597)<NewLine>blocks.3.1.conv_pw.1.stats tensor(0.9683)<NewLine>blocks.3.1.conv_dw.0.stats tensor(0.9532)<NewLine>blocks.3.1.conv_dw.1.stats tensor(0.8986)<NewLine>blocks.3.1.conv_pwl.stats tensor(0.8574)<NewLine>blocks.3.2.conv_pw.0.stats tensor(0.9549)<NewLine>blocks.3.2.conv_pw.1.stats tensor(0.9664)<NewLine>blocks.3.2.conv_dw.0.stats tensor(0.9599)<NewLine>blocks.3.2.conv_dw.1.stats tensor(0.8697)<NewLine>blocks.3.2.conv_pwl.stats tensor(0.7916)<NewLine>blocks.4.0.conv_pw.0.stats tensor(0.9387)<NewLine>blocks.4.0.conv_pw.1.stats tensor(0.9521)<NewLine>blocks.4.0.conv_dw.0.stats tensor(0.9650)<NewLine>blocks.4.0.conv_dw.1.stats tensor(0.9685)<NewLine>blocks.4.0.conv_pwl.stats tensor(0.8268)<NewLine>blocks.4.1.conv_pw.0.stats tensor(0.9460)<NewLine>blocks.4.1.conv_pw.1.stats tensor(0.9414)<NewLine>blocks.4.1.conv_dw.0.stats tensor(0.9698)<NewLine>blocks.4.1.conv_dw.1.stats tensor(0.9566)<NewLine>blocks.4.1.conv_pwl.stats tensor(0.7595)<NewLine>blocks.4.2.conv_pw.0.stats tensor(0.9490)<NewLine>blocks.4.2.conv_pw.1.stats tensor(0.9423)<NewLine>blocks.4.2.conv_dw.0.stats tensor(0.9809)<NewLine>blocks.4.2.conv_dw.1.stats tensor(0.9683)<NewLine>blocks.4.2.conv_pwl.stats tensor(0.7715)<NewLine>blocks.5.0.conv_pw.0.stats tensor(0.9567)<NewLine>blocks.5.0.conv_pw.1.stats tensor(0.9359)<NewLine>blocks.5.0.conv_dw.0.stats tensor(0.9930)<NewLine>blocks.5.0.conv_dw.1.stats tensor(0.9949)<NewLine>blocks.5.0.conv_pwl.stats tensor(0.9064)<NewLine>blocks.5.1.conv_pw.0.stats tensor(0.9649)<NewLine>blocks.5.1.conv_pw.1.stats tensor(0.9595)<NewLine>blocks.5.1.conv_dw.0.stats tensor(0.9741)<NewLine>blocks.5.1.conv_dw.1.stats tensor(0.9583)<NewLine>blocks.5.1.conv_pwl.stats tensor(0.8771)<NewLine>blocks.5.2.conv_pw.0.stats tensor(0.9700)<NewLine>blocks.5.2.conv_pw.1.stats tensor(0.9648)<NewLine>blocks.5.2.conv_dw.0.stats tensor(0.9810)<NewLine>blocks.5.2.conv_dw.1.stats tensor(0.9709)<NewLine>blocks.5.2.conv_pwl.stats tensor(0.8566)<NewLine>blocks.5.3.conv_pw.0.stats tensor(0.9687)<NewLine>blocks.5.3.conv_pw.1.stats tensor(0.9679)<NewLine>blocks.5.3.conv_dw.0.stats tensor(0.9842)<NewLine>blocks.5.3.conv_dw.1.stats tensor(0.9755)<NewLine>blocks.5.3.conv_pwl.stats tensor(0.8879)<NewLine>blocks.6.0.conv_pw.0.stats tensor(0.9637)<NewLine>blocks.6.0.conv_pw.1.stats tensor(0.9655)<NewLine>blocks.6.0.conv_dw.0.stats tensor(0.9802)<NewLine>blocks.6.0.conv_dw.1.stats tensor(0.9842)<NewLine>blocks.6.0.conv_pwl.stats tensor(0.8535)<NewLine>conv_head.0.stats tensor(0.6853)<NewLine>conv_head.1.stats tensor(0.6853)<NewLine>quant.stats tensor(0.9998)<NewLine>classifier.stats tensor(0.7695)<NewLine></code></pre><NewLine><p>it seems as if the conv_pwl layers are mostly different, along with the conv_head and classifier.<br/><NewLine>What might cause that?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/kfir_goldberg"">@kfir_goldberg</a> Did you solve it? I’m planing to quantize an EfficientNet-Lite0 for smartphone deployment. I chose EfficientNet-Lite as it seems to be quantization friendly, or at least, it’s what google claims.</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Unfortunately, I didn’t solve it. One of the issues that bothered me is that fusing Conv-Bn-Act is not possible with Relu6 as of now so I had to implement it myself, and I’m unsure if it worked right.<br/><NewLine>The best I managed to do is get about a 5% drop in accuracy (75% to 70%), and I eventually stopped trying.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""90990"" data-username=""kfir_goldberg""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kfir_goldberg/40/27275_2.png"" width=""20""/> kfir_goldberg:</div><NewLine><blockquote><NewLine><p>Unfortunately, I didn’t solve it. One of the issues that bothered me is that fusing Conv-Bn-Act is not possible with Relu6 as of now so I had to implement it myself, and I’m unsure if it worked right.</p><NewLine></blockquote><NewLine></aside><NewLine><p>we typically replace relu6 with relu, e.g.: <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/mobilenet.py#L75"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/mobilenet.py#L75</a></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>also did you try quantization aware training? <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training</a></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> I don’t see how replacing relu6 with relu doesn’t translate to a performance drop. It may work with MobileNetV2 but it isn’t a general approach.</p><NewLine><p>Also, I don’t understand why Pytorch doesn’t offer <code>ConvReLU2d</code> similar modules for relu6 and hardswish. They are a common pattern for nets that runs on low end or IoT devices like smartphones. At least, a <code>ConvReLU6_2d</code>  for relu6 as it’s like a relu.</p><NewLine><p>Moreover, I think it’s worth to add both patterns as like <a href=""https://arxiv.org/pdf/2003.13678.pdf"" rel=""nofollow noopener"">Regnet paper</a> says  “<em>We find that Swish outperforms ReLU at low flops, but ReLU is better at highflops.   Interestingly,  if g is  restricted  to  be  1  (depthwiseconv), Swish performs much better than ReLU. This sugggests that depthwise conv and Swish interact favorably, although the underlying reason is not at all clear.</em>” So, for lower flops and quantized network you may want to use hardswish instead of relu as activation.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, I’m not sure why we don’t support ConvReLU6_2d, maybe <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> knows.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/kfir_goldberg"">@kfir_goldberg</a>,<br/><NewLine>I’m about quantize Efficientnet-lite as well. Could you please share the conversion code? Maybe I’ll found sth. That would save me some time of doing conversion so that I could concentrate just to get similar accuracy for both versions.</p><NewLine><p>Thanks<br/><NewLine>Tomek</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is the code I used to support ConvBnReLU6 and ConvReLU6</p><NewLine><pre><code class=""lang-auto"">def fuse_model(model):<NewLine>    for m in model.modules():<NewLine>        if type(m) == DepthwiseSeparableConv:<NewLine>            torch.quantization.fuse_modules(m, ['conv_dw', 'bn1', 'act1'], inplace=True,<NewLine>                                            fuser_func=fuse_known_modules_mod)<NewLine>            # torch.quantization.fuse_modules(m, ['conv_pw', 'bn2', 'act2'], inplace=True,<NewLine>            torch.quantization.fuse_modules(m, ['conv_pw', 'bn2'], inplace=True,<NewLine>                                            fuser_func=fuse_known_modules_mod)<NewLine>        elif type(m) == InvertedResidual:<NewLine>            torch.quantization.fuse_modules(m, ['conv_pw', 'bn1', 'act1'], inplace=True,<NewLine>                                            fuser_func=fuse_known_modules_mod)<NewLine>            torch.quantization.fuse_modules(m, ['conv_dw', 'bn2', 'act2'], inplace=True,<NewLine>                                            fuser_func=fuse_known_modules_mod)<NewLine>            torch.quantization.fuse_modules(m, ['conv_pwl', 'bn3'], inplace=True, fuser_func=fuse_known_modules_mod)<NewLine>    torch.quantization.fuse_modules(model, ['conv_head', 'bn2', 'act2'], inplace=True,<NewLine>                                    fuser_func=fuse_known_modules_mod)<NewLine>    torch.quantization.fuse_modules(model, ['conv_stem', 'bn1', 'act1'], inplace=True,<NewLine>                                    fuser_func=fuse_known_modules_mod)<NewLine><NewLine><NewLine>def fuse_known_modules_mod(mod_list):<NewLine>    r""""""Returns a list of modules that fuses the operations specified<NewLine>     in the input module list.<NewLine>    Fuses only the following sequence of modules:<NewLine>    conv, bn<NewLine>    conv, bn, relu<NewLine>    conv, relu<NewLine>    linear, relu<NewLine>    For these sequences, the first element in the output module list performs<NewLine>    the fused operation. The rest of the elements are set to nn.Identity()<NewLine>    """"""<NewLine><NewLine>    OP_LIST_TO_FUSER_METHOD = {<NewLine>        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn,<NewLine>        (torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.ReLU): fuse_conv_bn_relu,<NewLine>        (torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.ReLU6): fuse_conv_bn_relu6,<NewLine>        (Conv2dSame, torch.nn.BatchNorm2d, torch.nn.ReLU6): fuse_conv_bn_relu6,<NewLine>        (torch.nn.Conv2d, torch.nn.ReLU): torch.nn.intrinsic.ConvReLU2d,<NewLine>        (torch.nn.Conv2d, torch.nn.ReLU6): ConvReLU6,<NewLine>        (torch.nn.Linear, torch.nn.ReLU): torch.nn.intrinsic.LinearReLU<NewLine>    }<NewLine><NewLine>    types = tuple(type(m) for m in mod_list)<NewLine>    fuser_method = OP_LIST_TO_FUSER_METHOD.get(types, None)<NewLine>    if fuser_method is None:<NewLine>        raise NotImplementedError(""Cannot fuse modules: {}"".format(types))<NewLine>    new_mod = [None] * len(mod_list)<NewLine>    new_mod[0] = fuser_method(*mod_list)<NewLine><NewLine>    for i in range(1, len(mod_list)):<NewLine>        new_mod[i] = torch.nn.Identity()<NewLine>        new_mod[i].training = mod_list[0].training<NewLine><NewLine>    return new_mod<NewLine><NewLine><NewLine>class ConvReLU6(nn.Sequential):<NewLine>    def __init__(self, conv, relu6):<NewLine>        super(ConvReLU6, self).__init__(conv, relu6)<NewLine><NewLine><NewLine>class ConvBnReLU6(torch.nn.Sequential):<NewLine>    def __init__(self, conv, bn, relu6):<NewLine>        super(ConvBnReLU6, self).__init__(conv, bn, relu6)<NewLine><NewLine><NewLine>def fuse_conv_bn_relu6(conv, bn, relu6):<NewLine>    assert (conv.training == bn.training == relu6.training), \<NewLine>        ""Conv and BN both must be in the same mode (train or eval).""<NewLine><NewLine>    if conv.training:<NewLine>        return ConvBnReLU6(conv, bn, relu6)<NewLine>    else:<NewLine>        return ConvReLU6(<NewLine>            torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn), relu6)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/kfir_goldberg"">@kfir_goldberg</a>,<br/><NewLine>do I understand correctly that you’ve created a new model class that inherited from timm.models.efficientnet_lite0 and 1) placed your fuse_model() there 2) modified forward method to add quantization support? Can you share code of this class? I’m not sure where exactly to put quant/dequant.</p><NewLine><p>Then you made:</p><NewLine><pre><code class=""lang-auto"">model_.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>torch.quantization.prepare(model_, inplace=True)<NewLine></code></pre><NewLine><p>and finally</p><NewLine><pre><code class=""lang-auto"">evaluate(model_, criterion, train_loader, neval_batches=num_calibration_batches)<NewLine>torch.quantization.convert(model_, inplace=True)<NewLine></code></pre><NewLine><p>to calibrate the model with training set and convert, right?</p><NewLine><p>I’m asking because when calibrating model (evaluate) I got:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU].<NewLine></code></pre><NewLine><p>and I found <a href=""https://github.com/pytorch/pytorch/issues/34583"" rel=""nofollow noopener"">here</a> that this is probably connected with the fact that QuantStub is not placed in the right place.</p><NewLine><p>Have you followed <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#"" rel=""nofollow noopener"">this</a> tutorial? They say that when performance drops per channel quantization may be needed or Quantization-aware training.</p><NewLine><p>Many thanks!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/vferrer; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/vferrer; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Tomek; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Tomek; <NewLine> ,"REPLY_DATE 1: July 30, 2020,  6:56am; <NewLine> REPLY_DATE 2: July 30, 2020,  7:48am; <NewLine> REPLY_DATE 3: July 31, 2020,  2:34pm; <NewLine> REPLY_DATE 4: July 31, 2020,  3:39pm; <NewLine> REPLY_DATE 5: August 19, 2020,  6:10pm; <NewLine> REPLY_DATE 6: August 20, 2020,  6:12pm; <NewLine> REPLY_DATE 7: August 21, 2020, 10:31pm; <NewLine> REPLY_DATE 8: August 21, 2020, 10:33pm; <NewLine> REPLY_DATE 9: August 24, 2020, 10:07am; <NewLine> REPLY_DATE 10: August 24, 2020, 11:54pm; <NewLine> REPLY_DATE 11: August 26, 2020,  1:24pm; <NewLine> REPLY_DATE 12: August 26, 2020,  2:34pm; <NewLine> REPLY_DATE 13: August 27, 2020, 11:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> 
93601,Qnnpack accuracy very poor on unet model,2020-08-21T08:07:14.122Z,3,116,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using Unet model for semantic segmentation. I pass a batch of images to the model. The model is expected to output 0 or 1 for each pixel of the image (depending upon whether pixel is part of person object or not). 0 is for background, and 1 is for foreground.</p><NewLine><p>I am trying to quantize the Unet model with Pytorch quantization apis (static quantization). The model accuracy is good for FBGEMM config. However, the model outputs all pixels as black pixels (background pixels) for all images. The model output is very high positive for background channel and very high negative for forground channel. (I perform softmax to get final classification of the pixel). The code works fine for FBGEMM. Same code produces very different results for QNNPACK. Is there anything missing for QNNPACK code? I am pasting my code below for reference.</p><NewLine><pre><code class=""lang-auto""># Static Quantization - FBGEMM/QNNPACK<NewLine><NewLine>import torch.quantization as Q<NewLine>import torch<NewLine><NewLine>framework = 'qnnpack'   # change to fbgemm for x86 architecture<NewLine>per_channel_quantized_model = model<NewLine>per_channel_quantized_model.eval()<NewLine>per_channel_quantized_model.fuse_model()<NewLine>per_channel_quantized_model.qconfig = Q.get_default_qconfig(framework)<NewLine>torch.backends.quantized.engine = framework<NewLine><NewLine>print(""Preparing . . ."")<NewLine>Q.prepare(per_channel_quantized_model, inplace=True)<NewLine><NewLine>print(""Running model . . ."")<NewLine>eval_model_for_quantization(per_channel_quantized_model, 'cpu')<NewLine><NewLine>print(""Converting . . ."")<NewLine>Q.convert(per_channel_quantized_model, inplace=True)<NewLine><NewLine>print(""***************"")<NewLine>print(per_channel_quantized_model)<NewLine><NewLine>print(""***************"")<NewLine>print(""Checking Accuracy . . ."")<NewLine>accuracy = eval_model_for_quantization(per_channel_quantized_model, 'cpu')<NewLine>print()<NewLine>print('Evaluation accuracy after quantization', accuracy)<NewLine></code></pre><NewLine><p>One issue that I did encounter is that the upsampling layers of Unet use nn.ConvTranspose2d which is not supported for quantization. Hence before this layer, we need to dequantize tensors, apply nn.ConvTranspose2d, and then requantize for subsequent layers. Can this be reason for lower accuracy?</p><NewLine><p>NOTE -  I did try with QAT for QNNPACK. However, model output does not change i.e. it gives out all black pixels.</p><NewLine></div>",https://discuss.pytorch.org/u/amitdedhia,(Amit),amitdedhia,"August 21, 2020,  9:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here I have more details…</p><NewLine><p>Following is the model output for some inputs. The LHS bracket is target value. Values in RHS are output values for 2 channels (background and foreground). The RHS values are passed to a Softmax function and the final result is obtained (…and to be compared with target)</p><NewLine><p>Model output before quantization:</p><NewLine><pre><code class=""lang-auto""> ..........(1.0) (1.42 16.16)<NewLine> ..........(1.0) (-40.55 42.14)<NewLine> ..........(0.0) (15.20 -19.15)<NewLine> ..........(1.0) (-21.16 25.58)<NewLine> ..........(1.0) (-43.54 41.77)<NewLine> ..........(0.0) (19.74 -23.29)<NewLine> ..........(1.0) (-29.66 33.56)<NewLine> ..........(1.0) (1.23 -7.96)<NewLine> ..........(1.0) (-35.54 42.13)<NewLine> ..........(0.0) (16.74 -19.38)<NewLine> ..........(0.0) (9.40 -2.54)<NewLine> ..........(0.0) (21.67 -27.59)<NewLine> ..........(1.0) (-52.96 53.53)<NewLine> ..........(0.0) (18.02 -20.90)<NewLine> ..........(1.0) (-19.79 22.51)<NewLine> ..........(0.0) (13.33 -20.11)<NewLine> ..........(0.0) (29.95 -31.26)<NewLine> ..........(0.0) (23.35 -29.38)<NewLine> ..........(1.0) (-15.23 9.97)<NewLine> ..........(0.0) (18.14 -24.80)<NewLine> ..........(0.0) (19.13 -26.98)<NewLine> ..........(1.0) (-18.12 22.96)<NewLine></code></pre><NewLine><p>Model output after FBGEMM quantization - as you can see below, the output values did change, but only to small extent</p><NewLine><pre><code class=""lang-auto"">..........(1.0) (0.00 18.41)<NewLine> ..........(1.0) (-45.41 50.32)<NewLine> ..........(0.0) (13.50 -14.73)<NewLine> ..........(1.0) (-24.55 30.69)<NewLine> ..........(1.0) (-39.28 38.05)<NewLine> ..........(0.0) (13.50 -17.18)<NewLine> ..........(1.0) (-22.09 25.78)<NewLine> ..........(1.0) (2.45 -7.36)<NewLine> ..........(1.0) (-23.32 29.46)<NewLine> ..........(0.0) (17.18 -23.32)<NewLine> ..........(0.0) (12.27 -6.14)<NewLine> ..........(0.0) (20.87 -23.32)<NewLine> ..........(1.0) (-45.41 49.10)<NewLine> ..........(0.0) (15.96 -18.41)<NewLine> ..........(1.0) (-17.18 20.87)<NewLine> ..........(0.0) (11.05 -18.41)<NewLine> ..........(0.0) (27.00 -27.00)<NewLine> ..........(0.0) (17.18 -23.32)<NewLine> ..........(1.0) (-2.45 1.23)<NewLine> ..........(0.0) (15.96 -20.87)<NewLine> ..........(0.0) (18.41 -24.55)<NewLine> ..........(1.0) (-15.96 20.87)<NewLine></code></pre><NewLine><p>Now look at following model output for QNNPACK quantization. The output is very different from the unquantized version. In particular, values for all pixels is positive for background channel and negative for foreground channel.</p><NewLine><pre><code class=""lang-auto"">..........(1.0) (14.06 -17.12)<NewLine> ..........(1.0) (11.61 -16.51)<NewLine> ..........(0.0) (20.17 -25.06)<NewLine> ..........(1.0) (18.34 -22.01)<NewLine> ..........(1.0) (15.89 -14.06)<NewLine> ..........(0.0) (20.17 -25.67)<NewLine> ..........(1.0) (22.62 -29.34)<NewLine> ..........(1.0) (24.45 -28.73)<NewLine> ..........(1.0) (14.06 -20.17)<NewLine> ..........(0.0) (22.62 -28.12)<NewLine> ..........(0.0) (27.51 -20.17)<NewLine> ..........(0.0) (21.40 -23.84)<NewLine> ..........(1.0) (20.78 -29.34)<NewLine> ..........(0.0) (17.12 -23.23)<NewLine> ..........(1.0) (28.73 -31.18)<NewLine> ..........(0.0) (18.34 -20.78)<NewLine> ..........(0.0) (20.17 -23.84)<NewLine> ..........(0.0) (20.78 -23.84)<NewLine> ..........(1.0) (13.45 -16.51)<NewLine> ..........(0.0) (17.12 -21.40)<NewLine> ..........(0.0) (21.40 -25.67)<NewLine> ..........(1.0) (21.40 -26.90)<NewLine></code></pre><NewLine><p>Any thoughts by anyone?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you set the qengine to qnnpack before you evaluate the model? you can set the qengine with <a href=""https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_quantized.py#L110"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_quantized.py#L110</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Jerry - thanks for the reply.</p><NewLine><p>Yes I have done this already. You can see following code line in my first post…</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""93601"" data-username=""amitdedhia""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/a/e68b1a/40.png"" width=""20""/> amitdedhia:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">torch.backends.quantized.engine = framework<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>is the code okay? Am I doing anything wrong?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. quantization code looks correct. cc <a class=""mention"" href=""/u/supriyar"">@supriyar</a> <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> could you take a look</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><strong>Update</strong> - Earlier when I worked on QAT, I was using wrong config. After correcting it, the QAT helps improve the accuracy. However I am still interested in knowing (in case of static quantization for qnnpack config)  why the output value for all pixels is positive for background channel and negative for foreground channel.</p><NewLine><p>The model I am using is available here:<br/><NewLine>source - <a href=""https://github.com/thuyngch/Human-Segmentation-PyTorch"" rel=""nofollow noopener"">https://github.com/thuyngch/Human-Segmentation-PyTorch</a><br/><NewLine>model file - <a href=""https://drive.google.com/file/d/17GZLCi_FHhWo4E4wPobbLAQdBZrlqVnF/view"" rel=""nofollow noopener"">https://drive.google.com/file/d/17GZLCi_FHhWo4E4wPobbLAQdBZrlqVnF/view</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/amitdedhia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/amitdedhia; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/amitdedhia; <NewLine> ,"REPLY_DATE 1: August 21, 2020,  8:41am; <NewLine> REPLY_DATE 2: August 21, 2020, 10:49pm; <NewLine> REPLY_DATE 3: August 24, 2020,  5:40am; <NewLine> REPLY_DATE 4: August 24, 2020, 11:51pm; <NewLine> REPLY_DATE 5: August 25, 2020,  2:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
91609,Quantisation aware training LSTM with pack_padded_sequences?,2020-08-04T13:03:19.538Z,0,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Using pytorch 1.6.<br/><NewLine>I’m trying to implement qat on a lstm based model I have.</p><NewLine><pre><code class=""lang-auto"">class Net(torch.nn.Module):<NewLine>    def __init__(self, seq_length):<NewLine>        super(Net, self).__init__()<NewLine><NewLine>        self.hidden_size = 16<NewLine>        self.input_size = 18<NewLine><NewLine>        self.seq_length = seq_length<NewLine><NewLine>        self.relu1 = torch.nn.ReLU()<NewLine>        # Need to specify input sizes up front<NewLine><NewLine>        # batch_first specifies an input shape of (nBatches, nSeq, nFeatures),<NewLine>        # otherwise this is (nSeq, nBatch, nFeatures)<NewLine>        self.lstm = torch.nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, batch_first = True)<NewLine>        self.linear1 = torch.nn.Linear(self.hidden_size, self.hidden_size)<NewLine>        self.dropout = torch.nn.Dropout(0.5)        #self.squeeze = torch.squeeze<NewLine>        self.linearOut = torch.nn.Linear(self.hidden_size, 1)<NewLine>        self.sigmoidOut = torch.nn.Sigmoid()<NewLine>        self.sqeeze1 = torch.Tensor.squeeze<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    def forward(self, x):<NewLine><NewLine>        # Can pass the initial hidden state, but not necessary here<NewLine>        #x, h = self.gru1(x)#, self.h0)<NewLine>        #x, h  = self.gru(x)#, self.h0)<NewLine>        x, (h,c) = self.lstm(x)#, self.h0)<NewLine><NewLine>        # Get last output, x[:,l - 1,:], equivalent to (last) hidden state<NewLine>        # Squeeze to remove length 1 dim<NewLine>        x = self.sqeeze1(h)<NewLine><NewLine>        x = self.dropout(x)<NewLine><NewLine>        x = self.linear1(x)<NewLine>        x = self.relu1(x)<NewLine>        x = self.linearOut(x)<NewLine><NewLine>        # Apply sigmoid either in the loss function, or in eval(...)<NewLine>        return x<NewLine>    def evaluate(self,x):<NewLine>        return self.sigmoidOut(self.forward(x))<NewLine></code></pre><NewLine><p>Standard training works fine, and after preparing the model for qat with,</p><NewLine><pre><code class=""lang-auto"">qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>print(qat_model.qconfig)<NewLine>qat_model = torch.quantization.prepare_qat(qat_model)<NewLine>print(qat_model)<NewLine><NewLine></code></pre><NewLine><p>I’m running the same training loop, just with different learning rates.</p><NewLine><pre><code class=""lang-auto"">for epoch in tqdm(range(qat_epochs)):<NewLine>    #model.train()<NewLine>    for batch in range(qat_nBatches):<NewLine>        start_time = time.time()<NewLine>        batch_data = data[batch * batch_size : (batch + 1) * batch_size]<NewLine>        batch_seq_lens = seq_lens[batch * batch_size : (batch + 1) * batch_size]<NewLine>        batch_labels = labels[batch * batch_size : (batch + 1) * batch_size]<NewLine>        packedData = pack_padded_sequence(batch_data,<NewLine>                                          batch_seq_lens,<NewLine>                                          batch_first = True,<NewLine>                                          enforce_sorted = False)<NewLine>        output = qat_model(packedData)<NewLine>        loss = lossF(output, batch_labels)<NewLine>        optimizer.zero_grad()<NewLine>        loss.backward()<NewLine>        optimizer.step()<NewLine>        pred = qat_model.evaluate(packedData).detach().cpu().numpy().flatten()<NewLine>    predClasses = np.zeros(pred.shape)<NewLine>    predClasses[pred &gt; 0.5] = 1<NewLine><NewLine>    losses.append(loss.detach().cpu().numpy())<NewLine><NewLine>    accuracy.append(accuracy_score(batch_labels.detach().cpu().numpy().flatten(), predClasses))<NewLine><NewLine>    packedDataTest = pack_padded_sequence(data[data.shape[0] // 2:],<NewLine>                                          seq_lens[data.shape[0] // 2:],<NewLine>                                          batch_first = True,<NewLine>                                          enforce_sorted = False)<NewLine><NewLine>    labelsTest = labels[data.shape[0] // 2:]<NewLine><NewLine>    quantised_model = torch.quantization.convert(qat_model.eval(), inplace = False)<NewLine>    predTestT = qat_model.evaluate(packedDataTest)<NewLine>    predTest = predTestT.detach().cpu().numpy().flatten()<NewLine>    predClassesTest = np.zeros(predTest.shape)<NewLine>    predClassesTest[predTest &gt; 0.5] = 1<NewLine><NewLine>    lossesTestQAT.append(lossF(predTestT, labelsTest).detach().cpu().numpy().flatten())<NewLine>    accuracyTestQAT.append(accuracy_score(labelsTest.detach().cpu().numpy().flatten(), predClassesTest))<NewLine></code></pre><NewLine><p>However, I get this error</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""rnn_qat.py"", line 307, in &lt;module&gt;<NewLine>    output = qat_model(packedData)<NewLine>  File ""/mnt/storage/home/rs17751/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""rnn_qat.py"", line 59, in forward<NewLine>    x, (h,c) = self.lstm(x)#, self.h0)<NewLine>  File ""/mnt/storage/home/rs17751/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 726, in _call_impl<NewLine>    hook_result = hook(self, input, result)<NewLine>  File ""/mnt/storage/home/rs17751/.local/lib/python3.7/site-packages/torch/quantization/quantize.py"", line 74, in _observer_forward_hook<NewLine>    return self.activation_post_process(output)<NewLine>  File ""/mnt/storage/home/rs17751/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/mnt/storage/home/rs17751/.local/lib/python3.7/site-packages/torch/quantization/fake_quantize.py"", line 91, in forward<NewLine>    self.activation_post_process(X.detach())<NewLine>AttributeError: 'tuple' object has no attribute 'detach'<NewLine><NewLine></code></pre><NewLine><p>At first I thought this was to do with the LSTM outputting tuples, as I had to change from GRU to LSTM for quantisation. But, if that was the problem, surely the normal training loop would fail in the same way. Any help is appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Robin-Simmons,(Robin Simmons),Robin-Simmons,"August 4, 2020,  1:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think qat is supported for LSTM. cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/supriyar"">@supriyar</a> to confirm</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That’s right, we currently do not support QAT for nn.LSTM. We support dynamic quantization of LSTM modules currently</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: August 21, 2020, 10:15pm; <NewLine> REPLY_DATE 2: August 25, 2020, 12:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
93514,Custom quantized Linear or Conv2d layer,2020-08-20T16:16:46.563Z,1,69,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear Users,</p><NewLine><p>I would like to ask for pointers for how to extend <code>nn.Linear</code> and <code>nn.Conv2d</code> for post-training static quantization or quantization-aware training without rewriting a lot of stuff, such that it can still be used with operator fusion etc… An example change could be to apply an affine transformation to the <em>weights</em> prior to calling the linear operation. Could someone help please? Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/martinferianc,(Martin Ferianc),martinferianc,"August 20, 2020,  4:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>the change in eager mode quantization will require inheriting <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L103"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L103</a> and also related fusion modules under <a href=""https://github.com/pytorch/pytorch/tree/master/torch/nn/intrinsic"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/torch/nn/intrinsic</a> folder, and pass a white_list(<a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L178"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L178</a>) extended with the new module. It will require familiarity of the whole eager mode quantization flow.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""93514"" data-username=""jerryzh168""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jerryzh168/40/15217_2.png"" width=""20""/> jerryzh168:</div><NewLine><blockquote><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L103"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L103</a></p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks Jerry, this is what I initially thought, but I wanted to double-check if my assumption was right. Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/martinferianc; <NewLine> ,"REPLY_DATE 1: August 22, 2020,  1:38pm; <NewLine> REPLY_DATE 2: August 22, 2020,  1:39pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
91849,Initializing QAT with pre-trained quantization parameters,2020-08-06T11:56:03.965Z,1,85,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I apologize if this question is covered elsewhere.</p><NewLine><p>I would like to perform quantization-aware training, but with the model initialized according to the pre-trained, post-training-quantized quantization parameters (e.g., a torchvision quantized model with layers initialized with the same scale, zero_point, etc. as in the pre-trained quantization model that is initialized with <code>quantize=True</code>).</p><NewLine><p>That is, I’d like the initial model used for QAT to produce the same output as a pre-trained model that has been quantized using a post-training method (e.g., static quantization).</p><NewLine><p>Is there an easy way to achieve this? I had tried hacking manually setting some of the QAT model’s FakeQuantizer parameters, but was unable to get it working properly.</p><NewLine><p>I appreciate any help! Please let me know if my question is unclear and I will rephrase it.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/jkosaian,,jkosaian,"August 6, 2020, 11:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think that is supported right now, instead if you have access to the original floating point model you can just do qat with that. you’ll get the model with same accuracy as the post training model if you call prepare_qat for the original model and calibrate it with the same data.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got it. Thanks for the response!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jkosaian; <NewLine> ,"REPLY_DATE 1: August 24, 2020, 11:47pm; <NewLine> REPLY_DATE 2: August 22, 2020, 12:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
88975,"RuntimeError: Could not run &lsquo;aten::native_batch_norm&rsquo; with arguments from the &lsquo;QuantizedCPUTensorId&rsquo; backend. &lsquo;aten::native_batch_norm&rsquo; is only available for these backends: [CPUTensorId, MkldnnCPUTensorId, VariableTensorId]",2020-07-13T10:27:11.281Z,14,390,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a quantized model which is basically a <code>resnet18</code>. the quantization seems to go just fine until, when I try to load the quantized model from disk using sth like this :</p><NewLine><pre><code class=""lang-python"">def load_quantized(quantized_checkpoint_file_path):<NewLine>    model = fvmodels.resnet18(pretrained=False, use_se=True)<NewLine>    model.eval()<NewLine>    model.fuse_model()<NewLine>    # print(f'model: {model}')<NewLine>    # Specify quantization configuration<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    # print(model.qconfig)<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    checkpoint = torch.load(quantized_checkpoint_file_path, map_location=torch.device('cpu'))<NewLine>    model.load_state_dict(checkpoint, strict=False)<NewLine>    # model = torch.jit.load(quantized_checkpoint_file_path, map_location=torch.device('cpu'))<NewLine>    fvmodels.print_size_of_model(model)<NewLine>    return model<NewLine></code></pre><NewLine><p>and while trying to use that :</p><NewLine><pre><code class=""lang-python"">model = load_quantized('path to model')<NewLine>model.eval()<NewLine>with torch.no_grad():<NewLine>    for img, lbl in dtloader:<NewLine>        features = model(img.unsqueeze(0))<NewLine></code></pre><NewLine><p>I face the following error :</p><NewLine><pre><code class=""lang-python"">RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::native_batch_norm' is only available for these backends: [CPUTensorId, MkldnnCPUTensorId, VariableTensorId].<NewLine></code></pre><NewLine><p>This seems to be casued by the fact that the batchnorm layer is not fused! and the issue is I dont know how to fuse it. to be more specific here is the resnet model I have at hand :</p><NewLine><pre><code class=""lang-python"">class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, use_se=True):<NewLine>        self.inplanes = 64<NewLine>        self.use_se = use_se<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64)<NewLine>        # self.prelu = nn.PReLU()<NewLine>        self.prelu = nn.ReLU()<NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(512)<NewLine>        self.dropout = nn.Dropout()<NewLine>        self.fc = nn.Linear(512 * 7 * 7, 512)<NewLine>        self.bn3 = nn.BatchNorm1d(512)<NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))<NewLine>        self.inplanes = planes<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, use_se=self.use_se))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        <NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.prelu(x)<NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.bn2(x)<NewLine>        x = self.dropout(x)<NewLine>        # x = x.view(x.size(0), -1)<NewLine>        x = x.reshape(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        x = self.bn3(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine><NewLine>        fuse_modules(self, [['conv1', 'bn1', 'prelu'],<NewLine>                            ['bn2'],<NewLine>                            ['bn3']], inplace=True)<NewLine>        for m in self.modules():<NewLine>            # print(m)<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine></code></pre><NewLine><p>as you can see in the forward pass we have :</p><NewLine><pre><code class=""lang-python"">...<NewLine>x = self.bn2(x)<NewLine>x = self.dropout(x)<NewLine></code></pre><NewLine><p>which is followed by a dropout and unlike previous ones, doesnt come with neither conv or relu!<br/><NewLine>the same thing goes to <code>bn3</code> a couple of lines later:</p><NewLine><pre><code class=""lang-python"">...<NewLine>x = self.fc(x)<NewLine>x = self.bn3(x)<NewLine>x = self.dequant(x)<NewLine>....<NewLine></code></pre><NewLine><p>So I’m not sure how I’m supposed to get around this. obviously the way I’m fusing is wrong:</p><NewLine><pre><code class=""lang-python"">def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'prelu'],<NewLine>                            ['bn2'],<NewLine>                            ['bn3']], inplace=True)<NewLine>        for m in self.modules():<NewLine>            # print(m)<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine></code></pre><NewLine><p>For the sake of completeness here are the whole models :</p><NewLine><pre><code class=""lang-python""><NewLine>def conv3x3(in_planes, out_planes, stride=1):<NewLine>    """"""3x3 convolution with padding""""""<NewLine>    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,<NewLine>                     padding=1, bias=False)<NewLine><NewLine>class BasicBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super().__init__()<NewLine>        self.conv1 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv2 = conv3x3(planes, planes)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        <NewLine>        self.add_relu = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine><NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.add_relu.add_relu(out, residual)<NewLine><NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],<NewLine>                                               ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class Bottleneck(nn.Module):<NewLine>    expansion = 4<NewLine><NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super(Bottleneck, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,<NewLine>                               padding=1, bias=False)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)<NewLine>        self.bn3 = nn.BatchNorm2d(planes * 4)<NewLine>        self.relu1 = nn.ReLU(inplace=False)<NewLine>        self.relu2 = nn.ReLU(inplace=False)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        <NewLine>        self.skip_add_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu1(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        out = self.relu2(out)<NewLine><NewLine>        out = self.conv3(out)<NewLine>        out = self.bn3(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.skip_add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'relu1'],<NewLine>                            ['conv2', 'bn2', 'relu2'],<NewLine>                            ['conv3', 'bn3']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine><NewLine>        self.fc = nn.Sequential(<NewLine>            nn.Linear(channel, channel // reduction),<NewLine>            # nn.PReLU(),<NewLine>            nn.ReLU(),<NewLine>            nn.Linear(channel // reduction, channel),<NewLine>            nn.Sigmoid()<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        y = self.fc(y).view(b, c, 1, 1)<NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine><NewLine>class IRBlock(nn.Module):<NewLine>    expansion = 1<NewLine><NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):<NewLine>        super().__init__()<NewLine>        self.bn0 = nn.BatchNorm2d(inplanes)<NewLine>        self.conv1 = conv3x3(inplanes, inplanes)<NewLine>        self.bn1 = nn.BatchNorm2d(inplanes)<NewLine>        # self.prelu = nn.PReLU()<NewLine>        self.prelu = nn.ReLU()<NewLine>        self.conv2 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.use_se = use_se<NewLine>        if self.use_se:<NewLine>            self.se = SEBlock(planes)<NewLine><NewLine>        self.add_residual_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.bn0(x)<NewLine>        out = self.conv1(out)<NewLine>        out = self.bn1(out)<NewLine>        out = self.prelu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.use_se:<NewLine>            out = self.se(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine><NewLine>        # out += residual<NewLine>        # out = self.prelu(out)<NewLine><NewLine>        # we may need to change prelu into relu and this, instead of add, use add_relu here<NewLine>        out = self.add_residual_relu.add_relu(out, residual)<NewLine>        # out = self.prelu(out)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'prelu'],<NewLine>                            ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, use_se=True):<NewLine>        self.inplanes = 64<NewLine>        self.use_se = use_se<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64)<NewLine>        # self.prelu = nn.PReLU()<NewLine>        self.prelu = nn.ReLU()<NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(512)<NewLine>        self.dropout = nn.Dropout()<NewLine>        self.fc = nn.Linear(512 * 7 * 7, 512)<NewLine>        self.bn3 = nn.BatchNorm1d(512)<NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))<NewLine>        self.inplanes = planes<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, use_se=self.use_se))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        <NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.prelu(x)<NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.bn2(x)<NewLine>        x = self.dropout(x)<NewLine>        # x = x.view(x.size(0), -1)<NewLine>        x = x.reshape(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        x = self.bn3(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine><NewLine>        fuse_modules(self, [['conv1', 'bn1', 'prelu'],<NewLine>                            ['bn2'],<NewLine>                            ['bn3']], inplace=True)<NewLine>        for m in self.modules():<NewLine>            # print(m)<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine><NewLine>def resnet18(pretrained, use_se, **kwargs):<NewLine>    model = ResNet(IRBlock, [2, 2, 2, 2], use_se=use_se, **kwargs)<NewLine>    if pretrained:<NewLine>        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))<NewLine>    return model<NewLine><NewLine></code></pre><NewLine><p><strong>side note:</strong><br/><NewLine>Also the actual model (resnet18 can be found from this <a href=""https://github.com/foamliu/InsightFace-v2/releases"" rel=""nofollow noopener"">link</a> in case someone might need it)<br/><NewLine><strong>Config:</strong><br/><NewLine>Im using Pytorch 1.5.0+cpu on windows 10 x64 v1803</p><NewLine><p>Any help is greatly appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"July 15, 2020,  6:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">fuse_modules(self, [['conv1', 'bn1', 'prelu'],<NewLine>                            ['bn2'],<NewLine>                            ['bn3']], inplace=True)<NewLine></code></pre><NewLine><p>change into：<br/><NewLine>fuse_modules(self, [‘conv1’, ‘bn1’, ‘prelu’], inplace=True)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks but, that was the initial attempt which results in the mentioned error as well.</p><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 265, in &lt;module&gt;<NewLine>    features = model(img.unsqueeze(0))<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 418, in forward<NewLine>    x = self.bn3(x)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\batchnorm.py"", line 106, in forward<NewLine>    exponential_average_factor, self.eps)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\functional.py"", line 1923, in batch_norm<NewLine>    training, momentum, eps, torch.backends.cudnn.enabled<NewLine>RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::native_batch_norm' is only available for these backends: [CPUTensorId, MkldnnCPUTensorId, VariableTensorId].<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you try printing the quantized model after prepare and convert? We do support quantized batch_norm so nn.BatchNorm2d module should get replaced with quantized one.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, here it is :<br/><NewLine>Ran using the latest nighly <strong>1.7.0.dev20200714+cpu</strong> and <strong>torchvision-0.8.0.dev20200714+cpu</strong></p><NewLine><pre><code class=""lang-python"">Size (MB): 87.218199<NewLine>QConfig(activation=functools.partial(&lt;class 'torch.quantization.observer.HistogramObserver'&gt;, reduce_range=True), weight=functools.partial(&lt;class 'torch.quantization.observer.PerChannelMinMaxObserver'&gt;, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))<NewLine>Model after being fused-prepared: ResNet(<NewLine>  (conv1): Conv2d(<NewLine>    3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>    (activation_post_process): HistogramObserver()<NewLine>  )<NewLine>  (bn1): Identity()<NewLine>  (prelu): PReLU(num_parameters=1)<NewLine>  (prelu_q): PReLU_Quantized(<NewLine>    (quantized_op): FloatFunctional(<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>    (quant): QuantStub(<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>    (dequant): DeQuantStub()<NewLine>  )<NewLine>  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<NewLine>  (layer1): Sequential(<NewLine>    (0): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (conv2): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=64, out_features=4, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=4, out_features=64, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>        (prelu_q): PReLU_Quantized(<NewLine>          (quantized_op): FloatFunctional(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (quant): QuantStub(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (dequant): DeQuantStub()<NewLine>        )<NewLine>        (fc_q): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU_Quantized(<NewLine>            (quantized_op): FloatFunctional(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (quant): QuantStub(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (dequant): DeQuantStub()<NewLine>          )<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>    (1): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (conv2): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=64, out_features=4, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=4, out_features=64, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>        (prelu_q): PReLU_Quantized(<NewLine>          (quantized_op): FloatFunctional(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (quant): QuantStub(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (dequant): DeQuantStub()<NewLine>        )<NewLine>        (fc_q): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU_Quantized(<NewLine>            (quantized_op): FloatFunctional(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (quant): QuantStub(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (dequant): DeQuantStub()<NewLine>          )<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (layer2): Sequential(<NewLine>    (0): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (conv2): Conv2d(<NewLine>        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (downsample): Sequential(<NewLine>        (0): Conv2d(<NewLine>          64, 128, kernel_size=(1, 1), stride=(2, 2)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): Identity()<NewLine>      )<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=128, out_features=8, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=8, out_features=128, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>        (prelu_q): PReLU_Quantized(<NewLine>          (quantized_op): FloatFunctional(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (quant): QuantStub(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (dequant): DeQuantStub()<NewLine>        )<NewLine>        (fc_q): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU_Quantized(<NewLine>            (quantized_op): FloatFunctional(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (quant): QuantStub(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (dequant): DeQuantStub()<NewLine>          )<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>    (1): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): Conv2d(<NewLine>        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (conv2): Conv2d(<NewLine>        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=128, out_features=8, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=8, out_features=128, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>        (prelu_q): PReLU_Quantized(<NewLine>          (quantized_op): FloatFunctional(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (quant): QuantStub(<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (dequant): DeQuantStub()<NewLine>        )<NewLine>        (fc_q): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU_Quantized(<NewLine>            (quantized_op): FloatFunctional(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (quant): QuantStub(<NewLine>              (activation_post_process): HistogramObserver()<NewLine>            )<NewLine>            (dequant): DeQuantStub()<NewLine>          )<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (layer3): Sequential(<NewLine>    (0): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        128, eps=1e-05, momentum=0.<NewLine></code></pre><NewLine><p>and for the sake of completeness, here are  the whole modules used :</p><NewLine><details><NewLine><summary><NewLine>Summary</summary><NewLine><pre><code class=""lang-python"">class PReLU_Quantized(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine>        self.quantized_op = nn.quantized.FloatFunctional()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        # inputs = max(0, inputs) + alpha * min(0, inputs) <NewLine>        # this is how we do it <NewLine>        # pos = torch.relu(inputs)<NewLine>        # neg = -alpha * torch.relu(-inputs)<NewLine>        # res3 = pos + neg<NewLine>        self.weight = self.quant(self.weight)<NewLine>        weight_min_res = self.quantized_op.mul(-self.weight, torch.relu(-inputs))<NewLine>        inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>        inputs = self.dequant(inputs)<NewLine>        self.weight = self.dequant(self.weight)<NewLine>        return inputs<NewLine><NewLine>def conv3x3(in_planes, out_planes, stride=1):<NewLine>    """"""3x3 convolution with padding""""""<NewLine>    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,<NewLine>                     padding=1, bias=False)<NewLine><NewLine>class BasicBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super().__init__()<NewLine>        self.conv1 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv2 = conv3x3(planes, planes)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        <NewLine>        self.add_relu = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine><NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.add_relu.add_relu(out, residual)<NewLine><NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],<NewLine>                                               ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class Bottleneck(nn.Module):<NewLine>    expansion = 4<NewLine><NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super(Bottleneck, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,<NewLine>                               padding=1, bias=False)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)<NewLine>        self.bn3 = nn.BatchNorm2d(planes * 4)<NewLine>        self.relu1 = nn.ReLU(inplace=False)<NewLine>        self.relu2 = nn.ReLU(inplace=False)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        <NewLine>        self.skip_add_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu1(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        out = self.relu2(out)<NewLine><NewLine>        out = self.conv3(out)<NewLine>        out = self.bn3(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.skip_add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'relu1'],<NewLine>                            ['conv2', 'bn2', 'relu2'],<NewLine>                            ['conv3', 'bn3']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine><NewLine>        self.fc = nn.Sequential(<NewLine>                                nn.Linear(channel, channel // reduction),<NewLine>                                nn.PReLU(),<NewLine>                                # nn.ReLU(),<NewLine>                                nn.Linear(channel // reduction, channel),<NewLine>                                nn.Sigmoid()<NewLine>                                )<NewLine>        self.fc1 = self.fc[0]<NewLine>        self.prelu = self.fc[1]<NewLine>        self.fc2 = self.fc[2]<NewLine>        self.sigmoid = self.fc[3]<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine><NewLine>    def forward(self, x):<NewLine>        print(f'&lt;inside se forward:&gt;')<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        # y = self.fc(y).view(b, c, 1, 1)<NewLine>        y = self.fc1(y)<NewLine>        print(f'X: {y}')<NewLine>        y = self.prelu_q(y)<NewLine>        y = self.fc2(y)<NewLine>        y = self.sigmoid(y).view(b, c, 1, 1)<NewLine>        print('--------------------------')<NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine><NewLine>class IRBlock(nn.Module):<NewLine>    expansion = 1<NewLine><NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):<NewLine>        super().__init__()<NewLine>        self.bn0 = nn.BatchNorm2d(inplanes)<NewLine>        self.conv1 = conv3x3(inplanes, inplanes)<NewLine>        self.bn1 = nn.BatchNorm2d(inplanes)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        # self.prelu = nn.ReLU()<NewLine>        self.conv2 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.use_se = use_se<NewLine>        if self.use_se:<NewLine>            self.se = SEBlock(planes)<NewLine><NewLine>        self.add_residual_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine><NewLine>        out = self.bn0(x)<NewLine>        out = self.conv1(out)<NewLine>        out = self.bn1(out)<NewLine><NewLine>        # out = self.prelu(out)<NewLine>        out = self.prelu_q(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.use_se:<NewLine>            out = self.se(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine><NewLine>        # out += residual<NewLine>        # out = self.prelu(out)<NewLine><NewLine>        # we may need to change prelu into relu and this, instead of add, use add_relu here<NewLine>        out = self.add_residual_relu.add_relu(out, residual)<NewLine>        # out = self.prelu(out)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1'],# 'prelu'],<NewLine>                            ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, use_se=True):<NewLine>        self.inplanes = 64<NewLine>        self.use_se = use_se<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        # self.prelu = nn.ReLU()<NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(512)<NewLine>        self.dropout = nn.Dropout()<NewLine>        self.fc = nn.Linear(512 * 7 * 7, 512)<NewLine>        self.bn3 = nn.BatchNorm1d(512)<NewLine><NewLine>        # self.bn2_q = BatchNorm2d_Quantized(self.bn2)<NewLine>        # self.bn3_q = BatchNorm1d_Quantized(self.bn3)<NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))<NewLine>        self.inplanes = planes<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, use_se=self.use_se))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        <NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine><NewLine>        # x = self.prelu(x)<NewLine>        x = self.prelu_q(x)<NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.bn2(x)<NewLine>        # x = self.bn2_q(x)<NewLine>        x = self.dropout(x)<NewLine>        # x = x.view(x.size(0), -1)<NewLine>        x = x.reshape(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        x = self.bn3(x)<NewLine>        # x = self.bn3_q(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine><NewLine>        fuse_modules(self, [['conv1', 'bn1'],# 'prelu'],<NewLine>                            # ['bn2'],  ['bn3']<NewLine>                            ], inplace=True)<NewLine>        for m in self.modules():<NewLine>            # print(m)<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine><NewLine>def resnet18(pretrained, use_se, **kwargs):<NewLine>    model = ResNet(IRBlock, [2, 2, 2, 2], use_se=use_se, **kwargs)<NewLine>    if pretrained:<NewLine>        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))<NewLine>    return model<NewLine></code></pre><NewLine></details><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is another sample, this is the model output when I removed all <code>PReLU</code>s and used <code>ReLU</code>s isntead(incase it was a hinderance):</p><NewLine><pre><code class=""lang-python"">Size (MB): 87.205847<NewLine>QConfig(activation=functools.partial(&lt;class 'torch.quantization.observer.HistogramObserver'&gt;, reduce_range=True), weight=functools.partial(&lt;class 'torch.quantization.observer.PerChannelMinMaxObserver'&gt;, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))<NewLine>Model after quantization(converted-prepared): ResNet(<NewLine>  (conv1): ConvReLU2d(<NewLine>    (0): Conv2d(<NewLine>      3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>    (1): ReLU(<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>  )<NewLine>  (bn1): Identity()<NewLine>  (prelu): PReLU(num_parameters=1)<NewLine>  (prelu_q): PReLU_Quantized(<NewLine>    (quantized_op): FloatFunctional(<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>    (quant): QuantStub(<NewLine>      (activation_post_process): HistogramObserver()<NewLine>    )<NewLine>    (dequant): DeQuantStub()<NewLine>  )<NewLine>  (reluooo): Identity()<NewLine>  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<NewLine>  (layer1): Sequential(<NewLine>    (0): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): ConvReLU2d(<NewLine>        (0): Conv2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): ReLU(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (reluooo): Identity()<NewLine>      (conv2): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=64, out_features=4, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=4, out_features=64, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>    (1): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): ConvReLU2d(<NewLine>        (0): Conv2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): ReLU(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (reluooo): Identity()<NewLine>      (conv2): Conv2d(<NewLine>        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=64, out_features=4, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=4, out_features=64, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=64, out_features=4, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=4, out_features=64, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (layer2): Sequential(<NewLine>    (0): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): ConvReLU2d(<NewLine>        (0): Conv2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): ReLU(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (reluooo): Identity()<NewLine>      (conv2): Conv2d(<NewLine>        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (downsample): Sequential(<NewLine>        (0): Conv2d(<NewLine>          64, 128, kernel_size=(1, 1), stride=(2, 2)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): Identity()<NewLine>      )<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=128, out_features=8, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=8, out_features=128, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>    (1): IRBlock(<NewLine>      (bn0): BatchNorm2d(<NewLine>        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (conv1): ConvReLU2d(<NewLine>        (0): Conv2d(<NewLine>          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (1): ReLU(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>      )<NewLine>      (bn1): Identity()<NewLine>      (prelu): PReLU(num_parameters=1)<NewLine>      (prelu_q): PReLU_Quantized(<NewLine>        (quantized_op): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (quant): QuantStub(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (dequant): DeQuantStub()<NewLine>      )<NewLine>      (reluooo): Identity()<NewLine>      (conv2): Conv2d(<NewLine>        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>      (bn2): Identity()<NewLine>      (se): SEBlock(<NewLine>        (avg_pool): AdaptiveAvgPool2d(output_size=1)<NewLine>        (mult_xy): FloatFunctional(<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (fc): Sequential(<NewLine>          (0): Linear(<NewLine>            in_features=128, out_features=8, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (1): PReLU(num_parameters=1)<NewLine>          (2): Linear(<NewLine>            in_features=8, out_features=128, bias=True<NewLine>            (activation_post_process): HistogramObserver()<NewLine>          )<NewLine>          (3): Sigmoid()<NewLine>        )<NewLine>        (fc1): Linear(<NewLine>          in_features=128, out_features=8, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (prelu): PReLU(num_parameters=1)<NewLine>        (fc2): Linear(<NewLine>          in_features=8, out_features=128, bias=True<NewLine>          (activation_post_process): HistogramObserver()<NewLine>        )<NewLine>        (sigmoid): Sigmoid()<NewLine>      )<NewLine>      (add_residual_relu): FloatFunctional(<NewLine>        (activation_post_process): HistogramObserver()<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>  (layer3): Sequential(<NewLine>    (0)<NewLine>Post Training Quantization Prepare: Inserting Observers<NewLine><NewLine> Inverted Residual Block:After observer insertion<NewLine><NewLine> ConvReLU2d(<NewLine>  (0): Conv2d(<NewLine>    3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>    (activation_post_process): HistogramObserver()<NewLine>  )<NewLine>  (1): ReLU(<NewLine>    (activation_post_process): HistogramObserver()<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>This is the error I get when using this model (above):</p><NewLine><pre><code class=""lang-python"">--------------------------<NewLine>Traceback (most recent call last):<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 270, in &lt;module&gt;<NewLine>    test_the_model(True)<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 218, in test_the_model<NewLine>    check_and_tell(model, pic1, pic2)<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 203, in check_and_tell<NewLine>    embd1 = model(img1.unsqueeze(0))<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 599, in forward<NewLine>    x = self.bn3(x)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\batchnorm.py"", line 136, in forward<NewLine>    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\functional.py"", line 2039, in batch_norm<NewLine>    training, momentum, eps, torch.backends.cudnn.enabled<NewLine>RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPU' backend. 'aten::native_batch_norm' is only available for these backends: [CPU, MkldnnCPU, BackendSelect, Named, Autograd, Profiler, Tracer, Autocast, Batched].<NewLine><NewLine>CPU: registered at aten\src\ATen\CPUType.cpp:1594 [kernel]<NewLine>MkldnnCPU: registered at aten\src\ATen\MkldnnCPUType.cpp:139 [kernel]<NewLine>BackendSelect: fallthrough registered at ..\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]<NewLine>Named: registered at ..\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]<NewLine>Autograd: registered at ..\torch\csrc\autograd\generated\VariableType_0.cpp:7879 [kernel]<NewLine>Profiler: registered at ..\torch\csrc\autograd\generated\ProfiledType_0.cpp:2050 [kernel]<NewLine>Tracer: registered at ..\torch\csrc\autograd\generated\TraceType_0.cpp:8256 [kernel]<NewLine>Autocast: fallthrough registered at ..\aten\src\ATen\autocast_mode.cpp:375 [backend fallback]<NewLine>Batched: registered at ..\aten\src\ATen\BatchingRegistrations.cpp:149 [backend fallback]<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried commenting out self.bn(x), and the code ran through. Is there any other solution to this problem?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, but thats not a solution to me, removing bn drastically affects the performance and aside from that, <a class=""mention"" href=""/u/supriyar"">@supriyar</a> says Pytorch has a quantized version of BatchNorm in place and it should have got converted in first place!<br/><NewLine>So its not known what is missing or what else needs to be done.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can replace Linear with a 1*1 convolutional layer, and then merge the convolutional layer and bn layer. I have tried and solved my problem</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, but the problem is, I have other instances of BN where they are used alone! one instance is in <code>IRBlock</code> where the first layer is bn!<br/><NewLine>So I need to fix this properly</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can try to add a 1*1 convolutional layer before the bn layer</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>That way I have to retrain the model as the 1x1 weights are uninitialized</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""88975"" data-username=""Shisho_Sama""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/shisho_sama/40/6926_2.png"" width=""20""/> Shisho_Sama:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">  (conv1): Conv2d(<NewLine>    3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>    (activation_post_process): HistogramObserver()<NewLine>  )<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Looking at the first conv of your model after convert, it doesn’t seem like it is actually quantized (it should be QuantizedConv), same for the subsequent modules.  One thing to debug would be why the modules are not getting replaced.</p><NewLine><p>Are you calling <code>model.eval()</code> before running convert?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Thanks, but no that model is not yet <code>converted</code>, what is printed up there, is just the output after running <code>fuse_model()</code> and then <code>torch.quantization.prepare</code>. if I comment out the single bns, (and also repalce PReLUs to not face the current issues), the final model does get quantized (its size becomes 22Mb from 88Mb and you see the QuantizedConv2d, etc as well.</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>the error message looks like you are trying to pass a quantized input to BN, but BN is not quantized.  So, you’d need to either fuse it to the preceding module, quantize it, or make sure the input is converted to floating point.  Here is a toy example of expected behavior:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>class M(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(M, self).__init__()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.conv1 = nn.Conv2d(1, 1, 1)<NewLine>        self.bn1 = nn.BatchNorm2d(1)<NewLine>        self.conv2 = nn.Conv2d(1, 1, 1)<NewLine>        self.bn2 = nn.BatchNorm2d(1)<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.conv2(x)<NewLine>        x = self.bn2(x)<NewLine>        return x<NewLine>    <NewLine>m = M()<NewLine>m.qconfig = torch.quantization.default_qconfig<NewLine>m.eval()<NewLine><NewLine>torch.quantization.fuse_modules(<NewLine>    m, <NewLine>    [<NewLine>        ['conv1', 'bn1'], # fuse bn1 into conv1<NewLine>        # for example's sake, don't fuse conv2 and bn2<NewLine>    ],<NewLine>    inplace=True)<NewLine><NewLine>torch.quantization.prepare(m, inplace=True)<NewLine><NewLine># toy calibration<NewLine>data = torch.randn(32, 1, 16, 16)<NewLine>m(data)<NewLine><NewLine>torch.quantization.convert(m, inplace=True)<NewLine># self.bn1 was fused with conv1 earlier<NewLine># self.bn2 will be QuantizedBatchNorm2d<NewLine>print(m)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot. good point. on a normal model this looks alright, but in the self contained example I made, this doesnt apply,.<br/><NewLine>Here have a look :<br/><NewLine>Here is a self contained example with Resnet18 and SimpleNetwork ( a simple 2 layered CNN) using fake data to demonstrate the problem. You can change the use_relu and disable_single_bns to see different results:</p><NewLine><pre><code class=""lang-python"">import os<NewLine>from os.path import abspath, dirname, join<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.utils.data import DataLoader<NewLine>from torchvision.datasets import FakeData<NewLine>import torchvision.transforms as transforms<NewLine>from torch.quantization import fuse_modules<NewLine><NewLine>use_relu = False<NewLine>disable_single_bns = False<NewLine><NewLine>class PReLU_Quantized(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine>        self.quantized_op = nn.quantized.FloatFunctional()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        # inputs = max(0, inputs) + alpha * min(0, inputs) <NewLine>        # this is how we do it <NewLine>        # pos = torch.relu(inputs)<NewLine>        # neg = -alpha * torch.relu(-inputs)<NewLine>        # res3 = pos + neg<NewLine>        self.weight = self.quant(self.weight)<NewLine>        weight_min_res = self.quantized_op.mul(-self.weight, torch.relu(-inputs))<NewLine>        inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>        inputs = self.dequant(inputs)<NewLine>        self.weight = self.dequant(self.weight)<NewLine>        return inputs<NewLine><NewLine>def conv3x3(in_planes, out_planes, stride=1):<NewLine>    """"""3x3 convolution with padding""""""<NewLine>    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,<NewLine>                     padding=1, bias=False)<NewLine><NewLine>class BasicBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super().__init__()<NewLine>        self.conv1 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv2 = conv3x3(planes, planes)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.add_relu = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],<NewLine>                                               ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class Bottleneck(nn.Module):<NewLine>    expansion = 4<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super(Bottleneck, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)<NewLine>        self.bn3 = nn.BatchNorm2d(planes * 4)<NewLine>        self.relu1 = nn.ReLU(inplace=False)<NewLine>        self.relu2 = nn.ReLU(inplace=False)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.skip_add_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu1(out)<NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        out = self.relu2(out)<NewLine>        out = self.conv3(out)<NewLine>        out = self.bn3(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.skip_add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'relu1'],<NewLine>                            ['conv2', 'bn2', 'relu2'],<NewLine>                            ['conv3', 'bn3']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine>        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction),<NewLine>                                nn.PReLU(),<NewLine>                                nn.Linear(channel // reduction, channel),<NewLine>                                nn.Sigmoid())<NewLine>        self.fc1 = self.fc[0]<NewLine>        self.prelu = self.fc[1]<NewLine>        self.fc2 = self.fc[2]<NewLine>        self.sigmoid = self.fc[3]<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>            self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>    def forward(self, x):<NewLine>        # print(f'&lt;inside se forward:&gt;')<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        # y = self.fc(y).view(b, c, 1, 1)<NewLine>        y = self.fc1(y)<NewLine>        y = self.prelu_q_or_relu(y)<NewLine>        y = self.fc2(y)<NewLine>        y = self.sigmoid(y).view(b, c, 1, 1)<NewLine>        # print('--------------------------')<NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine><NewLine>class IRBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):<NewLine>        super().__init__()<NewLine>        self.bn0 = nn.BatchNorm2d(inplanes)<NewLine>        if disable_single_bns:<NewLine>            self.bn0_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn0_or_identity = self.bn0<NewLine><NewLine>        self.conv1 = conv3x3(inplanes, inplanes)<NewLine>        self.bn1 = nn.BatchNorm2d(inplanes)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        <NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>            self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>        self.conv2 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.use_se = use_se<NewLine>        # if self.use_se:<NewLine>        self.se = SEBlock(planes)<NewLine>        self.add_residual = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        # TODO:<NewLine>        # this needs to be quantized as well!<NewLine>        out = self.bn0_or_identity(x)<NewLine><NewLine>        out = self.conv1(out)<NewLine>        out = self.bn1(out)<NewLine>        # out = self.prelu(out)<NewLine>        out = self.prelu_q_or_relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.use_se:<NewLine>            out = self.se(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.prelu(out)<NewLine>        out = self.prelu_q_or_relu(out)<NewLine>        # we may need to change prelu into relu and instead of add, use add_relu here<NewLine>        out = self.add_residual.add(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [# ['bn0'],<NewLine>                            ['conv1', 'bn1'],<NewLine>                            ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, use_se=True):<NewLine>        self.inplanes = 64<NewLine>        self.use_se = use_se<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        # This is to only get rid of the unimplemented CPUQuantization type error<NewLine>        # when we use PReLU_Quantized during test time<NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>             self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(512)<NewLine>        # This is to get around the single BatchNorms not getting fused and thus causing <NewLine>        # a RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPU' backend.<NewLine>        # 'aten::native_batch_norm' is only available for these backends: [CPU, MkldnnCPU, BackendSelect, Named, Autograd, Profiler, Tracer, Autocast, Batched].<NewLine>        # during test time<NewLine>        if disable_single_bns:<NewLine>            self.bn2_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn2_or_identity = self.bn2<NewLine><NewLine>        self.dropout = nn.Dropout()<NewLine>        self.fc = nn.Linear(512 * 7 * 7, 512)<NewLine>        self.bn3 = nn.BatchNorm1d(512)<NewLine>        if disable_single_bns:<NewLine>            self.bn3_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn3_or_identity = self.bn3<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))<NewLine>        self.inplanes = planes<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, use_se=self.use_se))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        <NewLine>        x = self.quant(x)<NewLine>        x = self.conv1(x)<NewLine>        # TODO: single bn needs to be fused<NewLine>        x = self.bn1(x)<NewLine><NewLine>        # x = self.prelu(x)<NewLine>        x = self.prelu_q_or_relu(x)<NewLine><NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.bn2_or_identity(x)<NewLine>        x = self.dropout(x)<NewLine>        # x = x.view(x.size(0), -1)<NewLine>        x = x.reshape(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        # TODO: single bn needs to be fused<NewLine>        x = self.bn3_or_identity(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine>        fuse_modules(self, ['conv1', 'bn1'], inplace=True)<NewLine>        for m in self.modules():<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine><NewLine>class SimpleNetwork(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        self.prelu_q = PReLU_Quantized(nn.PReLU())<NewLine>        self.bn = nn.BatchNorm2d(10)<NewLine><NewLine>        self.prelu_q_or_relu = torch.relu if use_relu else self.prelu_q<NewLine>        self.bn_or_identity = nn.Identity() if disable_single_bns else self.bn    <NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu1(x)<NewLine><NewLine>        x = self.prelu_q_or_relu(x)<NewLine>        x = self.bn_or_identity(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>def resnet18(use_se=True, **kwargs):<NewLine>    return ResNet(IRBlock, [2, 2, 2, 2], use_se=use_se, **kwargs)<NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"")/1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>def evaluate(model, data_loader, eval_batches):<NewLine>    model.eval()<NewLine>    with torch.no_grad():<NewLine>        for i, (image, target) in enumerate(data_loader):<NewLine>            features = model(image)<NewLine>            print(f'{i})feature dims: {features.shape}')<NewLine>            if i &gt;= eval_batches:<NewLine>                return<NewLine><NewLine>def load_quantized(model, quantized_checkpoint_file_path):<NewLine>    model.eval()<NewLine>    if type(model) == ResNet:<NewLine>        model.fuse_model()<NewLine>    # Specify quantization configuration<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    checkpoint = torch.load(quantized_checkpoint_file_path, map_location=torch.device('cpu'))<NewLine>    model.load_state_dict(checkpoint)<NewLine>    print_size_of_model(model)<NewLine>    return model<NewLine><NewLine>def test_the_model(model, dtloader):<NewLine>    current_dir = abspath(dirname(__file__))<NewLine>    model = load_quantized(model, join(current_dir, 'data', 'model_quantized_jit.pth'))<NewLine>    model.eval()<NewLine>    img, _ = next(iter(dtloader))<NewLine>    embd1 = model(img)<NewLine><NewLine>def quantize_model(model, dtloader):<NewLine>    calibration_batches = 10 <NewLine>    saved_model_dir = 'data'<NewLine>    scripted_quantized_model_file = 'model_quantized_jit.pth'<NewLine>    # model = resnet18()<NewLine>    model.eval()<NewLine>    if type(model) == ResNet:<NewLine>        model.fuse_model()<NewLine>    print_size_of_model(model)<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    print(model.qconfig)<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine><NewLine>    print(f'Model after fusion(prepared): {model}')<NewLine><NewLine>    # Calibrate first<NewLine>    print('Post Training Quantization Prepare: Inserting Observers')<NewLine>    print('\n Inverted Residual Block:After observer insertion \n\n', model.conv1)<NewLine><NewLine>    # Calibrate with the training set<NewLine>    evaluate(model, dtloader, eval_batches=calibration_batches)<NewLine>    print('Post Training Quantization: Calibration done')<NewLine><NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    print('Post Training Quantization: Convert done')<NewLine>    print('\n Inverted Residual Block: After fusion and quantization, note fused modules: \n\n', model.conv1)<NewLine><NewLine>    print(""Size of model after quantization"")<NewLine>    print_size_of_model(model)<NewLine>    script = torch.jit.script(model)<NewLine>    path_tosave = join(dirname(abspath(__file__)), saved_model_dir, scripted_quantized_model_file)<NewLine>    print(f'path to save: {path_tosave}')<NewLine>    with open(path_tosave, 'wb') as f:<NewLine>        torch.save(model.state_dict(), f)<NewLine><NewLine>    print(f'model after quantization (prepared and converted:) {model}')<NewLine>    # torch.jit.save(script, path_tosave)<NewLine><NewLine>dataset = FakeData(1000, image_size=(3, 112, 112), num_classes=5, transform=transforms.ToTensor())<NewLine>data_loader = DataLoader(dataset, batch_size=1)<NewLine><NewLine># quantize the model <NewLine>model = resnet18()<NewLine># model = SimpleNetwork()<NewLine>quantize_model(model, data_loader)<NewLine><NewLine># and load and test the quantized model<NewLine>model = resnet18()<NewLine># model = SimpleNetwork()<NewLine>test_the_model(model, data_loader)<NewLine><NewLine></code></pre><NewLine><p>I changed the SimpleNetwork, based on what you suggested and It doesnt fail anymore, but this is not the case with the ResNet18.</p><NewLine><p>I’ll try to dig a bit more and see why I find .<br/><NewLine>Thanks alot for your time really appreciate it</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>I noticed two things so far:<br/><NewLine>Pytorch has issues with branches in the model for some reason. that is, lets consider SimpleNetwork here.</p><NewLine><pre><code class=""lang-python""><NewLine>class SimpleNetwork(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        self.prelu_q = PReLU_Quantized(nn.PReLU())<NewLine>        self.bn = nn.BatchNorm2d(10)<NewLine><NewLine>        self.prelu_q_or_relu = torch.relu if use_relu else self.prelu_q<NewLine>        self.bn_or_identity = nn.Identity() if disable_single_bns else self.bn    <NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu1(x)<NewLine><NewLine>        x = self.prelu_q_or_relu(x)<NewLine>        x = self.bn_or_identity(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>This by default results in the infamous error stated in the op. However, if I simply remove :</p><NewLine><pre><code class=""lang-python"">self.bn_or_identity = nn.Identity() if disable_single_bns else self.bn    <NewLine></code></pre><NewLine><p>and simply use the</p><NewLine><pre><code class=""lang-python"">self.bn = nn.BatchNorm2d(10)<NewLine></code></pre><NewLine><p>in the forward pass, I no longer see that error!.<br/><NewLine>I tried to do the same thing to ResNet18, and it seems, all previous bns are fine except the bn3 in the ResNet model (the penultimate layer) which regardless of what I change, still give that error !</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>what is the error when you use<br/><NewLine><code>self.bn_or_identity = nn.Identity() if disable_single_bns else self.bn</code> ? I think this is what you need to do</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: July 13, 2020, 11:31am; <NewLine> REPLY_DATE 2: July 13, 2020, 11:33am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:39am; <NewLine> REPLY_DATE 4: July 21, 2020,  6:14am; <NewLine> REPLY_DATE 5: July 15, 2020,  6:49am; <NewLine> REPLY_DATE 6: July 16, 2020,  7:39am; <NewLine> REPLY_DATE 7: July 18, 2020,  1:58am; <NewLine> REPLY_DATE 8: July 20, 2020,  3:41am; <NewLine> REPLY_DATE 9: July 20, 2020,  7:11am; <NewLine> REPLY_DATE 10: July 20, 2020,  9:42am; <NewLine> REPLY_DATE 11: July 20, 2020,  9:55am; <NewLine> REPLY_DATE 12: July 20, 2020,  3:42pm; <NewLine> REPLY_DATE 13: July 21, 2020,  6:04am; <NewLine> REPLY_DATE 14: July 22, 2020,  3:53pm; <NewLine> REPLY_DATE 15: July 22, 2020,  5:42pm; <NewLine> REPLY_DATE 16: July 25, 2020,  3:10am; <NewLine> REPLY_DATE 17: August 21, 2020, 10:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 1 Like; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
91686,Quantizing Transformer Architecture Below 8-bit (post training quantization),2020-08-05T07:14:01.024Z,3,148,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to quantize BERT to 4 bits or mixed precision, and I don’t see available methods to to quantization aware training on BERT for any precision other than torch.uint8. This is given in the dynamic quantization tutorial.<br/><NewLine>I want to use both post training quantization and dynamic quantization for lower than 8 bits.</p><NewLine><p>Will I have to rewrite the modeling_bert.py (transformers/modeling_bert.py) layers with fake quantization added? How can lower than 8bit precision and mixed precision be implemented on BERT?</p><NewLine></div>",https://discuss.pytorch.org/u/pkadambi,,pkadambi,"August 5, 2020,  8:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The difficulty there is PyTorch inherently assumes that things are at least 1 byte when doing things with memory.<br/><NewLine>I’d probably convert to <a href=""https://tvm.ai/"" rel=""nofollow noopener"">TVM</a> and see what can be done there.<br/><NewLine>(QAT with fake quantization probably could work for 4 bits, too.)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s not an issue even if the weights are stored as FP32 values in memory.<br/><NewLine>I’m trying to evaluate post training quantization or fine tune the model with quantization aware training, but do this all under under fake quantization to any bit width of my choosing.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>While I don’t think it works out of the box, you could try to adapt the observers and fake quant layers to be more flexible. For example, there are some obvious 8 bit hard coded values here:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/a414bd69de8d01af44751bfe327703ec997dafd9/torch/quantization/observer.py#L146"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/a414bd69de8d01af44751bfe327703ec997dafd9/torch/quantization/observer.py#L146"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/a414bd69de8d01af44751bfe327703ec997dafd9/torch/quantization/observer.py#L146</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""136"" style=""counter-reset: li-counter 135 ;""><NewLine><li>    Learned Step Size Quantization: https://openreview.net/pdf?id=rkgO66VKDS</li><NewLine><li>    Trained Quantization Thresholds: https://arxiv.org/pdf/1903.08066.pdf</li><NewLine><li>    """"""</li><NewLine><li>    # The variable names are prefixed with ""initial"" because their values (qmin and qmax) might be adjusted</li><NewLine><li>    # based on whether quantization range is reduced and the datatype (signed/unsigned) used by the observer.</li><NewLine><li>    initial_qmin, initial_qmax = initial_dynamic_qrange</li><NewLine><li>    assert initial_qmin &lt;= 0 &lt;= initial_qmax, ""Dynamic quantization range must include 0.""</li><NewLine><li>    assert initial_qmin &lt; initial_qmax, ""qmin must be strictly less than qmax for dynamic quantization range.""</li><NewLine><li><NewLine></li><li>@torch.jit.export</li><NewLine><li class=""selected"">def _calculate_qmin_qmax(self):</li><NewLine><li>    # type: () -&gt; Tuple[int, int]</li><NewLine><li>    r""""""Calculates actual qmin and qmax based on the quantization range,</li><NewLine><li>    observer datatype and if range is reduced.</li><NewLine><li>    """"""</li><NewLine><li>    if self.is_dynamic_qrange:</li><NewLine><li>        # This initialization here is to be resolve TorchScript compilation issues and allow</li><NewLine><li>        # using of refinement to decouple initial_qmin and initial_qmax from quantization range.</li><NewLine><li>        # The actual values of initial_qmin and initial_qmax will be reset below.</li><NewLine><li>        initial_qmin, initial_qmax = 0, 255</li><NewLine><li>        # The following assignment of initial_qrange to a local variable and the if check refine the</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""91686"" data-username=""pkadambi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/eb9ed0/40.png"" width=""20""/> pkadambi:</div><NewLine><blockquote><NewLine><p>I’m trying to evaluate post training quantization or fine tune the model with quantization aware training, but do this all under under fake quantization to any bit width of my choosing.</p><NewLine></blockquote><NewLine></aside><NewLine><p>we do have the support for lower bits in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L185"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L185</a> now, one of our interns just added this recently.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pkadambi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: August 5, 2020, 12:22pm; <NewLine> REPLY_DATE 2: August 6, 2020,  4:04am; <NewLine> REPLY_DATE 3: August 11, 2020,  2:23am; <NewLine> REPLY_DATE 4: August 21, 2020, 10:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
93585,Static Quantization of nn.ConstantPad2d,2020-08-21T05:06:02.557Z,0,78,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a module like this  -<br/><NewLine><code>self.conv = nn.Sequence(nn.ConstantPad2d((1,2,1,2)), nn.Conv2d(...))</code></p><NewLine><p>Model converts successfully into quantized form but when I try to evaluate it, I get this error -</p><NewLine><blockquote><NewLine><p>RuntimeError: Could not run ‘aten::empty.memory_format’ with arguments from the ‘QuantizedCPU’ backend. ‘aten::empty.memory_format’ is only available<br/><NewLine>for these backends: [CPU, CUDA, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Autograd, Profiler, Tracer].</p><NewLine></blockquote><NewLine><p><strong>I think this is because quantization of <code>nn.ConstantPad2d</code> is not supported. So, any solution around it?</strong></p><NewLine><p><em>I cannot merge ConstantPad2d and Conv2d because Conv2d don’t support  odd paddings (equivalent of nn.ConstantPad2d((1,2,1,2))) .</em></p><NewLine></div>",https://discuss.pytorch.org/u/deepak_mangla,(Deepak Mangla),deepak_mangla,"August 21, 2020,  5:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think <a class=""mention"" href=""/u/zafar"">@Zafar</a> is working on supporting constant pad right now: <a href=""https://github.com/pytorch/pytorch/pull/43304"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/43304</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: August 21, 2020, 10:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
88098,Reduction in performance of quantized bert model,2020-07-06T11:29:53.288Z,6,228,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am using dynamic quantization on fine-tuned bert model. When I performed inference on quantized model before saving it, I am getting almost similar results(accuracy score) between unquantized and quantized model and reduction in inference time too.</p><NewLine><p>However, when I load the quantized model and do inference on that, there is significant difference (around 30 to 40% decrease in accuracy) in the results, Is this because of way of loading the quantized model?</p><NewLine><p>Any leads will be appreciable…<br/><NewLine>Thanks</p><NewLine><h1>Following is the code</h1><NewLine><pre><code>def load_model(args):<NewLine><NewLine>config = BertConfig.from_pretrained(args.model_dir)<NewLine>tokenizer = BertTokenizer.from_pretrained(<NewLine>    args.model_dir, do_lower_case=args.do_lower_case<NewLine>)<NewLine>model = BertForSequenceClassification.from_pretrained(args.model_dir, config=config)<NewLine><NewLine>return model, tokenizer<NewLine><NewLine>def predict_label(model, inputs):<NewLine><NewLine>    with torch.no_grad():<NewLine>        outputs = model(**inputs)<NewLine><NewLine>logits = outputs[0]<NewLine>logits = F.softmax(logits, dim=1)<NewLine>logits_label = torch.argmax(logits, dim=1)<NewLine>labels = logits_label.detach().cpu().numpy().tolist()<NewLine>label_confidences = [<NewLine>    confidence[label].item() for confidence, label in zip(logits, labels)<NewLine>]<NewLine><NewLine>return labels, label_confidences<NewLine><NewLine><NewLine>def predict(eval_dataloader, model, examples, device):<NewLine><NewLine>index = 0<NewLine><NewLine>labels_for_evaluations = []<NewLine><NewLine>for batch in tqdm(eval_dataloader, desc=""Evaluating""):<NewLine><NewLine>    input_ids = batch[""input_ids""]<NewLine>    mask_ids = batch[""mask_ids""]<NewLine>    token_type_ids = batch[""token_type_ids""]<NewLine><NewLine>    input_ids = input_ids.to(device, dtype=torch.long)<NewLine>    mask_ids = mask_ids.to(device, dtype=torch.long)<NewLine>    token_type_ids = token_type_ids.to(device, dtype=torch.long)<NewLine>    inputs = {""input_ids"": input_ids, ""attention_mask"": mask_ids}<NewLine>    predicted_labels, label_confidences = predict_label(model, inputs)<NewLine>    <NewLine>    for confidence, pred_label in zip(label_confidences, predicted_labels):<NewLine>        labels_for_evaluations.append(str(pred_label))<NewLine>       <NewLine>return labels_for_evaluations<NewLine><NewLine>if __name__ == ""__main__"":<NewLine><NewLine>examples, labels = read_tsv_file(args.data_file)<NewLine>bert_model, tokenizer = load_model(args)<NewLine>bert_model.to(args.device)<NewLine><NewLine># perform quantization<NewLine>quantized_model = quantization.quantize_dynamic(bert_model, {nn.Linear}, dtype=torch.qint8)<NewLine><NewLine>dataframe = pd.DataFrame({""text"": examples})<NewLine>batch_size = 1<NewLine><NewLine>print(""quantized model "", quantized_model)<NewLine>eval_dataloader = create_dataloader(<NewLine>    dataframe, tokenizer, args.max_seq_length, batch_size, test_data=True<NewLine>)<NewLine><NewLine># inference<NewLine>positive_predicted_sentences, labels_for_evaluations = predict(<NewLine>    eval_dataloader, quantized_model, examples, args.device<NewLine>)<NewLine><NewLine># serialized the quatized model<NewLine>quantized_output_dir = args.model_dir + ""_quantized_batch1""<NewLine><NewLine>if not os.path.exists(quantized_output_dir):<NewLine>    os.makedirs(quantized_output_dir)<NewLine>    quantized_model.save_pretrained(quantized_output_dir)<NewLine>    tokenizer.save_pretrained(quantized_output_dir)<NewLine><NewLine>print(""accuracy score "", accuracy_score(labels, labels_for_evaluations))<NewLine></code></pre><NewLine><h1>Update</h1><NewLine><p>I found many people are facing similar issue, when you load the quantized BERT model then there is huge decrease in accuracy.  Here are related issues on github</p><NewLine><p><a href=""https://github.com/huggingface/transformers/issues/2542"" rel=""nofollow noopener"">Dynamic Quantization on ALBERT (pytorch) #2542</a><br/><NewLine><a href=""https://github.com/huggingface/transformers/issues/2556"" rel=""nofollow noopener"">Quantized model not preserved when imported using from_pretrained() #2556</a></p><NewLine></div>",https://discuss.pytorch.org/u/Ramesh_Kumar,(Ramesh Kumar),Ramesh_Kumar,"July 7, 2020,  8:24am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi Ramesh, would you be able to provide some more information?  What is your model def, and what code are you using to save and load the model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> thanks for your response. Please check the code.</p><NewLine><p>I am loading quantized bert model in a similar way as we load the pre-trained bert model. When I convert model to quantization and determine the accuracy it is pretty much similar to without quantized model. However, when I load the quantized model, after saving it, then there is lot of variation in the results.</p><NewLine><p><a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> I have updated the github issues link too, many people are facing similar issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/ramesh_kumar"">@Ramesh_Kumar</a>, could you also provide the code used to load the quantized model?  Are you using the same <code>load_model</code> function, and how are you calling it?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> thanks for your response. Yes, I am loading same load  function to load the quantized model.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88098"" data-username=""Ramesh_Kumar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ramesh_kumar/40/12432_2.png"" width=""20""/> Ramesh_Kumar:</div><NewLine><blockquote><NewLine><p><code>BertForSequenceClassification.from_pretrained</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>ah, I see.  In that case, one place to check would be <code>BertForSequenceClassification.from_pretrained</code> - it might be assuming a floating point model.  You would have to modify the loading code.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I’m facing a similar issue when quantizing Efficientnet.<br/><NewLine>I opened a thread about it <a href=""https://discuss.pytorch.org/t/performance-drop-when-quantizing-efficientnet/90990/4"">here</a>, but i was wondering if you found any solutions for your problem</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> could you please guide what modifications I have to do? I cannot find any leads regarding this. Thanks</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/kfir_goldberg"">@kfir_goldberg</a> no i am still looking for the solution.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>does the solution posted in <a href=""https://github.com/huggingface/transformers/issues/2542"" rel=""nofollow noopener"">https://github.com/huggingface/transformers/issues/2542</a> solve your problem?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kfir_goldberg; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: July 6, 2020,  6:07pm; <NewLine> REPLY_DATE 2: July 7, 2020,  8:25am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:58pm; <NewLine> REPLY_DATE 4: July 24, 2020, 12:38pm; <NewLine> REPLY_DATE 5: July 24, 2020, 10:44pm; <NewLine> REPLY_DATE 6: July 31, 2020,  2:37pm; <NewLine> REPLY_DATE 7: August 6, 2020,  1:45pm; <NewLine> REPLY_DATE 8: August 6, 2020,  1:45pm; <NewLine> REPLY_DATE 9: August 21, 2020, 10:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
93318,Calculate_qparams should not be called for NoopObserver,2020-08-19T04:55:48.181Z,0,42,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.<br/><NewLine>I’m trying to use Pytorch’s quantization scheme.<br/><NewLine>I’d like to quantize only weight with fake-quantization(QAT), not activation.</p><NewLine><p>I tried this:</p><NewLine><pre><code class=""lang-auto"">import torch.quantization as Q<NewLine><NewLine>model = load_model(my_config) # currently I'm using resnet architecture<NewLine>qat_model = Q.fuse_modules(model, my_modules_to_fuse)<NewLine>qat_model = Q.Qconfig(activation=Q.NoopObserver, weight=Q.FakeQuantize)<NewLine></code></pre><NewLine><p>and this process from <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">pytorch quantization tutorial</a></p><NewLine><pre><code class=""lang-auto"">for nepoch in range(8):<NewLine>    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)<NewLine>    if nepoch &gt; 3:<NewLine>        # Freeze quantizer parameters<NewLine>        qat_model.apply(torch.quantization.disable_observer)<NewLine>    if nepoch &gt; 2:<NewLine>        # Freeze batch norm mean and variance estimates<NewLine>        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    # Check the accuracy after each epoch<NewLine>    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)<NewLine>    quantized_model.eval()<NewLine>    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)<NewLine>    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))<NewLine></code></pre><NewLine><p>But the program gives this error:</p><NewLine><blockquote><NewLine><p>calculate_qparams should not be called for NoopObserver</p><NewLine></blockquote><NewLine><p>the reason I why used NoopObserver is avoiding calculate_qparams for activation… but It’s confused result.<br/><NewLine>How to solve this problem? any suggestion will be appreciated.<br/><NewLine>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/FruitVinegar,(NHK),FruitVinegar,"August 19, 2020,  4:59am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can’t skip quantizing just by setting the observer to NoopObserver. I don’t think weight only quantization is support in convert stage. You can evaluate the accuracy of the qat module directly without convert.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: August 21, 2020,  9:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
59531,Dynamic Quantization not reducing model size,2019-10-30T03:19:17.107Z,4,782,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am trying to use the quantize_dynamic module, but the size of the quantized model is the same as the original model in my case.</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import torch<NewLine>from craft import CRAFT<NewLine>trained_model='craft_mlt_25k.pth'<NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"")/1e6)<NewLine>    os.remove('temp.p')<NewLine>    <NewLine>from collections import OrderedDict<NewLine>def copyStateDict(state_dict):<NewLine>    if list(state_dict.keys())[0].startswith(""module""):<NewLine>        start_idx = 1<NewLine>    else:<NewLine>        start_idx = 0<NewLine>    new_state_dict = OrderedDict()<NewLine>    for k, v in state_dict.items():<NewLine>        name = ""."".join(k.split(""."")[start_idx:])<NewLine>        new_state_dict[name] = v<NewLine>    return new_state_dict<NewLine><NewLine>net = CRAFT()     # initialize<NewLine>net.load_state_dict(copyStateDict(torch.load(trained_model, map_location='cpu')))<NewLine>net.eval()<NewLine>print_size_of_model(net)<NewLine>quantized = torch.quantization.quantize_dynamic(net, dtype=torch.qint8)<NewLine>print_size_of_model(quantized)<NewLine></code></pre><NewLine><p>Size of both original and quantized model is 83.14 MB. Why doesn’t the model size change ?<br/><NewLine>Any suggestions would be appreciated</p><NewLine><p>Similarily when I tried to quantize the pretrained model ResNet18 model from torchvision, size only changed from 46 MB to 45 MB.</p><NewLine></div>",https://discuss.pytorch.org/u/Raghav_Gurbaxani,(Raghav Gurbaxani),Raghav_Gurbaxani,"October 30, 2019,  3:21am",3 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>Dynamic quantization only helps in reducing the model size for models that use Linear and LSTM  modules. For the case of resnet18, the model consists of conv layers which do not have dynamic quantization support yet. For your model, can you check if it has linear layers?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a><br/><NewLine>Thanks for your response.<br/><NewLine>But dynamic quantization is able to reduce model size from 553 MB to 182 MB, while VGG16 is mostly convolution layers, why such a drastic change then ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s probably because most of the weights are in the last few Linear layers?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, for the case of VGG16, the last two fc layers contain the bulk of the weights.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I am trying to quantize a BERT but the size is not reducing, I wonder why, Can someone help me?</p><NewLine><p>Here is the code snippet:-</p><NewLine><p>import torch as torch</p><NewLine><p>import os</p><NewLine><p>from transformers.modeling_bert import BertConfig, BertForPreTraining, load_tf_weights_in_bert, BertModel</p><NewLine><p>tf_checkpoint_path=""./distlang/""<br/><NewLine>bert_config_file = “./config.json”<br/><NewLine>pytorch_dump_path=""./distlangpytorch/""</p><NewLine><p>device = “cpu”</p><NewLine><p>torch.backends.quantized.engine = ‘qnnpack’</p><NewLine><p>qconfig = torch.quantization.get_default_qconfig(‘fbgemm’)</p><NewLine><p>print(qconfig)<br/><NewLine>config = BertConfig.from_json_file(bert_config_file)<br/><NewLine>print(“Building PyTorch model from configuration: {}”.format(str(config)))<br/><NewLine>model = BertModel.from_pretrained(""./distlangpytorch/"")</p><NewLine><p>model.to(device)</p><NewLine><p>torch.quantization.prepare(model)</p><NewLine><p>quantized_model=torch.quantization.convert(model)</p><NewLine><p>def print_size_of_model(model):<br/><NewLine>torch.save(model.state_dict(), “temp.p”)<br/><NewLine>print(‘Size (MB):’, os.path.getsize(“temp.p”)/1e6)<br/><NewLine>os.remove(‘temp.p’)</p><NewLine><p>print_size_of_model(model)<br/><NewLine>print_size_of_model(quantized_model)</p><NewLine><p>quantized_output_dir = “./quantized_model”<br/><NewLine>if not os.path.exists(quantized_output_dir):<br/><NewLine>os.makedirs(quantized_output_dir)</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/sagar_gupta"">@Sagar_Gupta</a>,</p><NewLine><p>In this mode of quantization the model has to be calibrated(evaluate your model after prepare()) to capture the qparams (zeropoint&amp;scale)<br/><NewLine>Which are needed to quantize the model i.e weights and all</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/bolloju_aravind"">@BOLLOJU_ARAVIND</a>,  Can you refer some guide for  doing this?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Sagar_Gupta; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/deepak_mangla; <NewLine> ,"REPLY_DATE 1: June 29, 2020,  2:17am; <NewLine> REPLY_DATE 2: November 2, 2019,  6:27pm; <NewLine> REPLY_DATE 3: December 18, 2019, 12:39am; <NewLine> REPLY_DATE 4: December 19, 2019,  4:26am; <NewLine> REPLY_DATE 5: June 2, 2020,  1:38pm; <NewLine> REPLY_DATE 6: June 3, 2020,  2:53am; <NewLine> REPLY_DATE 7: August 19, 2020,  7:24am; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
91673,Convert floating point 32 bit of input and pretrained weight to 8bit,2020-08-05T05:46:28.565Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><ul><NewLine><li><NewLine><p>I am using alexnet model ,where 7layers are binarized(input and weight),1st layer is not binarized(input,weight are floating point 32 bit).I want  only 1st layer input,weight to be converted to 8 bit  before sending into the convolution function without harming the other.</p><NewLine></li><NewLine><li><NewLine><p>i am using pretrained weight here</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/Ganga,(Gangadhara R),Ganga,"August 7, 2020, 11:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just to make it clear – when you say “convert to 8bit” are you using quantization or are you just casting the types down? Also, we don’t support quantization lower than 8 bits, so binarization of the layers might not be supported without custom hacks.</p><NewLine><p>Lastly, if you already have the weights, and you just need an 8-bit model, you can follow these steps:</p><NewLine><ol><NewLine><li>Make sure your model is quantizable – all layers in your network must be stateful and unique, that is, no “implied” layers in the forward and no inplace computation</li><NewLine><li>Prepare the model using <code>prepare</code> function</li><NewLine><li>Calibrate the prepared model by running through your data AT LEAST once</li><NewLine><li>Convert your model to the quantized version.</li><NewLine></ol><NewLine><p>You can follow the PTQ tutorial here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><p>On the first point:</p><NewLine><p>This model cannot be quantized:</p><NewLine><pre><code class=""lang-python"">class Model(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(Model, self).__init__()<NewLine>    self.relu = nn.ReLU(inplace=True)<NewLine>  def forward(self, a, b):<NewLine>    ra = self.relu(a)<NewLine>    rb = self.relu(b)<NewLine>    return ra + rb<NewLine></code></pre><NewLine><p>To make the model quantizable, you need to make sure there are no inplace operations, and every operation can save the state:</p><NewLine><pre><code class=""lang-python"">class Model(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(Model, self).__init__()<NewLine>    self.relu_a = nn.ReLU(inplace=False)<NewLine>    self.relu_b = nn.ReLU(inplace=False)<NewLine>    self.F = nn.quantized.FloatFunctional()<NewLine>  def forward(self, a, b):<NewLine>    ra = self.relu_a(a)<NewLine>    rb = self.relu_b(b)<NewLine>    return self.F.add(ra, rb)<NewLine></code></pre><NewLine><p>If you want to have the model take FP input and return the FP output you will need to insert the <code>QuantStub</code>/<code>DequantStub</code> at the appropriate locations:</p><NewLine><pre><code class=""lang-python"">class Model(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(Model, self).__init__()<NewLine>    self.quant_stub_a = torch.quantization.QuantStub()<NewLine>    self.quant_stub_b = torch.quantization.QuantStub()<NewLine>    self.relu_a = nn.ReLU(inplace=False)<NewLine>    self.relu_b = nn.ReLU(inplace=False)<NewLine>    self.F = nn.quantized.FloatFunctional()<NewLine>    self.dequant_stub = torch.quantization.DeQuantStub()<NewLine>  def forward(self, a, b):<NewLine>    qa = self.quant_stub_a(a)<NewLine>    qb = self.quant_stub_b(b)<NewLine>    ra = self.relu_a(qa)<NewLine>    rb = self.relu_b(qb)<NewLine>    return self.dequant_stub(self.F.add(ra, rb))<NewLine></code></pre><NewLine><p>Similarly, if you would like to only quantize a single layer, you would need to place the quant/dequant only where you want to quantize. Please, note that you would need to specify the quantization parameters appropriately:</p><NewLine><pre><code class=""lang-python"">class Model(nn.Module):<NewLine>  def __init__(self):<NewLine>    super(Model, self).__init__()<NewLine>    self.quant_stub_a = torch.quantization.QuantStub()<NewLine>    self.relu_a = nn.ReLU(inplace=False)<NewLine>    self.relu_b = nn.ReLU(inplace=False)<NewLine>  def forward(self, a, b):<NewLine>    qa = self.quant_stub_a(a)<NewLine>    ra = self.relu_a(qa)<NewLine>    a = self.dequant_stub(ra)<NewLine>    rb = self.relu(b)<NewLine>    return ra + rb<NewLine></code></pre><NewLine><p>The model above will be partially quantizable, and you would need to give the qconfig to the quant_stub and the relu only.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  7:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90019,Simple quantized model doesn&rsquo;t export to ONNX,2020-07-21T11:12:10.116Z,5,246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I’m having problems exporting a very simple quantized model to ONNX. The error message I’m seeing is -</p><NewLine><p><code>AttributeError: 'torch.dtype' object has no attribute 'detach'</code></p><NewLine><p>The cause of this is that <strong>(‘fc1._packed_params.dtype’, torch.qint8)</strong> is ends up in the state_dict.</p><NewLine><p>I asked on a previous (and old) thread if there was a solution and the answer was that this could be solved in the latest version of PyTorch. So I installed <strong>1.7.0.dev20200705+cpu</strong>, but no joy.</p><NewLine><p>I’ve pasted the example below.</p><NewLine><p>Any thoughts on whether this is a fault on my part, a bug, or not supported, greatly appreciated.</p><NewLine><pre><code class=""lang-auto"">#Import libraries<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>import torch.optim as optim<NewLine>#Needed for quantization<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>import torch.quantization<NewLine><NewLine>class Net(nn.Module):<NewLine>    def __init__(self):<NewLine>        #create instance of base class<NewLine>        super().__init__()<NewLine>        self.fc1 = nn.Linear(28*28, 10) #Inputs, outputs<NewLine>    <NewLine>        #Optimizer parameters<NewLine>        self.learning_rate = 0.01<NewLine>        self.epochs = 10<NewLine>        self.log_interval = 10<NewLine>        self.batch_size=200<NewLine><NewLine>        #Needed for quantization, per pytorch examples<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine>        #Training related functions<NewLine>        self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9)<NewLine>        self.criterion = nn.NLLLoss()<NewLine><NewLine>    def forward(self, x, save_intermediate = False, count=0):<NewLine>        x1 = self.quant(x)<NewLine>        x2 = self.fc1(x1)<NewLine>        x3 = self.dequant(x2)<NewLine>        return x3<NewLine><NewLine><NewLine>net = Net()<NewLine><NewLine>net.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(net, inplace=True)<NewLine>torch.quantization.convert(net, inplace=True)<NewLine><NewLine>torch.onnx.export(net,<NewLine>      torch.zeros([1,784]),<NewLine>      'simple.onnx',<NewLine>      opset_version=11,<NewLine>      verbose=True<NewLine>)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/G4V,(Gavin Simpson),G4V,"July 21, 2020, 11:12am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>General export of quantized models to ONNX isn’t currently supported. We currently only support conversion to ONNX for Caffe2 backend. This thread has additional context on what we currently support - <a href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/8"">ONNX export of quantized model </a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a>, many thanks.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""90019"" data-username=""G4V""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/g/f05b48/40.png"" width=""20""/> G4V:</div><NewLine><blockquote><NewLine><p>fc1._packed_params</p><NewLine></blockquote><NewLine></aside><NewLine><p>can you print the type of <strong>fc1._packed_params</strong></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>As mentioned in the other thread by <a class=""mention"" href=""/u/supriyar"">@supriyar</a><br/><NewLine>can you try</p><NewLine><pre><code class=""lang-auto"">torch.onnx.export(q_model, pt_inputs, f, input_names=input_names, example_outputs=output,<NewLine>                  operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> and <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>, many thanks again.</p><NewLine><p>Following the example, I’ve managed to get the model to convert. Is it the ATen fallback that forces the exporter to export specifically to Caffe2? Is Caffe2 effectively a subset?</p><NewLine><p>My goal is to get this model through the Glow compiler. This supports both importing ONNX and Caffe2 but still seeing issues with unknown element kind. A long shot as this is not the correct forum, but any experience on whether this is doable?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not familiar with onnx export, here is the doc for that: <a href=""https://github.com/pytorch/pytorch/blob/master/docs/source/onnx.rst#onnx-aten-fallback"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/docs/source/onnx.rst#onnx-aten-fallback</a></p><NewLine><p>I think we are integrating with glow now, I’ll ask someone from glow team to answer the question.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>still seeing issues with unknown element kind</p><NewLine></blockquote><NewLine><p>Is this error during importing to Glow or with ONNX export?</p><NewLine><p>Glow’s onnx importer doesn’t match up with the latest ONNX very closely so you could run into some issues. Not sure what model you’re working with but you could also try using to_glow to lower from PyTorch to Glow more directly, though this path is fairly new and has been tested mostly on ResNet-like models. (<a href=""https://github.com/pytorch/glow/blob/master/torch_glow/tests/functionality/conv_to_glow_test.py"" rel=""nofollow noopener"">like this</a>)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jackm321; <NewLine> ,"REPLY_DATE 1: July 21, 2020, 11:40pm; <NewLine> REPLY_DATE 2: July 25, 2020,  7:19am; <NewLine> REPLY_DATE 3: July 30, 2020,  3:57pm; <NewLine> REPLY_DATE 4: July 31, 2020, 12:10am; <NewLine> REPLY_DATE 5: August 3, 2020,  3:16pm; <NewLine> REPLY_DATE 6: August 12, 2020,  4:31pm; <NewLine> REPLY_DATE 7: August 12, 2020,  5:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
92150,Layer wise quantization,2020-08-09T08:52:26.999Z,1,77,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How do we perform layer wise quantization in pytorch i.e I want to quantize only third and fourth layer, how can i do it?<br/><NewLine>when we prepare model for quantization using model.prepare all the modules present in the whitelist are quantising . But i didn’t find a way to quantize a single layer. Any kind of help is appreciated?</p><NewLine></div>",https://discuss.pytorch.org/u/snehaoladri,(snehaoladri),snehaoladri,"August 9, 2020,  8:52am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am finally able to achieve it by setting different config for the layers i want to quantize, rather than using model.qconfig (which does for all the layers in the model).</p><NewLine><p>For example:</p><NewLine><p>I quantized the first layer by accessing the first layer and assigning it the qconfig.<br/><NewLine>per_channel_quantized_model.features[0][0].qconfig=torch.quantization.get_default_qconfig(‘fbgemm’)<br/><NewLine>per_channel_quantized_model.features[0][0].qconfig=torch.quantization.get_default_qconfig(‘fbgemm’)</p><NewLine><p>Hope this helps. And if there is any other efficient approach to achieve this please do let me know.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, this is how we do it in eager mode, we have prototype graph mode that works on torchscript models: <a href=""https://pytorch.org/tutorials/prototype/graph_mode_static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/prototype/graph_mode_static_quantization_tutorial.html</a> which can configure layers with a <code>qconfig_dict</code>. Although we might move away from this soon, but this should still generally work if you need to use it now. Note to use the prototype you will need to use nightly build.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/snehaoladri; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: August 12, 2020,  5:44pm; <NewLine> REPLY_DATE 2: August 12, 2020,  5:44pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88514,Quantization/QAT causing jit.script to fail,2020-07-09T07:27:51.326Z,11,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>I’m trying to do QAT -&gt; Torchscript but am getting an error.</p><NewLine><p>My model is</p><NewLine><details><NewLine><summary> Click here </summary><NewLine><pre><code class=""lang-auto"">import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>def SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, activation_fn=nn.ReLU):<NewLine>    """"""Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.<NewLine>    """"""<NewLine>    return nn.Sequential(<NewLine>        nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,<NewLine>                  groups=in_channels, stride=stride, padding=padding),<NewLine>        activation_fn(),<NewLine>        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)<NewLine>    )<NewLine><NewLine><NewLine>class SSD(nn.Module):<NewLine>    def __init__(self, num_classes: int, is_test=False, config=None, device=None, activation_fn=nn.ReLU):<NewLine>        """"""Compose a SSD model using the given components.<NewLine>        """"""<NewLine>        super(SSD, self).__init__()<NewLine>        self.base_channel = 16<NewLine>        self.num_classes = num_classes<NewLine>        self.is_test = is_test<NewLine><NewLine>        if device:<NewLine>            self.device = device<NewLine>        else:<NewLine>            self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<NewLine><NewLine>        if config:<NewLine>            self.center_variance = torch.tensor([config['center_variance']], device=device)<NewLine>            self.size_variance = torch.tensor([config['size_variance']], device=device)<NewLine>            self.priors = config['priors'].to(self.device)<NewLine>        else:<NewLine>            self.center_variance = torch.tensor([0.1], device=device)<NewLine>            self.size_variance = torch.tensor([0.2], device=device)<NewLine>        <NewLine>        self.extras = nn.Sequential(<NewLine>            nn.Conv2d(in_channels=self.base_channel * 16, out_channels=self.base_channel * 4, kernel_size=1),<NewLine>            activation_fn(),<NewLine>            SeperableConv2d(in_channels=self.base_channel * 4, out_channels=self.base_channel *<NewLine>                               16, kernel_size=3, stride=2, padding=1, activation_fn=activation_fn),<NewLine>            activation_fn()<NewLine>        )<NewLine><NewLine>        self.regression_headers0 = SeperableConv2d(in_channels=self.base_channel * 4, out_channels=3 *<NewLine>                                                   4, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.regression_headers1 = SeperableConv2d(in_channels=self.base_channel * 8, out_channels=2 *<NewLine>                                                   4, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.regression_headers2 = SeperableConv2d(in_channels=self.base_channel * 16, out_channels=2 *<NewLine>                                                   4, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.regression_headers3 = nn.Conv2d(in_channels=self.base_channel * 16,<NewLine>                                             out_channels=3 * 4, kernel_size=3, padding=1)<NewLine><NewLine>        self.classification_headers0 = SeperableConv2d(in_channels=self.base_channel * 4, out_channels=3 *<NewLine>                                                       num_classes, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.classification_headers1 = SeperableConv2d(in_channels=self.base_channel * 8, out_channels=2 *<NewLine>                                                       num_classes, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.classification_headers2 = SeperableConv2d(in_channels=self.base_channel * 16, out_channels=2 *<NewLine>                                                       num_classes, kernel_size=3, padding=1, activation_fn=activation_fn)<NewLine>        self.classification_headers3 = nn.Conv2d(in_channels=self.base_channel *<NewLine>                                              16, out_channels=3 * num_classes, kernel_size=3, padding=1)<NewLine><NewLine>        def conv_bn(inp, oup, stride):<NewLine>            return nn.Sequential(<NewLine>                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),<NewLine>                nn.BatchNorm2d(oup),<NewLine>                activation_fn()<NewLine>            )<NewLine><NewLine>        def conv_dw(inp, oup, stride):<NewLine>            return nn.Sequential(<NewLine>                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),<NewLine>                nn.BatchNorm2d(inp),<NewLine>                activation_fn(),<NewLine><NewLine>                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),<NewLine>                nn.BatchNorm2d(oup),<NewLine>                activation_fn(),<NewLine>            )<NewLine>        self.backbone_chunk1 = nn.Sequential(<NewLine>            conv_bn(3, self.base_channel, 2),  # 160*120<NewLine>            conv_dw(self.base_channel, self.base_channel * 2, 1),<NewLine>            conv_dw(self.base_channel * 2, self.base_channel * 2, 2),  # 80*60<NewLine>            conv_dw(self.base_channel * 2, self.base_channel * 2, 1),<NewLine>            conv_dw(self.base_channel * 2, self.base_channel * 4, 2),  # 40*30<NewLine>            conv_dw(self.base_channel * 4, self.base_channel * 4, 1),<NewLine>            conv_dw(self.base_channel * 4, self.base_channel * 4, 1),<NewLine>            # BasicRFB(self.base_channel * 4, self.base_channel * 4, stride=1, scale=1.0, activation_fn=activation_fn)<NewLine>        )<NewLine>        self.backbone_chunk2 = nn.Sequential(<NewLine>            conv_dw(self.base_channel * 4, self.base_channel * 8, 2),  # 20*15<NewLine>            conv_dw(self.base_channel * 8, self.base_channel * 8, 1),<NewLine>            conv_dw(self.base_channel * 8, self.base_channel * 8, 1),<NewLine>        )<NewLine>        self.backbone_chunk3 = nn.Sequential(<NewLine>            conv_dw(self.base_channel * 8, self.base_channel * 16, 2),  # 10*8<NewLine>            conv_dw(self.base_channel * 16, self.base_channel * 16, 1)<NewLine>        )<NewLine><NewLine>        self.quant0 = QuantStub()<NewLine>        self.quant1 = QuantStub()<NewLine>        self.quant2 = QuantStub()<NewLine>        self.quant3 = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine><NewLine>    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:<NewLine>        confidences = []<NewLine>        locations = []<NewLine><NewLine>        x = self.quant0(x)<NewLine>        for layer in self.backbone_chunk1:<NewLine>            x = layer(x)<NewLine>        x = self.dequant(x)<NewLine>        confidence = self.classification_headers0(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers0(x)<NewLine>        location = location.permute(0, 2, 3, 1).contiguous()<NewLine>        location = location.view(location.size(0), -1, 4)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        # x = self.quant1(x)<NewLine>        for layer in self.backbone_chunk2:<NewLine>            x = layer(x)<NewLine>        # x = self.dequant(x)<NewLine>        confidence = self.classification_headers1(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers1(x)<NewLine>        location = location.permute(0, 2, 3, 1).contiguous()<NewLine>        location = location.view(location.size(0), -1, 4)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        # x = self.quant2(x)<NewLine>        for layer in self.backbone_chunk3:<NewLine>            x = layer(x)<NewLine>        # x = self.dequant(x)<NewLine>        confidence = self.classification_headers2(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers2(x)<NewLine>        location = location.permute(0, 2, 3, 1).contiguous()<NewLine>        location = location.view(location.size(0), -1, 4)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        # x = self.quant3(x)<NewLine>        x = self.extras(x)<NewLine>        # x = self.dequant(x)<NewLine>        confidence = self.classification_headers3(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers3(x)<NewLine>        location = location.permute(0, 2, 3, 1).contiguous()<NewLine>        location = location.view(location.size(0), -1, 4)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        confidences = torch.cat(confidences, 1)<NewLine>        locations = torch.cat(locations, 1)<NewLine><NewLine>        return confidences, locations<NewLine><NewLine>    def load(self, model):<NewLine>        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))<NewLine><NewLine>    def save(self, model_path):<NewLine>        torch.save(self.state_dict(), model_path)<NewLine><NewLine></code></pre><NewLine></details><NewLine><p>I do QAT -&gt; torchscript and test it by running the code:</p><NewLine><pre><code class=""lang-auto"">from ssd import SSD<NewLine>...<NewLine>net = SSD(num_classes=2, device=device, config=config)<NewLine>...<NewLine>net.load(trained_model_path)<NewLine>net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(net, inplace=True)<NewLine><NewLine>for epoch in range(num_epochs):<NewLine>    train() etc...<NewLine><NewLine>    net.eval()<NewLine>    net.cpu()<NewLine>    # convert to quantised<NewLine>    quant_net = deepcopy(net)<NewLine>    quant_net = torch.quantization.convert(quant_net, inplace=False)<NewLine>    quant_net.save(os.path.join(args.checkpoint_folder,<NewLine>                                          f""quantised-net.pth""))<NewLine>            <NewLine>    m = torch.jit.script(quant_net)<NewLine>    m.cpu()<NewLine>    dummy = torch.randn(1, 3, 480, 640).cpu().float()<NewLine>    a = m.forward(dummy) # test to see if scripted module works<NewLine><NewLine>    torch.jit.save(m, os.path.join(args.checkpoint_folder, f""jit-net.pt""))<NewLine><NewLine>    net.to(DEVICE)<NewLine></code></pre><NewLine><p>and I get the error on the line <code> a = m.forward(dummy)</code> :</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train_testt.py"", line 360, in &lt;module&gt;<NewLine>    a = m.forward(dummy)<NewLine>RuntimeError: The following operation failed in the TorchScript interpreter.<NewLine>Traceback of TorchScript (most recent call last):<NewLine>  File ""/home/joel/Desktop/Ultra-Light-Fast-Generic-Face-Detector-1MB/minimod.py"", line 124, in forward<NewLine>            x = layer(x)<NewLine>        x = self.dequant(x)<NewLine>        confidence = self.classification_headers0(x)<NewLine>                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>  File ""/home/joel/anaconda3/envs/nightlytorch/lib/python3.8/site-packages/torch/nn/modules/container.py"", line 117, in forward<NewLine>    def forward(self, input):<NewLine>        for module in self:<NewLine>            input = module(input)<NewLine>                    ~~~~~~ &lt;--- HERE<NewLine>        return input<NewLine>  File ""/home/joel/anaconda3/envs/nightlytorch/lib/python3.8/site-packages/torch/nn/modules/container.py"", line 117, in forward<NewLine>    def forward(self, input):<NewLine>        for module in self:<NewLine>            input = module(input)<NewLine>                    ~~~~~~ &lt;--- HERE<NewLine>        return input<NewLine>  File ""/home/joel/anaconda3/envs/nightlytorch/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py"", line 326, in forward<NewLine>        if len(input.shape) != 4:<NewLine>            raise ValueError(""Input shape must be `(N, C, H, W)`!"")<NewLine>        return ops.quantized.conv2d(<NewLine>               ~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>            input, self._packed_params, self.scale, self.zero_point)<NewLine>RuntimeError: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, BackendSelect, Named, Autograd, Profiler, Tracer, Autocast, Batched].<NewLine><NewLine>QuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/native/quantized/cpu/qconv.cpp:736 [kernel]<NewLine>BackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]<NewLine>Named: registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]<NewLine>Autograd: fallthrough registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/core/VariableFallbackKernel.cpp:31 [backend fallback]<NewLine>Profiler: registered at /opt/conda/conda-bld/pytorch_1594145889316/work/torch/csrc/autograd/profiler.cpp:677 [backend fallback]<NewLine>Tracer: fallthrough registered at /opt/conda/conda-bld/pytorch_1594145889316/work/torch/csrc/jit/frontend/tracer.cpp:960 [backend fallback]<NewLine>Autocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/autocast_mode.cpp:375 [backend fallback]<NewLine>Batched: registered at /opt/conda/conda-bld/pytorch_1594145889316/work/aten/src/ATen/BatchingRegistrations.cpp:149 [backend fallback]<NewLine></code></pre><NewLine><p>This error does not occur if I remove all QAT/quantization lines and just jit.script the original model. The same error still occurs if I remove all Quant/DeQuantStubs in the model.<br/><NewLine>Does anyone know why this error occurs?<br/><NewLine>Could I also ask whether the commented out QuantStubs/DeStubs in the model are correctly placed?</p><NewLine><p>Thank you!</p><NewLine></div>",https://discuss.pytorch.org/u/kekpirat,(Joel),kekpirat,"July 9, 2020,  7:28am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>this means the QuantStub/DeQuantStub is not placed correctly in the model, and the input of quantized::conv2d is not quantized yet, you can look at the model and see if you have a missing QuantStub before conv2d module.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88514"" data-username=""kekpirat""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kekpirat/40/26428_2.png"" width=""20""/> kekpirat:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">for layer in self.backbone_chunk1:<NewLine>            x = layer(x)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>looking at the code most likely it’s here:</p><NewLine><pre><code class=""lang-auto"">x = self.quant0(x)<NewLine>for layer in self.backbone_chunk1:<NewLine>            x = layer(x)<NewLine></code></pre><NewLine><p>only the first x is quantized in this case, instead you should have a quant x for each activation in the loop:</p><NewLine><pre><code class=""lang-auto"">x = self.quant0(x)<NewLine>for i, layer in enumerate(self.backbone_chunk1):<NewLine>            x = layer(x)<NewLine>            x = self.quants[i](x)<NewLine></code></pre><NewLine><p>and define a list of quantstub instances with same length of self.backbone_chunk1 in <strong>init</strong></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello. Thanks for the reply!</p><NewLine><p>I tried what you suggested but still get a slightly different error</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'quantized::conv2d' with arguments from the 'CPUTensorId' backend. 'quantized::conv2d' is only available for these backends: [QuantizedCPUTensorId].<NewLine></code></pre><NewLine><p>I’m wondering perhaps how to solve the problem on a simpler model which has the same error/issue</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn, optim<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>from copy import deepcopy<NewLine><NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>        self.backbone0 = nn.Sequential(<NewLine>            nn.Conv2d(3, 1, 1, bias=False),<NewLine>            nn.BatchNorm2d(1),<NewLine>            nn.ReLU(),<NewLine>        )<NewLine>        self.backbone1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 2, 3, stride=2, padding=1, bias=False),<NewLine>            nn.BatchNorm2d(2),<NewLine>            nn.AvgPool2d(14),<NewLine>            nn.Sigmoid(),<NewLine>        )<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.backbone0(x)<NewLine>        x = self.dequant(x)<NewLine>        x = self.backbone1(x)<NewLine>       <NewLine>        return x<NewLine><NewLine>model = Model()<NewLine><NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model, inplace=True)<NewLine><NewLine>optimizer = optim.Adam(model.parameters(), lr=1)<NewLine><NewLine>model.to(device)<NewLine>print(model)<NewLine><NewLine>criterion = nn.BCELoss()<NewLine><NewLine>for epoch in range(10):<NewLine>    model.train()<NewLine><NewLine>    inputs = torch.rand(2, 3, 28, 28)<NewLine>    labels = torch.FloatTensor([[1, 1], [0, 0]])<NewLine><NewLine>    inputs = inputs.to(device)<NewLine>    labels = labels.to(device)<NewLine>    outputs = model(inputs)<NewLine>    loss = criterion(outputs.view(2, 2), labels)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine><NewLine>    if epoch &gt;= 2:<NewLine>        model.apply(torch.quantization.disable_observer)<NewLine><NewLine>    if epoch &gt;= 3:<NewLine>        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    quant_model = deepcopy(model)<NewLine>    quant_model = torch.quantization.convert(quant_model.eval().cpu(), inplace=False)<NewLine><NewLine>    with torch.no_grad():<NewLine>        out = quant_model(torch.rand(1, 3, 28, 28))<NewLine></code></pre><NewLine><p>I tried to prepare_qat only backbone0 as well but got the same error:</p><NewLine><pre><code class=""lang-auto"">model.backbone0.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model.backbone0, inplace=True)<NewLine></code></pre><NewLine><p>What is the correct way to place QuantStubs in the forward() such that only <code>backbone0</code> is quantised?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Managed to get it to look correct (from looking at print(quant_model)) and not error out by only preparing qat on backbone0, and inserting the Quant/DeQuantStub into the nn.Sequential itself</p><NewLine><pre><code class=""lang-auto"">...<NewLine>    self.backbone0 = nn.Sequential(<NewLine>            QuantStub(),<NewLine>            nn.Conv2d(3, 1, 1, bias=False),<NewLine>            nn.BatchNorm2d(1),<NewLine>            nn.ReLU(),<NewLine>            DeQuantStub()<NewLine>        )<NewLine>...<NewLine>model.backbone0.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model.backbone0, inplace=True)<NewLine></code></pre><NewLine><p>I notice that when I forward zeros through it, it has a random chance of having either the same output as the non quant-converted model or drastically different outputs.<br/><NewLine>Is this the wrong way to get backbone0 quantised?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>I notice that when I forward zeros through it, it has a random chance of having either the same output as the non quant-converted model or drastically different outputs.</p><NewLine></blockquote><NewLine><p>is this after qat?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes</p><NewLine><details><NewLine><summary> The code now </summary><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn, optim<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>from copy import deepcopy<NewLine><NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>        self.backbone0 = nn.Sequential(<NewLine>            QuantStub(),<NewLine>            nn.Conv2d(3, 1, 1, bias=False),<NewLine>            nn.BatchNorm2d(1),<NewLine>            nn.ReLU(),<NewLine>            DeQuantStub(),<NewLine>        )<NewLine>        self.backbone1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 2, 3, stride=2, padding=1, bias=False),<NewLine>            nn.BatchNorm2d(2),<NewLine>            nn.AvgPool2d(14),<NewLine>            nn.Sigmoid(),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.backbone0(x)<NewLine>        x = self.backbone1(x)<NewLine>        return x<NewLine><NewLine>model = Model()<NewLine><NewLine>model.backbone0.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model.backbone0, inplace=True)<NewLine><NewLine>optimizer = optim.Adam(model.parameters(), lr=1)<NewLine>model.to(device)<NewLine>print(model)<NewLine><NewLine><NewLine><NewLine>criterion = nn.BCELoss()<NewLine><NewLine>for epoch in range(10):<NewLine>    print('EPOCH', epoch)<NewLine>    model.train()<NewLine><NewLine>    inputs = torch.rand(2, 3, 28, 28)<NewLine>    labels = torch.FloatTensor([[1, 1], [0, 0]])<NewLine><NewLine>    inputs = inputs.to(device)<NewLine>    labels = labels.to(device)<NewLine>    outputs = model(inputs)<NewLine>    loss = criterion(outputs.view(2, 2), labels)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine><NewLine>    if epoch &gt;= 2:<NewLine>        model.apply(torch.quantization.disable_observer)<NewLine>        pass<NewLine><NewLine>    if epoch &gt;= 3:<NewLine>        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    quant_model = deepcopy(model)<NewLine>    quant_model = torch.quantization.convert(quant_model.eval().cpu(), inplace=False)<NewLine><NewLine>    with torch.no_grad():<NewLine>        inp = torch.zeros([1, 3, 28, 28], device='cpu')<NewLine>        model.eval().cpu()<NewLine>        quant_model.eval().cpu()<NewLine>        qout = quant_model.forward(inp)<NewLine>        out = model.forward(inp)<NewLine>        print(qout.view(2).tolist())<NewLine>        print(out.view(2).tolist())<NewLine><NewLine>        model.to(device)<NewLine></code></pre><NewLine></details><NewLine><p>I printed the state_dicts of the not converted and converted models if the output from forwarding zeros diverged/stayed the same:</p><NewLine><details><NewLine><summary> diverged </summary><NewLine><pre><code class=""lang-auto""># Forward zeros output<NewLine>[0.28445518016815186, 0.3933817744255066] #not converted<NewLine>[0.07719799876213074, 0.6743956208229065] #converted<NewLine><NewLine>&lt;================ NON-CONVERTED MODEL ====================&gt;<NewLine>OrderedDict([('backbone0.0.activation_post_process.scale', tensor([0.0077])), ('backbone0.0.activation_post_process.zero_point', tensor([0])), ('backbone0.0.activation_post_process.activation_post_process.min_val', tensor(0.0006)), ('backbone0.0.activation_post_process.activation_post_process.max_val', tensor(0.9801)), ('backbone0.1.weight', tensor([[[[-4.4932]],<NewLine><NewLine>         [[ 3.9661]],<NewLine><NewLine>         [[-4.1858]]]])), ('backbone0.1.activation_post_process.scale', tensor([0.0039])), ('backbone0.1.activation_post_process.zero_point', tensor([127])), ('backbone0.1.activation_post_process.activation_post_process.min_val', tensor(-0.4932)), ('backbone0.1.activation_post_process.activation_post_process.max_val', tensor(0.0016)), ('backbone0.1.weight_fake_quant.scale', tensor([0.0028])), ('backbone0.1.weight_fake_quant.zero_point', tensor([0])), ('backbone0.1.weight_fake_quant.activation_post_process.min_vals', tensor([-0.3583])), ('backbone0.1.weight_fake_quant.activation_post_process.max_vals', tensor([0.0458])), ('backbone0.2.weight', tensor([4.2045])), ('backbone0.2.bias', tensor([-0.3767])), ('backbone0.2.running_mean', tensor([-0.1198])), ('backbone0.2.running_var', tensor([0.3677])), ('backbone0.2.num_batches_tracked', tensor(10)), ('backbone0.2.activation_post_process.scale', tensor([0.0341])), ('backbone0.2.activation_post_process.zero_point', tensor([59])), ('backbone0.2.activation_post_process.activation_post_process.min_val', tensor(-2.0061)), ('backbone0.2.activation_post_process.activation_post_process.max_val', tensor(2.3278)), ('backbone0.3.activation_post_process.scale', tensor([0.0184])), ('backbone0.3.activation_post_process.zero_point', tensor([0])), ('backbone0.3.activation_post_process.activation_post_process.min_val', tensor(0.0400)), ('backbone0.3.activation_post_process.activation_post_process.max_val', tensor(2.3427)), ('backbone1.0.weight', tensor([[[[-3.8434, -4.1207,  4.9514],<NewLine>          [ 5.1071, -4.7582, -3.8411],<NewLine>          [-3.6406, -4.1518, -1.5902]]],<NewLine><NewLine><NewLine>        [[[-4.2797, -4.4012,  1.1095],<NewLine>          [-5.2368,  5.8240,  0.5995],<NewLine>          [-3.5678, -3.8644,  1.4833]]]])), ('backbone1.1.weight', tensor([-2.8446,  2.8394])), ('backbone1.1.bias', tensor([ 0.2494, -0.2510])), ('backbone1.1.running_mean', tensor([-12.0386,  -4.0614])), ('backbone1.1.running_var', tensor([157.3076, 138.7101])), ('backbone1.1.num_batches_tracked', tensor(10))])<NewLine><NewLine>&lt;============== QUANT CONVERTED MODEL ===============&gt;<NewLine>OrderedDict([('backbone0.0.scale', tensor([0.0077])), ('backbone0.0.zero_point', tensor([0])), ('backbone0.1.weight', tensor([[[[-0.3597]],<NewLine><NewLine>         [[ 0.3569]],<NewLine><NewLine>         [[-0.3597]]]], size=(1, 3, 1, 1), dtype=torch.qint8,<NewLine>       quantization_scheme=torch.per_channel_affine,<NewLine>       scale=tensor([0.0028], dtype=torch.float64), zero_point=tensor([0]),<NewLine>       axis=0)), ('backbone0.1.scale', tensor(0.0039)), ('backbone0.1.zero_point', tensor(127)), ('backbone0.1.bias', None), ('backbone0.2.weight', tensor([1.])), ('backbone0.2.bias', tensor([0.])), ('backbone0.2.running_mean', tensor([0.])), ('backbone0.2.running_var', tensor([1.])), ('backbone0.2.num_batches_tracked', tensor(0)), ('backbone1.0.weight', tensor([[[[-3.8434, -4.1207,  4.9514],<NewLine>          [ 5.1071, -4.7582, -3.8411],<NewLine>          [-3.6406, -4.1518, -1.5902]]],<NewLine><NewLine><NewLine>        [[[-4.2797, -4.4012,  1.1095],<NewLine>          [-5.2368,  5.8240,  0.5995],<NewLine>          [-3.5678, -3.8644,  1.4833]]]])), ('backbone1.1.weight', tensor([-2.8446,  2.8394])), ('backbone1.1.bias', tensor([ 0.2494, -0.2510])), ('backbone1.1.running_mean', tensor([-12.0386,  -4.0614])), ('backbone1.1.running_var', tensor([157.3076, 138.7101])), ('backbone1.1.num_batches_tracked', tensor(10))])<NewLine></code></pre><NewLine></details><NewLine><details><NewLine><summary> still the same </summary><NewLine><pre><code class=""lang-auto""># Forward zeros output<NewLine>[0.4619605243206024, 0.3693790137767792] #not converted<NewLine>[0.4619605243206024, 0.3693790137767792] #converted<NewLine><NewLine>&lt;================ NON-CONVERTED MODEL ====================&gt;<NewLine>OrderedDict([('backbone0.0.activation_post_process.scale', tensor([0.0077])), ('backbone0.0.activation_post_process.zero_point', tensor([0])), ('backbone0.0.activation_post_process.activation_post_process.min_val', tensor(8.7249e-05)), ('backbone0.0.activation_post_process.activation_post_process.max_val', tensor(0.9802)), ('backbone0.1.weight', tensor([[[[4.5793]],<NewLine><NewLine>         [[3.9695]],<NewLine><NewLine>         [[4.1871]]]])), ('backbone0.1.activation_post_process.scale', tensor([0.0046])), ('backbone0.1.activation_post_process.zero_point', tensor([42])), ('backbone0.1.activation_post_process.activation_post_process.min_val', tensor(-0.1939)), ('backbone0.1.activation_post_process.activation_post_process.max_val', tensor(0.3904)), ('backbone0.1.weight_fake_quant.scale', tensor([0.0035])), ('backbone0.1.weight_fake_quant.zero_point', tensor([0])), ('backbone0.1.weight_fake_quant.activation_post_process.min_vals', tensor([-0.1654])), ('backbone0.1.weight_fake_quant.activation_post_process.max_vals', tensor([0.4444])), ('backbone0.2.weight', tensor([3.7733])), ('backbone0.2.bias', tensor([-3.2874])), ('backbone0.2.running_mean', tensor([0.4043])), ('backbone0.2.running_var', tensor([0.3758])), ('backbone0.2.num_batches_tracked', tensor(10)), ('backbone0.2.activation_post_process.scale', tensor([0.0358])), ('backbone0.2.activation_post_process.zero_point', tensor([65])), ('backbone0.2.activation_post_process.activation_post_process.min_val', tensor(-2.3070)), ('backbone0.2.activation_post_process.activation_post_process.max_val', tensor(2.2333)), ('backbone0.3.activation_post_process.scale', tensor([0.0179])), ('backbone0.3.activation_post_process.zero_point', tensor([0])), ('backbone0.3.activation_post_process.activation_post_process.min_val', tensor(0.)), ('backbone0.3.activation_post_process.activation_post_process.max_val', tensor(2.2680)), ('backbone1.0.weight', tensor([[[[ 4.3533, -3.5816,  5.0651],<NewLine>          [-4.1010, -3.6161, -4.3417],<NewLine>          [ 4.0427, -4.3517,  4.1981]]],<NewLine><NewLine><NewLine>        [[[ 2.5564, -3.3695,  4.0380],<NewLine>          [-3.9976, -7.2543, -4.0428],<NewLine>          [ 3.7343, -3.5447,  2.7283]]]])), ('backbone1.1.weight', tensor([-0.5695, -2.9817])), ('backbone1.1.bias', tensor([-0.1784, -0.1795])), ('backbone1.1.running_mean', tensor([ 0.1879, -0.5091])), ('backbone1.1.running_var', tensor([16.9781, 18.2449])), ('backbone1.1.num_batches_tracked', tensor(10))])<NewLine><NewLine>&lt;============== QUANT CONVERTED MODEL ===============&gt;<NewLine>OrderedDict([('backbone0.0.scale', tensor([0.0077])), ('backbone0.0.zero_point', tensor([0])), ('backbone0.1.weight', tensor([[[[0.4426]],<NewLine><NewLine>         [[0.4426]],<NewLine><NewLine>         [[0.4426]]]], size=(1, 3, 1, 1), dtype=torch.qint8,<NewLine>       quantization_scheme=torch.per_channel_affine,<NewLine>       scale=tensor([0.0035], dtype=torch.float64), zero_point=tensor([0]),<NewLine>       axis=0)), ('backbone0.1.scale', tensor(0.0046)), ('backbone0.1.zero_point', tensor(42)), ('backbone0.1.bias', None), ('backbone0.2.weight', tensor([1.])), ('backbone0.2.bias', tensor([0.])), ('backbone0.2.running_mean', tensor([0.])), ('backbone0.2.running_var', tensor([1.])), ('backbone0.2.num_batches_tracked', tensor(0)), ('backbone1.0.weight', tensor([[[[ 4.3533, -3.5816,  5.0651],<NewLine>          [-4.1010, -3.6161, -4.3417],<NewLine>          [ 4.0427, -4.3517,  4.1981]]],<NewLine><NewLine><NewLine>        [[[ 2.5564, -3.3695,  4.0380],<NewLine>          [-3.9976, -7.2543, -4.0428],<NewLine>          [ 3.7343, -3.5447,  2.7283]]]])), ('backbone1.1.weight', tensor([-0.5695, -2.9817])), ('backbone1.1.bias', tensor([-0.1784, -0.1795])), ('backbone1.1.running_mean', tensor([ 0.1879, -0.5091])), ('backbone1.1.running_var', tensor([16.9781, 18.2449])), ('backbone1.1.num_batches_tracked', tensor(10))])<NewLine></code></pre><NewLine></details><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you paste the code for printing as well?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>Here’s the current code where I turn on QAT only near the end of training, and still has the same issue:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn, optim<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>from copy import deepcopy<NewLine><NewLine>print(torch.__version__)<NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>        self.backbone0 = nn.Sequential(<NewLine>            QuantStub(),<NewLine>            nn.Conv2d(3, 1, 1, bias=False),<NewLine>            nn.BatchNorm2d(1),<NewLine>            nn.ReLU(),<NewLine>            DeQuantStub(),<NewLine>        )<NewLine>        self.backbone1 = nn.Sequential(<NewLine>            nn.Conv2d(1, 2, 3, stride=2, padding=1, bias=False),<NewLine>            nn.BatchNorm2d(2),<NewLine>            nn.MaxPool2d(14),<NewLine>            nn.Sigmoid(),<NewLine>        )<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.backbone0(x)<NewLine>        x = self.backbone1(x)<NewLine>        return x<NewLine><NewLine>model = Model()<NewLine><NewLine># torch.quantization.fuse_modules(model, [['1', '2', '3'], ['4', '5']], inplace=True)<NewLine><NewLine># model.backbone0.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine># torch.quantization.prepare_qat(model.backbone0, inplace=True)<NewLine><NewLine>optimizer = optim.Adam(model.parameters(), lr=1)<NewLine>model.to(device)<NewLine><NewLine>criterion = nn.BCELoss()<NewLine>for epoch in range(1000):<NewLine>    # print('EPOCH', epoch)<NewLine>    model.train()<NewLine><NewLine>    inputs = torch.rand(2, 3, 28, 28)<NewLine>    labels = torch.FloatTensor([[1, 1], [0, 0]])<NewLine><NewLine>    inputs = inputs.to(device)<NewLine>    labels = labels.to(device)<NewLine>    outputs = model(inputs)<NewLine>    loss = criterion(outputs.view(2, 2), labels)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine>    <NewLine>    if epoch == 945: # turn on qat<NewLine>        model.to('cpu')<NewLine>        model.backbone0.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>        torch.quantization.prepare_qat(model.backbone0, inplace=True)<NewLine>        model.to(device)<NewLine><NewLine>    if epoch &gt;= 950:<NewLine>        model.apply(torch.quantization.disable_observer)<NewLine>        pass<NewLine><NewLine>    if epoch &gt;= 950:<NewLine>        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    if epoch == 999:<NewLine>        # print('MODEL', model)<NewLine>        quant_model = deepcopy(model)<NewLine>        quant_model = torch.quantization.convert(quant_model.eval().cpu(), inplace=False)<NewLine><NewLine>        with torch.no_grad():<NewLine>            inp = torch.zeros([1, 3, 28, 28], device='cpu')<NewLine>            model.eval().cpu()<NewLine>            quant_model.eval().cpu()<NewLine><NewLine>            qout = quant_model.forward(inp)<NewLine>            out = model.forward(inp)<NewLine><NewLine>            print(f""&lt;============== EPOCH {epoch} ===============&gt;"")<NewLine>            print(out.view(2).tolist(), ""#not converted"")<NewLine>            print(qout.view(2).tolist(), ""#quant converted"")<NewLine>            print(f""&lt;============== NOT CONVERTED MODEL ===============&gt;"")<NewLine>            print(model.state_dict())<NewLine>            print(f""&lt;============== QUANT CONVERTED MODEL ===============&gt;"")<NewLine>            print(quant_model.state_dict())<NewLine><NewLine>            model.to(device)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like in the case when they diverged the state_dict still matches? did you enable fake quantization when you compare the result?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. It was enabled although I’m wondering if enabling it only on a sequential container layer in the module as I did is incorrect?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Updating to torch nightly from torch 1.5.1 fixed the issue! (did not try 1.6)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/kekpirat; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/kekpirat; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/kekpirat; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/kekpirat; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/kekpirat; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/kekpirat; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  7:06pm; <NewLine> REPLY_DATE 2: July 9, 2020,  7:09pm; <NewLine> REPLY_DATE 3: July 13, 2020,  5:51am; <NewLine> REPLY_DATE 4: July 13, 2020, 10:15am; <NewLine> REPLY_DATE 5: July 14, 2020,  5:00pm; <NewLine> REPLY_DATE 6: July 15, 2020,  2:54am; <NewLine> REPLY_DATE 7: July 22, 2020,  9:37pm; <NewLine> REPLY_DATE 8: July 23, 2020,  1:36am; <NewLine> REPLY_DATE 9: July 24, 2020,  9:07pm; <NewLine> REPLY_DATE 10: August 11, 2020, 10:35am; <NewLine> REPLY_DATE 11: August 12, 2020,  5:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> 
90954,Slow quantization,2020-07-29T10:56:04.905Z,4,252,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve tried to quantize a simple model with conv+bn+relu combination but it performs much slower in int8.<br/><NewLine>Am I missing something here?</p><NewLine><p><strong>Code To Reproduce</strong></p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import time<NewLine><NewLine>import torch.nn as nn<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>backend = 'qnnpack'<NewLine># backend = 'fbgemm'<NewLine>import torch<NewLine>torch.backends.quantized.engine = backend<NewLine><NewLine><NewLine>class DownBlockQ(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super().__init__()<NewLine>        self.quant_input = QuantStub()<NewLine>        self.dequant_output = DeQuantStub()<NewLine><NewLine>        self.conv1 = nn.Conv2d(in_ch, in_ch, 4, stride=2, padding=1, groups=in_ch)<NewLine>        self.bn1 = nn.BatchNorm2d(in_ch)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        self.conv2 = nn.Conv2d(in_ch, out_ch, 1)<NewLine>        self.bn2 = nn.BatchNorm2d(out_ch)<NewLine>        self.relu2 = nn.ReLU()<NewLine><NewLine>    def forward(self, x):<NewLine>        # x = self.quant_input(x)<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu1(x)<NewLine><NewLine>        x = self.conv2(x)<NewLine>        x = self.bn2(x)<NewLine>        x = self.relu2(x)<NewLine>        # x = self.dequant_output(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)<NewLine>        torch.quantization.fuse_modules(self, ['conv2', 'bn2', 'relu2'], inplace=True)<NewLine><NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self, filters=22):<NewLine>        super().__init__()<NewLine>        self.quant_input = QuantStub()<NewLine>        self.dequant_output = DeQuantStub()<NewLine><NewLine>        self.db1 = DownBlockQ(filters * 1, filters * 2)  # 128<NewLine>        self.db2 = DownBlockQ(filters * 2, filters * 4)  # 64<NewLine>        self.db3 = DownBlockQ(filters * 4, filters * 8)  # 32<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant_input(x)<NewLine>        x = self.db1(x)<NewLine>        x = self.db2(x)<NewLine>        x = self.db3(x)<NewLine>        x = self.dequant_output(x)<NewLine>        return x<NewLine><NewLine><NewLine>def fuse_model(model):<NewLine>    if hasattr(model, 'fuse_model'):<NewLine>        model.fuse_model()<NewLine><NewLine>    for p in list(model.modules())[1:]:<NewLine>        fuse_model(p)<NewLine><NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine><NewLine>def benchmark(func, iters=10, *args):<NewLine>    t1 = time.time()<NewLine>    for _ in range(iters):<NewLine>        res = func(*args)<NewLine>    print(f'{((time.time() - t1) / iters):.6f} sec')<NewLine>    return res<NewLine><NewLine><NewLine>def quantize():<NewLine>    dummy = torch.rand(1, 22, 256, 256)<NewLine>    # model = DownBlockQ(22 * 1, 22 * 2)<NewLine>    model = Model(filters=22)<NewLine>    model = model.eval()<NewLine>    print(""Before quantization"")<NewLine>    print_size_of_model(model)<NewLine><NewLine>    benchmark(model, 20, dummy)<NewLine>    # print(model)<NewLine>    fuse_model(model)<NewLine><NewLine>    model.qconfig = torch.quantization.get_default_qconfig(backend)<NewLine>    # print(model.qconfig)<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine>    # print(model)<NewLine>    print(""After quantization"")<NewLine>    print_size_of_model(model)<NewLine>    benchmark(model, 20, dummy)<NewLine>    # torch.jit.script(model).save('models/model_scripted.pt')<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    quantize()<NewLine></code></pre><NewLine><h2>Expected behavior</h2><NewLine><p>Int8 model to be 2-3 times faster than float32.</p><NewLine><h2>Environment</h2><NewLine><p>PyTorch version: 1.7.0.dev20200727<br/><NewLine>Is debug build: No<br/><NewLine>CUDA used to build PyTorch: 10.2</p><NewLine><p>OS: Ubuntu 20.04 LTS<br/><NewLine>GCC version: (Ubuntu 8.4.0-3ubuntu2) 8.4.0<br/><NewLine>CMake version: version 3.16.3</p><NewLine><p>Python version: 3.7<br/><NewLine>Is CUDA available: Yes<br/><NewLine>CUDA runtime version: 10.2.89<br/><NewLine>GPU models and configuration: GPU 0: GeForce GTX 1070<br/><NewLine>Nvidia driver version: 440.100<br/><NewLine>cuDNN version: Could not collect</p><NewLine><p>Versions of relevant libraries:<br/><NewLine>[pip3] numpy==1.19.0<br/><NewLine>[pip3] torch==1.7.0.dev20200727<br/><NewLine>[pip3] torchvision==0.8.0.dev20200727<br/><NewLine>[conda] Could not collect</p><NewLine></div>",https://discuss.pytorch.org/u/dklvch,(Kelvich Daniel),dklvch,"July 29, 2020, 10:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for flagging, the input sizes to the conv layers seem a bit unconventional so I’m wondering if that is causing a slowdown. Are these sizes part of an actual model?<br/><NewLine>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a></p><NewLine><p>I tried printing the model</p><NewLine><pre><code class=""lang-auto"">Model(<NewLine>  (quant_input): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)<NewLine>  (dequant_output): DeQuantize()<NewLine>  (db1): DownBlockQ(<NewLine>    (quant_input): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)<NewLine>    (dequant_output): DeQuantize()<NewLine>    (conv1): QuantizedConvReLU2d(22, 22, kernel_size=(4, 4), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=22)<NewLine>    (bn1): Identity()<NewLine>    (relu1): Identity()<NewLine>    (conv2): QuantizedConvReLU2d(22, 44, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)<NewLine>    (bn2): Identity()<NewLine>    (relu2): Identity()<NewLine>  )<NewLine>  (db2): DownBlockQ(<NewLine>    (quant_input): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)<NewLine>    (dequant_output): DeQuantize()<NewLine>    (conv1): QuantizedConvReLU2d(44, 44, kernel_size=(4, 4), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=44)<NewLine>    (bn1): Identity()<NewLine>    (relu1): Identity()<NewLine>    (conv2): QuantizedConvReLU2d(44, 88, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)<NewLine>    (bn2): Identity()<NewLine>    (relu2): Identity()<NewLine>  )<NewLine>  (db3): DownBlockQ(<NewLine>    (quant_input): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)<NewLine>    (dequant_output): DeQuantize()<NewLine>    (conv1): QuantizedConvReLU2d(88, 88, kernel_size=(4, 4), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=88)<NewLine>    (bn1): Identity()<NewLine>    (relu1): Identity()<NewLine>    (conv2): QuantizedConvReLU2d(88, 176, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)<NewLine>    (bn2): Identity()<NewLine>    (relu2): Identity()<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think kernel size = 4 and dewpthwise conv is the culprit here. Quanatized depthwise is optimized mainly for common kernel sizes 3 and 5. Just to reiterate <a class=""mention"" href=""/u/supriyar"">@supriyar</a>’s question: Is there any reason to use kernel  size 4?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> <a class=""mention"" href=""/u/supriyar"">@supriyar</a> Thanks for your replies! Yes, it is a part of an actual model. So it is very undesirable to change it. I’ve tried convs with kernels 3 and 5 but even with such config int8 slower than float32.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>For kernel sizes 3 and 5 I can take a look at it to see why it’s slow.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please, take a look:</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>import time<NewLine><NewLine>import torch.nn as nn<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine># backend = 'qnnpack'<NewLine>backend = 'fbgemm'<NewLine>import torch<NewLine><NewLine>torch.backends.quantized.engine = backend<NewLine><NewLine><NewLine>class DownBlockQ(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1, groups=in_ch)<NewLine>        self.bn1 = nn.BatchNorm2d(in_ch)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        self.conv2 = nn.Conv2d(in_ch, out_ch, 1)<NewLine>        self.bn2 = nn.BatchNorm2d(out_ch)<NewLine>        self.relu2 = nn.ReLU()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu1(x)<NewLine><NewLine>        x = self.conv2(x)<NewLine>        x = self.bn2(x)<NewLine>        x = self.relu2(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)<NewLine>        torch.quantization.fuse_modules(self, ['conv2', 'bn2', 'relu2'], inplace=True)<NewLine><NewLine><NewLine>class Model(nn.Module):<NewLine>    def __init__(self, filters=22, quant=True):<NewLine>        super().__init__()<NewLine>        self.quant = quant<NewLine>        self.quant_input = QuantStub()<NewLine>        self.dequant_output = DeQuantStub()<NewLine><NewLine>        self.db1 = DownBlockQ(filters * 1, filters * 2)  # 128<NewLine>        self.db2 = DownBlockQ(filters * 2, filters * 4)  # 64<NewLine>        self.db3 = DownBlockQ(filters * 4, filters * 8)  # 32<NewLine><NewLine>    def forward(self, x):<NewLine>        if self.quant:<NewLine>            x = self.quant_input(x)<NewLine>        x = self.db1(x)<NewLine>        x = self.db2(x)<NewLine>        x = self.db3(x)<NewLine>        if self.quant:<NewLine>            x = self.dequant_output(x)<NewLine>        return x<NewLine><NewLine><NewLine>def fuse_model(model):<NewLine>    if hasattr(model, 'fuse_model'):<NewLine>        model.fuse_model()<NewLine><NewLine>    for p in list(model.modules())[1:]:<NewLine>        fuse_model(p)<NewLine><NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine><NewLine>def benchmark(func, iters=10, *args):<NewLine>    t1 = time.time()<NewLine>    for _ in range(iters):<NewLine>        res = func(*args)<NewLine>    print(f'{((time.time() - t1) / iters):.6f} sec')<NewLine>    return res<NewLine><NewLine><NewLine>def quantize():<NewLine>    dummy = torch.rand(1, 22, 256, 256)<NewLine>    model = Model(filters=22, quant=False).eval()<NewLine>    print(""Before quantization"")<NewLine>    print_size_of_model(model)<NewLine>    benchmark(model, 20, dummy)<NewLine><NewLine>    # print(model)<NewLine>    model = Model(filters=22, quant=True).eval()<NewLine>    fuse_model(model)<NewLine><NewLine>    model.qconfig = torch.quantization.get_default_qconfig(backend)<NewLine>    # print(model.qconfig)<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine>    print(""After quantization"")<NewLine>    print_size_of_model(model)<NewLine>    benchmark(model, 20, dummy)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    quantize()<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dklvch"">@dklvch</a>,</p><NewLine><p>Int8 depthwise convolution is very slow when filters is not a multiple of 8. Could you try with filters = 16 or 24?</p><NewLine><pre><code class=""lang-auto"">dummy = torch.rand(1, 22, 256, 256) =&gt; dummy = torch.rand(1, 24, 256, 256)<NewLine>model = Model(filters=22, quant=False).eval() =&gt; model = Model(filters=24, quant=False).eval()<NewLine>model = Model(filters=22, quant=True).eval() =&gt; model = Model(filters=24, quant=True).eval()<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dklvch; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dklvch; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: July 29, 2020,  4:50pm; <NewLine> REPLY_DATE 2: July 29, 2020,  6:20pm; <NewLine> REPLY_DATE 3: July 30, 2020, 12:02pm; <NewLine> REPLY_DATE 4: July 30, 2020,  4:48pm; <NewLine> REPLY_DATE 5: July 31, 2020,  7:16am; <NewLine> REPLY_DATE 6: August 4, 2020,  1:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
84588,[Nightly] Packed params no longer returned via state_dict() method,2020-06-08T04:08:23.381Z,3,176,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’ve installed the nightly build 1.6.0.dev20200607 today and ran my scripts that exercises quantization and jit.</p><NewLine><p>Until v1.5, I was able to get all packed params of a top level quantized module, say quantized resnet in torchvision, via state_dict() method. But now with nightly, I only get <code>quant.scale</code> and <code>quant.zero_point</code> from the same module.</p><NewLine><p>I also noticed that a packed param is now an instance of <code>torch._C.ScriptObject</code>, instead of QTensor as was the case until v1.5</p><NewLine><p>How do I get all parameters from quantized + jitted model now? Can you point me to github issues/PRs that introduced relevant changes?</p><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a></p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"June 8, 2020,  4:08am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, <a href=""https://github.com/pytorch/pytorch/pull/35923"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/35923</a> and <a href=""https://github.com/pytorch/pytorch/pull/34140"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/34140</a> are relevant changes.</p><NewLine><p>We are using TorchBind object for the packed params now.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, I’ll take a look. Seems like a big change.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>,</p><NewLine><p>Now that v1.6 is out, I came back to this issue. As I mentioned,  <code>state_dict()</code> method on traced quantized networks like qresnet from torchvision no longer returns all quantized parameters.</p><NewLine><p>After some digging and thanks to the onnx export implementation below, I found that I can use <code>torch._C._jit_pass_lower_graph(graph, model._c)</code> to get at quantized paremeters I’ve been looking for. Is this the recommend way for third party pkg like TVM to get quantized parameters? Having to pass <code>model._c</code> seems like a very internal API…</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/onnx/utils.py#L357"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/torch/onnx/utils.py#L357"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/torch/onnx/utils.py#L357</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""347"" style=""counter-reset: li-counter 346 ;""><NewLine><li><NewLine></li><li>if isinstance(example_outputs, torch.Tensor):</li><NewLine><li>    example_outputs = [example_outputs]</li><NewLine><li><NewLine></li><li>torch_out = None</li><NewLine><li>if isinstance(model, torch.jit.ScriptModule):</li><NewLine><li>    assert example_outputs is not None, ""example_outputs must be provided when exporting a ScriptModule""</li><NewLine><li>    try:</li><NewLine><li>        graph = model.forward.graph</li><NewLine><li>        torch._C._jit_pass_onnx_function_substitution(graph)</li><NewLine><li class=""selected"">        method_graph, params = torch._C._jit_pass_lower_graph(graph, model._c)</li><NewLine><li>        in_vars, in_desc = torch.jit._flatten(tuple(args) + tuple(params))</li><NewLine><li>        graph = _propagate_and_assign_input_shapes(</li><NewLine><li>            method_graph, tuple(in_vars), False, propagate)</li><NewLine><li>    except AttributeError:</li><NewLine><li>        raise RuntimeError('\'forward\' method must be a script method')</li><NewLine><li>elif isinstance(model, torch.jit.ScriptFunction):</li><NewLine><li>    assert example_outputs is not None, ""example_outputs must be provided when exporting a TorchScript ScriptFunction""</li><NewLine><li>    method = model</li><NewLine><li>    params = ()</li><NewLine><li>    in_vars, in_desc = torch.jit._flatten(tuple(args))</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/james_reed"">@James_Reed</a> can you take a look?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>does the conv packed params/linear packed params appear in state_dict? you can call unpack on these object to get the parameters I think: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/conv_packed_params.h#L17"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/conv_packed_params.h#L17</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok here is the test script.</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torchvision.models.quantization import mobilenet as qmobilenet<NewLine><NewLine><NewLine>def quantize_model(model, inp):<NewLine>    model.fuse_model()<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    model(inp)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine><NewLine>qmodel = qmobilenet.mobilenet_v2(pretrained=True).eval()<NewLine><NewLine>pt_inp = torch.rand((1, 3, 224, 224))<NewLine>quantize_model(qmodel, pt_inp)<NewLine>script_module = torch.jit.trace(qmodel, pt_inp).eval()<NewLine><NewLine>graph = script_module.graph<NewLine>print(script_module.state_dict())<NewLine>_, params = torch._C._jit_pass_lower_graph(graph, script_module._c)<NewLine></code></pre><NewLine><p>The model is quantized mobilenet v2 from torchvision. The output of <code>state_dict()</code> from above script is different between v1.6 and v1.5.1:</p><NewLine><ul><NewLine><li>With v1.5.1, all packed quantized parameters (conv and linear) are returned. Unpacking is also no problem.</li><NewLine><li>With v1.6, I only get <code>OrderedDict([('quant.scale', tensor([0.0079])), ('quant.zero_point', tensor([0]))]) </code>, so there is nothing that can be unpacked.</li><NewLine></ul><NewLine><p>In both version, the last line in the above script, <code>torch._C._jit_pass_lower_graph(graph, script_module._c)</code>, returns all quantized parameters. So technically my original problem is solved. My question is if this is an expected behavior.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""84588"" data-username=""masahi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/masahi/40/19725_2.png"" width=""20""/> masahi:</div><NewLine><blockquote><NewLine><p>In both version, the last line in the above script, <code>torch._C._jit_pass_lower_graph(graph, script_module._c)</code> , returns all quantized parameters. So technically my original problem is solved. My question is if this is an expected behavior.</p><NewLine></blockquote><NewLine></aside><NewLine><p>probably not, I’ll create an issue for this, this for reporting</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 9, 2020,  1:31am; <NewLine> REPLY_DATE 2: June 9, 2020,  2:54am; <NewLine> REPLY_DATE 3: July 30, 2020, 10:54pm; <NewLine> REPLY_DATE 4: July 31, 2020, 12:07am; <NewLine> REPLY_DATE 5: July 31, 2020,  1:05am; <NewLine> REPLY_DATE 6: July 31, 2020,  1:28am; <NewLine> REPLY_DATE 7: August 3, 2020, 11:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> 
90948,How to freeze the FakeQuantize zero_point during train,2020-07-29T09:53:31.588Z,0,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I need to train a quantized model which has 0 offset due to limitations of my inference framework.<br/><NewLine>I’m following the flow described in <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a><br/><NewLine>So the model is prepared with prepare_qat which adds FakeQuantize  layers<br/><NewLine>The problem is that both scale and zero_point are being trained. I need the zero_point to be fixed at 0.</p><NewLine></div>",https://discuss.pytorch.org/u/Artak_Arakelyan,(Artak Arakelyan),Artak_Arakelyan,"July 29, 2020,  9:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The scale and zero_point aren’t trained - they are calculated by observers inserted in the network. You can implement an observer specific to your use-case which will fix the zero_point at 0. For reference the zero_point calculation happens in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L187"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L187</a><br/><NewLine>Observers are set when you initialize the qconfig (in this case you seem to be using the default. i.e. <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/qconfig.py#L90"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/qconfig.py#L90</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your input.</p><NewLine><p>Do you know a good example of applying custom observers ?<br/><NewLine>qconfig = QConfig(activation=FakeQuantize.with_args(observer=,<br/><NewLine>quant_min=0,<br/><NewLine>quant_max=255,<br/><NewLine>reduce_range=True),<br/><NewLine>weight=default_per_channel_weight_fake_quant)<br/><NewLine>Is this enough or setting a custom observer or there are some nuances ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can follow any of the observers defined in [<a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py</a>] as a starting point.</p><NewLine><p>To enable it in the qconfig you can do<br/><NewLine><code>FakeQuantize.with_args(observer=MyObserver, quant_min=0, quant_max=255, dtype=torch.qint8, qscheme=torch.per_tensor_affine)</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Artak_Arakelyan; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: July 29, 2020,  5:00pm; <NewLine> REPLY_DATE 2: July 29, 2020,  5:19pm; <NewLine> REPLY_DATE 3: August 21, 2020, 10:24pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
91114,Not able to run quantized model on android,2020-07-30T14:47:28.880Z,0,127,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a quantized model which works on intel cpu and can be traced but fails to be run on android. Float32 model works fine on mobile though. Unfortunately, I cannot share the model. I get the following error:</p><NewLine><p><code>java.lang.IllegalArgumentException: at::Tensor scalar type is not supported on java side</code></p><NewLine><h2>Environment</h2><NewLine><p>PyTorch version: 1.7.0.dev20200727<br/><NewLine>Is debug build: No<br/><NewLine>CUDA used to build PyTorch: 10.2</p><NewLine><p>OS: Ubuntu 20.04 LTS<br/><NewLine>GCC version: (Ubuntu 8.4.0-3ubuntu2) 8.4.0<br/><NewLine>CMake version: version 3.16.3</p><NewLine><p>Python version: 3.7<br/><NewLine>Is CUDA available: Yes<br/><NewLine>CUDA runtime version: 10.2.89<br/><NewLine>GPU models and configuration: GPU 0: GeForce GTX 1070<br/><NewLine>Nvidia driver version: 440.100<br/><NewLine>cuDNN version: Could not collect</p><NewLine><p>Versions of relevant libraries:<br/><NewLine>[pip3] numpy==1.19.0<br/><NewLine>[pip3] torch==1.7.0.dev20200727<br/><NewLine>[pip3] torchvision==0.8.0.dev20200727<br/><NewLine>[conda] Could not collect</p><NewLine><p>cc <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> <span class=""mention"">@jianyuh</span> <a class=""mention"" href=""/u/dzhulgakov"">@dzhulgakov</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <span class=""mention"">@jamesr66a</span> <span class=""mention"">@vkuzo</span></p><NewLine></div>",https://discuss.pytorch.org/u/dklvch,(Kelvich Daniel),dklvch,"July 30, 2020,  2:48pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Most likely you’re trying to return a quantized tensor (we should improve the error message <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/> ). We don’t have Java binding for quantized tensor yet. You can try to dequantize the tensor within your model (something like <code>result.dequantize()</code>) or return individual components of the tensor (<code>result.int_repr(), result.q_scale(), result.q_zero_point()</code>)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dzhulgakov; <NewLine> ,"REPLY_DATE 1: August 21, 2020, 10:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
59754,Onnx export failed int8 model,2019-11-01T08:30:51.477Z,11,907,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is quantize_per_tensor not supported by ONNX? Will more ops(like PReLU) be supported by nn.quantized?</p><NewLine></div>",https://discuss.pytorch.org/u/Hao_ZHANG,(Hao ZHANG),Hao_ZHANG,"November 1, 2019,  8:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s not yet supported, we are still figuring out the plan for quantization support in ONNX.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>pytorch1.4.0 is supported for  quantized for onnx？</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> has tested the quantization in onnx with one of our internal models, but I’m not sure about the long term plans for that. <a class=""mention"" href=""/u/supriyar"">@supriyar</a> can you comment?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The support that exists currently is for Pytorch -&gt; ONNX -&gt; Caffe2 path. The intermediate onnx operators contain references to the C2 ops so cannot be executed standalone in ONNX. See <a href=""https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py</a> for  more info.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’ve read your answer, but I am confused. You need first an onnx model which you later convert to caffe2. But if I get an error when exporting to onnx, how I can get to second step?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>could you paste the error message?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I installed the nightly version of Pytorch.</p><NewLine><p>torch.quantization.convert(model, inplace=True)<br/><NewLine>torch.onnx.export(model, img, “8INTmodel.onnx”, verbose=True)</p><NewLine><pre><code class=""lang-auto""><NewLine>Traceback (most recent call last):<NewLine>  File ""check_conv_op.py"", line 92, in &lt;module&gt;<NewLine>    quantize(img)<NewLine>  File ""check_conv_op.py"", line 59, in quantize<NewLine>    torch.onnx.export(model, img, ""8INTmodel.onnx"", verbose=True)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 168, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 69, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 485, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 334, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 282, in _trace_and_get_graph_from_model<NewLine>    orig_state_dict_keys = _unique_state_dict(model).keys()<NewLine>  File ""/usr/local/lib/python3.7/site-packages/torch/jit/__init__.py"", line 302, in _unique_state_dict<NewLine>    filtered_dict[k] = v.detach()<NewLine>AttributeError: 'torch.dtype' object has no attribute 'detach'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""59754"" data-username=""dassima""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dassima/40/20474_2.png"" width=""20""/> dassima:</div><NewLine><blockquote><NewLine><p>filtered_dict[k] = v.detach()</p><NewLine></blockquote><NewLine></aside><NewLine><p>looks like it’s calling detach on a dtype object, could you paste <code>check_conv_op.py</code>?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dassima"">@dassima</a> and <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> - did you manage to get to the bottom of this? I’m seeing exactly the same error. A simple model exports fine without quantization.</p><NewLine><p>Setting a break on the point of failure, I’m seeing the object to be detached is <strong>torch.qint8</strong></p><NewLine><p>Then dumping the state_dict for both non-quantized and quantized versions, the quantized version has this as an entry - <strong>(‘fc1._packed_params.dtype’, torch.qint8)</strong>. The non quantized version has only tensors.</p><NewLine><p>Any thoughts as to what’s going on greatly appreciated!</p><NewLine><p>Thanks.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>it’s probably because of this: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L60"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L60</a></p><NewLine><p>what version of pytorch are you using? if you update to nightly the problem should be gone since we changed the serialization format for linear: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L220"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.py#L220</a></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Many thanks for getting back.</p><NewLine><p>I was on 1.5.1 but just pulled <strong>1.7.0.dev20200705+cpu</strong> but alas, still no joy.</p><NewLine><p>Anything I can do to help debug this?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>, any ideas on next steps? Not sure if it’s something I’m doing incorrectly or a general problem with exporting.</p><NewLine><p>Many thanks.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>are you getting the same error message after updating to nightly?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>, yes. Updated initially to <strong>1.7.0.dev20200705+cpu</strong> and just tried <strong>torch-1.7.0.dev20200724+cpu</strong>. No luck with either.</p><NewLine><p>As I hijacked an old thread, I thought best to raise a separate issue with a simple example (single fully connected layer) to replicate -</p><NewLine><aside class=""quote quote-modified"" data-post=""1"" data-topic=""90019""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/letter_avatar_proxy/v4/letter/g/f05b48/40.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/simple-quantized-model-doesnt-export-to-onnx/90019"">Simple quantized model doesn't export to ONNX</a> <a class=""badge-wrapper bullet"" href=""/c/quantization/17""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is for questions, discussion and issues related to PyTorch’s quantization feature."">quantization</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hello, I’m having problems exporting a very simple quantized model to ONNX. The error message I’m seeing is - <NewLine>AttributeError: 'torch.dtype' object has no attribute 'detach' <NewLine>The cause of this is that (‘fc1._packed_params.dtype’, torch.qint8) is ends up in the state_dict. <NewLine>I asked on a previous (and old) thread if there was a solution and the answer was that this could be solved in the latest version of PyTorch. So I installed 1.7.0.dev20200705+cpu, but no joy. <NewLine>I’ve pasted the example below. <NewLine>A…<NewLine>  </blockquote><NewLine></aside><NewLine><p>I’ve had one reply with comment explaining that exporting of quantized models is not yet supported and a link to another thread. Sounds like it’s WIP. Would be good to get your take on the example in the other thread.</p><NewLine><p>Many thanks again.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group quote-modified"" data-post=""15"" data-topic=""59754"" data-username=""G4V""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/g/f05b48/40.png"" width=""20""/> G4V:</div><NewLine><blockquote><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><p>Hello, I’m having problems exporting a very simple quantized model to ONNX. The error message I’m seeing is - AttributeError: ‘torch.dtype’ object has no attribute ‘detach’ The cause of this is that (‘fc1._packed_params.dtype’, torch.qint8) is ends up in the state_dict. I asked on a previous (and old) thread if there was a solution and the answer was that this could be solved in the latest version of PyTorch. So I installed 1.7.0.dev20200705+cpu, but no joy. I’ve pasted the example below. A…</p><NewLine></blockquote><NewLine></aside><NewLine></blockquote><NewLine></aside><NewLine><p>cc <a class=""mention"" href=""/u/supriyar"">@supriyar</a> is quantized Linear supported in ONNX?</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>what is the error message? i think linear is supported according to <a href=""https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py</a></p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>How are you exporting the quantized model to ONNX? Like previously mentioned we only currently support a custom conversion flow through ONNX to Caffe2 for quantized models. The models aren’t represented in native ONNX format, but a format specific to Caffe2.<br/><NewLine>If you wish to export model to caffe2, you can follow the steps here to do so (model needs to be traced first and need to set operator_export_type to ONNX_ATEN_FALLBACK)</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/test/onnx/test_pytorch_onnx_caffe2_quantized.py#L17-L35"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/test/onnx/test_pytorch_onnx_caffe2_quantized.py#L17-L35"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/test/onnx/test_pytorch_onnx_caffe2_quantized.py#L17-L35</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""17"" style=""counter-reset: li-counter 16 ;""><NewLine><li>torch.backends.quantized.engine = ""qnnpack""</li><NewLine><li>pt_inputs = tuple(torch.from_numpy(x) for x in sample_inputs)</li><NewLine><li>model.qconfig = torch.quantization.get_default_qconfig('qnnpack')</li><NewLine><li>q_model = torch.quantization.prepare(model, inplace=False)</li><NewLine><li>q_model = torch.quantization.convert(q_model, inplace=False)</li><NewLine><li><NewLine></li><li>traced_model = torch.jit.trace(q_model, pt_inputs)</li><NewLine><li>buf = io.BytesIO()</li><NewLine><li>torch.jit.save(traced_model, buf)</li><NewLine><li>buf.seek(0)</li><NewLine><li>q_model = torch.jit.load(buf)</li><NewLine><li><NewLine></li><li>q_model.eval()</li><NewLine><li>output = q_model(*pt_inputs)</li><NewLine><li><NewLine></li><li>f = io.BytesIO()</li><NewLine><li>torch.onnx.export(q_model, pt_inputs, f, input_names=input_names, example_outputs=output,</li><NewLine><li>                  operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)</li><NewLine><li>f.seek(0)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/zif520; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/G4V; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: November 1, 2019,  6:04pm; <NewLine> REPLY_DATE 2: January 19, 2020,  4:06am; <NewLine> REPLY_DATE 3: February 14, 2020,  6:37pm; <NewLine> REPLY_DATE 4: February 22, 2020,  1:02am; <NewLine> REPLY_DATE 5: February 24, 2020,  2:48pm; <NewLine> REPLY_DATE 6: February 24, 2020, 11:10pm; <NewLine> REPLY_DATE 7: February 25, 2020,  2:37pm; <NewLine> REPLY_DATE 8: March 3, 2020,  3:07am; <NewLine> REPLY_DATE 9: July 14, 2020,  8:32am; <NewLine> REPLY_DATE 10: July 14, 2020,  4:55pm; <NewLine> REPLY_DATE 11: July 14, 2020,  5:47pm; <NewLine> REPLY_DATE 12: July 19, 2020,  2:17pm; <NewLine> REPLY_DATE 13: July 22, 2020,  9:27pm; <NewLine> REPLY_DATE 14: July 25, 2020,  7:34am; <NewLine> REPLY_DATE 15: July 30, 2020,  3:53pm; <NewLine> REPLY_DATE 16: July 30, 2020,  3:55pm; <NewLine> REPLY_DATE 17: August 21, 2020, 10:26pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: 1 Like; <NewLine> 
82244,[ONNX] Quantized fused Conv2d won&rsquo;t trace,2020-05-20T21:28:02.355Z,12,475,"<div class=""post"" itemprop=""articleBody""><NewLine><p><span class=""hashtag"">#onnx</span> <a class=""hashtag"" href=""/c/jit/13"">#<span>jit</span></a> <a class=""hashtag"" href=""/c/quantization/17"">#<span>quantization</span></a></p><NewLine><p>Hi, I am very confused.</p><NewLine><p>While tracing to ONNX my quantized model faced an error. This happens with fused <code>QuantizedConvReLU2d</code>. I use <code>OperatorExportTypes.ONNX_ATEN_FALLBACK</code>.<br/><NewLine>Pytorch version is <code>1.6.0.dev20200520</code></p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/caffe2_converter.py"", line 115, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 144, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 63, in export_onnx_model<NewLine>    export_params=True,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 530, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 366, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 319, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 284, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 577, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 372, in forward<NewLine>    self._force_outplace,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 358, in wrapper<NewLine>    outs.append(self.inner(*trace_inputs))<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/contextlib.py"", line 74, in inner<NewLine>    return func(*args, **kwds)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_modeling.py"", line 319, in forward<NewLine>    features = self._wrapped_model.backbone(images.tensor)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/DensePose_ADASE/densepose/modeling/quantize_caffe2.py"", line 166, in new_forward<NewLine>    p5, p4, p3, p2 = self.bottom_up(x)  # top-&gt;down<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/timm/models/efficientnet.py"", line 350, in forward<NewLine>    x = self.conv_stem(x)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py"", line 71, in forward<NewLine>    input, self._packed_params, self.scale, self.zero_point)<NewLine>RuntimeError: Tried to trace &lt;__torch__.torch.classes.quantized.Conv2dPackedParamsBase object at 0x5600474e9670&gt; but it is not part of the active trace. Modules that are called during a trace must be registered <NewLine>as submodules of the thing being traced.<NewLine></code></pre><NewLine><p>May presense of <code>pre_forward</code> hooks in <code>self.bottom_up(x)</code> (but not the <code>self.conv_stem(x)</code>) affect tracing such way?<br/><NewLine>Model were QAT with preserving hooks from commit <a href=""https://github.com/pytorch/pytorch/pull/37233"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/37233</a><br/><NewLine>Also PT -&gt; ONNX -&gt; Caffe2 exporting works on this very model without quantization patching</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"May 20, 2020,  9:33pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>P.S. here’s also a warning</p><NewLine><pre><code class=""lang-auto"">/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/quantized/modules/utils.py:10: UserWarning: 0quantize_tensor_per_tensor_affine current rounding mode is not set to round-to-nearest-ties-to-e<NewLine>ven (FE_TONEAREST). This will cause accuracy issues in quantized models. (Triggered internally at  /opt/conda/conda-bld/pytorch_1589958443755/work/aten/src/ATen/native/quantized/affine_quantizer.cpp:25.)<NewLine>  float(wt_scale), int(wt_zp), torch.qint8)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/james_reed"">@James_Reed</a> is this related to TorchBind object?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a> could you give a minimal repo of the issue?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> <a class=""mention"" href=""/u/james_reed"">@James_Reed</a></p><NewLine><p>I’ve prepared a repro. It might be not minimal but it mocks the pipeline I use.<br/><NewLine>Two files:</p><NewLine><pre><code class=""lang-auto""># network.py<NewLine><NewLine>import torch<NewLine><NewLine>class ConvModel(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(ConvModel, self).__init__()<NewLine>        self.conv_stem = torch.nn.Conv2d(<NewLine>            3, 5, 2, bias=True<NewLine>        ).to(dtype=torch.float)<NewLine><NewLine>        self.bn1 = torch.nn.BatchNorm2d(5)<NewLine>        self.act1 = torch.nn.ReLU()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv_stem(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.act1(x)<NewLine>        return x<NewLine></code></pre><NewLine><pre><code class=""lang-auto""># actions.py<NewLine><NewLine>import torch<NewLine>import io<NewLine>import onnx<NewLine>from torch.onnx import OperatorExportTypes<NewLine><NewLine><NewLine>def ConvModel_decorate(cls):<NewLine><NewLine>    def fuse(self):<NewLine>        torch.quantization.fuse_modules(<NewLine>            self, <NewLine>            ['conv_stem', 'bn1', 'act1'], <NewLine>            inplace=True<NewLine>        )<NewLine><NewLine>    cls.fuse = fuse<NewLine>    return cls<NewLine><NewLine>def fuse_modules(module):<NewLine>    module_output = module<NewLine>    if callable(getattr(module_output, ""fuse"", None)):<NewLine>        module_output.fuse()<NewLine>    for name, child in module.named_children():<NewLine>        new_child = fuse_modules(child)<NewLine>        if new_child is not child:<NewLine>            module_output.add_module(name, new_child)<NewLine>    return module_output<NewLine><NewLine>def create_and_update_model():<NewLine>    import network<NewLine>    network.ConvModel = ConvModel_decorate(network.ConvModel)<NewLine>    model = network.ConvModel()<NewLine>    backend = 'qnnpack'<NewLine>    model = fuse_modules(model)<NewLine>    model.qconfig = torch.quantization.get_default_qat_qconfig(backend)<NewLine>    torch.backends.quantized.engine = backend<NewLine>    torch.quantization.prepare_qat(model, inplace=True)<NewLine>    model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine>    return model<NewLine><NewLine>def QAT(model):<NewLine>    N = 100<NewLine>    for idx in range(N):<NewLine>        input_tensor = torch.rand(1, 3, 6, 6)<NewLine>        model(input_tensor)<NewLine>    return model<NewLine><NewLine>if __name__ == '__main__':<NewLine>    model = create_and_update_model()<NewLine>    model = QAT(model)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    <NewLine>    model.eval()<NewLine>    inputs = torch.rand(1, 3, 6, 6)<NewLine>    # Export the model to ONNX<NewLine>    with torch.no_grad():<NewLine>        with io.BytesIO() as f:<NewLine>            torch.onnx.export(<NewLine>                model,<NewLine>                inputs,<NewLine>                f,<NewLine>                opset_version=11,<NewLine>                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,<NewLine>                verbose=True,  # NOTE: uncomment this for debugging<NewLine>                export_params=True,<NewLine>            )<NewLine>            onnx_model = onnx.load_from_string(f.getvalue())<NewLine></code></pre><NewLine><p>Error:</p><NewLine><pre><code class=""lang-auto"">(pytorch-gpu) root@ca7d6f51c4c7:~/some_detectron2# /root/anaconda2/envs/pytorch-gpu/bin/python /root/some_detectron2/min_repro/actions.py<NewLine>/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/quantized/modules/utils.py:10: UserWarning: 0quantize_tensor_per_tensor_affine current rounding mode is not set to round-to-nearest-ties-to-even (FE_TONEAREST). This will cause accuracy issues in quantized models. (Triggered internally at  /opt/conda/conda-bld/pytorch_1589958443755/work/aten/src/ATen/native/quantized/affine_quantizer.cpp:25.)<NewLine>  float(wt_scale), int(wt_zp), torch.qint8)<NewLine>/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py:243: UserWarning: `add_node_names' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `add_node_names` argument will be ignored.<NewLine>  ""`{}` argument will be ignored."".format(arg_name, arg_name))<NewLine>/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py:243: UserWarning: `do_constant_folding' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `do_constant_folding` argument will be ignored.<NewLine>  ""`{}` argument will be ignored."".format(arg_name, arg_name))<NewLine>Traceback (most recent call last):<NewLine>  File ""/root/some_detectron2/min_repro/actions.py"", line 65, in &lt;module&gt;<NewLine>    export_params=True,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 530, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 366, in _model_to_graph<NewLine>    graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 319, in _trace_and_get_graph_from_model<NewLine>    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 284, in _get_trace_graph<NewLine>    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 577, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 372, in forward<NewLine>    self._force_outplace,<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 358, in wrapper<NewLine>    outs.append(self.inner(*trace_inputs))<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/some_detectron2/min_repro/network.py"", line 14, in forward<NewLine>    x = self.conv_stem(x)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py"", line 71, in forward<NewLine>    input, self._packed_params, self.scale, self.zero_point)<NewLine>RuntimeError: Tried to trace &lt;__torch__.torch.classes.quantized.Conv2dPackedParamsBase object at 0x564c572bd980&gt; but it is not part of the active trace. Modules that are called during a trace must be registered as submodules of the thing being traced.<NewLine></code></pre><NewLine><pre><code class=""lang-auto""></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a>,</p><NewLine><p>First, your model does not run with the given inputs. The quantized model expects a quantized input, but <code>inputs</code> in your script is float-valued. QuantWrapper can be used to force quantization/dequantization for inputs/outputs of the model, respectively:</p><NewLine><pre><code class=""lang-auto"">@@ -31,7 +31,7 @@ def fuse_modules(module):<NewLine> def create_and_update_model():<NewLine>     import network<NewLine>     network.ConvModel = ConvModel_decorate(network.ConvModel)<NewLine>-    model = network.ConvModel()<NewLine>+    model = torch.quantization.QuantWrapper(network.ConvModel())<NewLine></code></pre><NewLine><p>Second, there’s a strange difference in behavior here between when ONNX is tracing the model and when we use the standalone TorchScript tracer. Tracing the model works fine when we use the standalone tracer. To workaround this issue, you can do this:</p><NewLine><pre><code class=""lang-auto"">@@ -54,16 +54,19 @@ if __name__ == '__main__':<NewLine>     <NewLine>     model.eval()<NewLine>     inputs = torch.rand(1, 3, 6, 6)<NewLine>+    traced = torch.jit.trace(model, (inputs,))<NewLine>+<NewLine>     # Export the model to ONNX<NewLine>     with torch.no_grad():<NewLine>         with io.BytesIO() as f:<NewLine>             torch.onnx.export(<NewLine>-                model,<NewLine>+                traced,<NewLine>                 inputs,<NewLine>                 f,<NewLine>                 opset_version=11,<NewLine>                 operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,<NewLine>                 verbose=True,  # NOTE: uncomment this for debugging<NewLine>                 export_params=True,<NewLine>+                example_outputs=traced(inputs)<NewLine>             )<NewLine>             onnx_model = onnx.load_from_string(f.getvalue())<NewLine></code></pre><NewLine><p>We will investigate this difference in tracing</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/james_reed"">@James_Reed</a></p><NewLine><p>I had knew that we should trace a model before passing it to ONNX <code>export</code>, <a href=""https://discuss.pytorch.org/t/onnx-tried-to-trace-submodule-but-it-is-not-part-of-the-active-trace/80320/3"">but</a>.<br/><NewLine>For now I know it for sure</p><NewLine><p>Could you please help me reveal what’s going on with traced model during exporting when I see the following:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/caffe2_converter.py"", line 115, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 147, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 66, in export_onnx_model<NewLine>    example_outputs=traced(inputs[0])<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 530, in _export<NewLine>    fixed_batch_size=fixed_batch_size)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 384, in _model_to_graph<NewLine>    fixed_batch_size=fixed_batch_size, params_dict=params_dict)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 171, in _optimize_graph<NewLine>    torch._C._jit_pass_onnx_unpack_quantized_weights(graph, params_dict)<NewLine>RuntimeError: quantized::conv2d_relu expected scale to be 7th input<NewLine></code></pre><NewLine><p>How could it be that layer has lost its parameters?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""82244"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p><code>expected scale to be 7th input</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>we did some refactor in this PR: <a href=""https://github.com/pytorch/pytorch/pull/35923/files"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/35923/files</a> that removed some arguments from <code>quantized::conv2d</code> related ops. Does it work for <code>quantized::conv2d</code>?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> thankyou, this allowed us to take a step forward!</p><NewLine><p>Changed fusing configuration.<br/><NewLine>Now all <code>QuantizedConvReLU2d</code> to <code>QuantizedConv2d + QuantizedReLU</code>. Don’t know whether it’s work but it <strong>produces</strong> a graph but it’s inconsistent. It causes an error that I’ve seen already.<br/><NewLine>Something wrong with produced ONNX graph.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/caffe2_converter.py"", line 115, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 147, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 66, in export_onnx_model<NewLine>    example_outputs=traced(inputs[0])<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 172, in export<NewLine>    custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 92, in export<NewLine>    use_external_data_format=use_external_data_format)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py"", line 557, in _export<NewLine>    _check_onnx_proto(proto)<NewLine>RuntimeError: Attribute 'kernel_shape' is expected to have field 'ints'<NewLine><NewLine>==&gt; Context: Bad node spec: input: ""735"" input: ""98"" input: ""99"" output: ""743"" op_type: ""Conv"" attribute { name: ""dilations"" ints: 1 ints: 1 type: INTS } attribute { name: ""group"" i: 1 type: INT } attribute { na<NewLine>me: ""kernel_shape"" type: INTS } attribute { name: ""pads"" ints: 1 ints: 1 ints: 1 ints: 1 type: INTS } attribute { name: ""strides"" ints: 1 ints: 1 type: INTS }<NewLine></code></pre><NewLine><p>This is very location where quantized output is dequantized and fed into <code>Conv</code> of <code>RPN</code>.<br/><NewLine>Here are the bits of a graph output:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>%98 : Long(1:1),<NewLine>%99 : Long(1:1),<NewLine>...<NewLine>%620 : QUInt8(1:1638400, 64:25600, 128:200, 200:1) = _caffe2::Int8Relu[Y_scale=0.045047003775835037, Y_zero_point=119](%619), scope: __module._wrapped_model.backbone/__module._wrapped_model.backbone.p2_out/__module._wrapped_model.backbone.p2_out.2 # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/quantized/functional.py:381:0<NewLine>...<NewLine>%735 : Float(1:1638400, 64:25600, 128:200, 200:1) = _caffe2::Int8Dequantize(%620), scope: __module._wrapped_model.backbone/__module._wrapped_model.backbone.dequant_out # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:74:0<NewLine>...<NewLine>%743 : Float(1:1638400, 64:25600, 128:200, 200:1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=annotate(List[int], []), pads=[1, 1, 1, 1], strides=[1, 1]](%735, %98, %99), scope: __module._wrapped_model.proposal_generator/__module._wrapped_model.proposal_generator.rpn_head/__module._wrapped_model.proposal_generator.rpn_head.conv # /root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/conv.py:374:0<NewLine></code></pre><NewLine><p>Important that eager mode model without quantization smoothly passes through this convertion pipeline and it is not data dependent.<br/><NewLine>If you are interested this is detectron2 export to Caffe2 <a href=""https://github.com/facebookresearch/detectron2/tree/master/detectron2/export"" rel=""nofollow noopener"">pipeline</a></p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""82244"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p><code>RuntimeError: quantized::conv2d_relu expected scale to be 7th input</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>Please re-try with pytorch nightly build, we recently fixed this so you shouldn’t be seeing this error anymore.</p><NewLine><aside class=""quote no-group"" data-post=""9"" data-topic=""82244"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">RuntimeError: Attribute 'kernel_shape' is expected to have field 'ints'<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Seems like the conv layer is not quantized so it produces <code>onnx::Conv</code> as opposed to the <code>_caffe2::Int8Conv</code> operator. Currently the onnx export path to caffe2 does not support partially quantized model, so it expects the entire pytorch model to be able to get quantized.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><p>I am still eager to find a solution. I’ve tried your assumptions, installed fresh build and tried again. Re-run QAT on model (just to make sure) and exporting process.</p><NewLine><p>Now it says that <code>MaxPool</code> cannot be created.</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""./tools/caffe2_converter.py"", line 114, in &lt;module&gt;<NewLine>    caffe2_model = export_caffe2_model(cfg, model, first_batch)<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 157, in export_caffe2_model<NewLine>    return Caffe2Tracer(cfg, model, inputs).export_caffe2()<NewLine>  File ""/root/some_detectron2/detectron2/export/api.py"", line 95, in export_caffe2<NewLine>    predict_net, init_net = export_caffe2_detection_model(model, inputs)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 151, in export_caffe2_detection_model<NewLine>    onnx_model = export_onnx_model(model, (tensor_inputs,))<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_export.py"", line 53, in export_onnx_model<NewLine>    traced = torch.jit.trace(model, inputs, strict=False)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 900, in trace<NewLine>    check_tolerance, strict, _force_outplace, _module_class)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1054, in trace_module<NewLine>    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, strict, _force_outplace)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/contextlib.py"", line 74, in inner<NewLine>    return func(*args, **kwds)<NewLine>  File ""/root/some_detectron2/detectron2/export/caffe2_modeling.py"", line 319, in forward<NewLine>    features = self._wrapped_model.backbone(images.tensor)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/DensePose_ADASE/densepose/modeling/quantize.py"", line 205, in new_forward<NewLine>    return {""p2"": p2_out, ""p3"": p3_out, ""p4"": p4_out, ""p5"": p5_out, ""p6"": self.top_block(p5_out)[0]}<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 575, in __call__<NewLine>    result = self._slow_forward(*input, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 561, in _slow_forward<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/root/some_detectron2/detectron2/modeling/backbone/fpn.py"", line 177, in forward<NewLine>    return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/_jit_internal.py"", line 210, in fn<NewLine>    return if_false(*args, **kwargs)<NewLine>  File ""/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/functional.py"", line 576, in _max_pool2d<NewLine>    input, kernel_size, stride, padding, dilation, ceil_mode)<NewLine>RuntimeError: createStatus == pytorch_qnnp_status_success INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1590649859799/work/aten/src/ATen/native/quantized/cpu/qpool.cpp"":313, please report a bug to PyTorch. failed to create QNNPACK MaxPool operator<NewLine><NewLine></code></pre><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/352731bd6edefbec707fbef3662f4a60934b8443/aten/src/ATen/native/quantized/cpu/qpool.cpp#L330"" rel=""nofollow noopener"">Path to the source</a></p><NewLine><p>Looks weird, why didn’t it happen earlier?</p><NewLine><p><strong>UPD</strong>: It looks like nested <code>F.max_pool2d</code> won’t quantize. Test showed that it works with <code>float32</code> after <code>convert</code></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""10"" data-topic=""82244"" data-username=""supriyar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/> supriyar:</div><NewLine><blockquote><NewLine><p>Seems like the conv layer is not quantized so it produces <code>onnx::Conv</code> as opposed to the <code>_caffe2::Int8Conv</code> operator.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Is it possible to find a workaround for now? Do I understand correctly, that it is impossible to have a network with quantize dequantize during inference in Caffe2 export?</p><NewLine><p><strong>UPD:</strong> what if we just make all Convs are quantized for ONNX.export not fail</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""82244"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p><strong>UPD:</strong> what if we just make all Convs are quantized for ONNX.export not fail</p><NewLine></blockquote><NewLine></aside><NewLine><p>If all convs in the network are quantized it should work and you will see <code>_caffe2::Int8Conv</code> ops in the converted network.</p><NewLine><aside class=""quote no-group"" data-post=""12"" data-topic=""82244"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>Do I understand correctly, that it is impossible to have a network with quantize dequantize during inference in Caffe2 export?</p><NewLine></blockquote><NewLine></aside><NewLine><p>You would have quantize dequantize at the start and end of the network. Which implies all the network ops are quantized.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you <a class=""mention"" href=""/u/supriyar"">@supriyar</a> , I see</p><NewLine><p>I have another question about non-quant operations in network</p><NewLine><p>What do you think if we register C10 export like it is done <a href=""https://github.com/pytorch/pytorch/blob/master/caffe2/operators/roi_align_op.cc#L302"" rel=""nofollow noopener"">here</a><br/><NewLine>would it be possible to patch non-quantized operators from <code>torch.nn.ConvTranspose2d</code> to <code>torch.ops._caffe2.ConvTranspose2d</code> to use them as is? Or is it better to implement quantized version of <code>nn.ConvTranspose2d</code>?</p><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> I could implement a PR of such functionality if it is valid</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think we are already working on quantized version of conv2d transpose, cc <a class=""mention"" href=""/u/zafar"">@Zafar</a></p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, can you now export the quantized model to Caffe2, and then export Caffe2 to ncnn? Thank you!</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, <a class=""mention"" href=""/u/blueskywwc"">@blueskywwc</a></p><NewLine><p>First of all I haven’t managed to export quantized network to Caffe2. I do not know is it possible to export it to ncnn. Just a suggestion, maybe it is better to export model to ONNX and than to ncnn</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>hello,<a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>   When will it be possible to support the conversion of the quantified model to onnx, I hope there is a reference time, thank you！</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p>we are not working on onnx conversions, feel free to submit PRs to add the support.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/James_Reed; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  9:30pm; <NewLine> REPLY_DATE 2: May 20, 2020,  9:32pm; <NewLine> REPLY_DATE 3: May 20, 2020, 10:22pm; <NewLine> REPLY_DATE 4: May 21, 2020, 10:06am; <NewLine> REPLY_DATE 5: May 21, 2020,  6:36pm; <NewLine> REPLY_DATE 6: May 21, 2020,  7:45pm; <NewLine> REPLY_DATE 7: May 21, 2020,  8:08pm; <NewLine> REPLY_DATE 8: May 21, 2020,  9:49pm; <NewLine> REPLY_DATE 9: May 28, 2020,  4:57pm; <NewLine> REPLY_DATE 10: May 28, 2020,  9:25pm; <NewLine> REPLY_DATE 11: May 28, 2020, 10:17pm; <NewLine> REPLY_DATE 12: May 29, 2020,  4:07am; <NewLine> REPLY_DATE 13: May 29, 2020, 10:50am; <NewLine> REPLY_DATE 14: May 29, 2020,  5:50pm; <NewLine> REPLY_DATE 15: July 22, 2020,  5:59am; <NewLine> REPLY_DATE 16: July 22, 2020, 11:06am; <NewLine> REPLY_DATE 17: July 27, 2020,  8:17am; <NewLine> REPLY_DATE 18: August 21, 2020, 10:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> 
89465,nn.MultiheadAttention fails after quantization,2020-07-16T15:54:30.444Z,6,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello all,<br/><NewLine>I try to quantize <code>nn.TransformerEncoder</code>, but get errors during inference.<br/><NewLine>The problem is with <code>nn.MultiheadAttention</code>, which is basically a set of <code>nn.Linear</code> operations and should work OK after quantization.<br/><NewLine>Minimal example:</p><NewLine><pre><code class=""lang-python"">import torch<NewLine><NewLine>mlth = torch.nn.MultiheadAttention(512, 8)<NewLine>possible_input = torch.rand((10, 10, 512))<NewLine>quatized = torch.quantization.quantize_dynamic(mlth)<NewLine>quatized(possible_input, possible_input, possible_input)<NewLine></code></pre><NewLine><p>It fails with:</p><NewLine><pre><code class=""lang-auto"">/opt/miniconda/lib/python3.7/site-packages/torch/nn/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)<NewLine>   3946     assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]<NewLine>   3947     attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)<NewLine>-&gt; 3948     attn_output = linear(attn_output, out_proj_weight, out_proj_bias)<NewLine>   3949 <NewLine>   3950     if need_weights:<NewLine><NewLine>/opt/miniconda/lib/python3.7/site-packages/torch/nn/functional.py in linear(input, weight, bias)<NewLine>   1610         ret = torch.addmm(bias, input, weight.t())<NewLine>   1611     else:<NewLine>-&gt; 1612         output = input.matmul(weight.t())<NewLine>   1613         if bias is not None:<NewLine>   1614             output += bias<NewLine><NewLine>AttributeError: 'function' object has no attribute 't'<NewLine></code></pre><NewLine><br/> <NewLine>That's because `.weight` is not parameter anymore, but the method (for components of the quantized module).<NewLine><p>You can check it like:</p><NewLine><pre><code class=""lang-auto"">mlth.out_proj.weight<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Parameter containing:<NewLine>tensor([[-0.0280,  0.0016,  0.0163,  ...,  0.0375,  0.0153, -0.0435],<NewLine>        [-0.0168,  0.0310, -0.0211,  ..., -0.0258,  0.0043, -0.0094],<NewLine>        [ 0.0412, -0.0078,  0.0262,  ...,  0.0328,  0.0439,  0.0066],<NewLine>        ...,<NewLine>        [-0.0278,  0.0337,  0.0189,  ..., -0.0402,  0.0193, -0.0163],<NewLine>        [ 0.0034, -0.0364, -0.0418,  ..., -0.0248, -0.0375, -0.0236],<NewLine>        [-0.0312,  0.0236,  0.0404,  ...,  0.0266,  0.0255,  0.0265]],<NewLine>       requires_grad=True)<NewLine></code></pre><NewLine><p>while</p><NewLine><pre><code class=""lang-auto"">quatized.out_proj.weight<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">&lt;bound method Linear.weight of DynamicQuantizedLinear(in_features=512, out_features=512, qscheme=torch.per_tensor_affine)&gt;<NewLine></code></pre><NewLine><p>Can you please guide me about this? Is it expected behavior? Should I report it to pyTorch GitHub issues?<br/><NewLine>It looks like quantization break all the module which use <code>.weight</code> inside.</p><NewLine><p>Thanks in advance</p><NewLine></div>",https://discuss.pytorch.org/u/skurzhanskyi,(Alex Skurzhanskyi),skurzhanskyi,"July 16, 2020,  3:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/skurzhanskyi"">@skurzhanskyi</a>, I am able to run your example without issues on the nighly.  What version of PyTorch are you using?  Can you check if using a more recent version / a nightly build fixes your issue?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a><br/><NewLine>Thanks for the reply. Indeed, in the nightly version, there’s no error. At the same time, <code>nn.Multihead</code> doesn’t compress, nevertheless, it’s just a set of Linear operations <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/><br/><NewLine>Is there any information regarding adding quantization to the layer (or for instance nn.Embedings)?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, currently <code>nn.MultiheadAttention</code> is not supported yet in eager mode quantization.  There are folks working on adding support for both this and embeddings quantization.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good to hear that. Is there any open information when it will be released (at least approximately)?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/skurzhanskyi"">@skurzhanskyi</a>, one other thing you could try is <a href=""https://pytorch.org/blog/pytorch-1.6-released/#graph-mode-quantization"" rel=""nofollow noopener"">https://pytorch.org/blog/pytorch-1.6-released/#graph-mode-quantization</a> , which we just released today in v1.6.  It might be easier to make multiheadattention work in graph mode.</p><NewLine><p>As far as first class quantization for <code>nn.MultiheadAttention</code> and <code>nn.EmbeddingBag / nn.Embedding</code> - we don’t have a specific timeline we can share, but it should be on the order of months (not weeks or years) - we have folks actively working on this.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> thanks a lot for your answer</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/skurzhanskyi; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/skurzhanskyi; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/skurzhanskyi; <NewLine> ,"REPLY_DATE 1: July 23, 2020, 11:48am; <NewLine> REPLY_DATE 2: July 20, 2020,  9:58pm; <NewLine> REPLY_DATE 3: July 22, 2020,  3:55pm; <NewLine> REPLY_DATE 4: July 23, 2020, 11:51am; <NewLine> REPLY_DATE 5: July 28, 2020, 10:40pm; <NewLine> REPLY_DATE 6: July 29, 2020,  4:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
90788,Variation of results,2020-07-28T07:56:35.775Z,0,62,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have quantized version of resnet18 network. When do the inference, I fed a single image from sequentially sampled data loader.<br/><NewLine>I’ve tried to printed the image tensor and input to the first convolution layer,got some mismatches.</p><NewLine><p>To print input image fed from data loader, I used:</p><NewLine><pre><code class=""lang-auto"">img = image.detach().numpy()<NewLine>img = np.transpose(img,(2,3,1,0))<NewLine></code></pre><NewLine><p>To get the input to first conv layer:</p><NewLine><pre><code class=""lang-auto"">layer_input={}<NewLine>def get_input(name):<NewLine>	def hook(model, input, output):<NewLine>		layer_input[name] = input<NewLine><NewLine>	return hook<NewLine><NewLine>model.conv1.register_forward_hook(get_input('conv1'))<NewLine><NewLine>qdm = torch.nn.quantized.DeQuantize()<NewLine>deqout = qdm( val )<NewLine>deqout = deqout.numpy()<NewLine>deqout = np.transpose( deqout, (2, 3, 1, 0) )<NewLine></code></pre><NewLine><p>Image data:</p><NewLine><blockquote><NewLine><p>tensor([[[[-0.5082, -0.3883, -0.4226,  …,  0.9303,  0.3823,  0.6392],<br/><NewLine>[-0.6281, -0.6965, -0.4397,  …,  0.8104,  0.5878,  0.2111],<br/><NewLine>[-0.5767, -0.1486,  0.0741,  …,  0.7419,  0.8961,  0.2282],</p><NewLine></blockquote><NewLine><p>Input to conv layer:</p><NewLine><blockquote><NewLine><p>-0.52449334,-0.5619572,-0.7492762,-0.3746381,-0.41210192,-0.5619572,-0.41210192,-0.03746381,0.07492762,0.0,-0.26224667,-0.59942096,-0.18731906,-0.41210192,-0.7118124 ,-0.7118124</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/Nilakshan_Kunanantha,(Nilakshan Kunananthaseelan),Nilakshan_Kunanantha,"July 28, 2020,  7:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>These should be close to each other. Just to confirm, your “img” input is after the normalize transformation mentioned here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><pre><code class=""lang-auto"">        transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                             std=[0.229, 0.224, 0.225]),<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: July 28, 2020,  5:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
77399,"Error in inference phase, after loading quantized model",2020-04-19T07:47:38.377Z,2,171,"<div class=""post"" itemprop=""articleBody""><NewLine><p>After I used <code>torch.quantization.quantize_dynamic()</code> to quantize the original model, I saved and loaded the quantized model. But, when I ran inference, it returned this error. The original model still ran inference well, I don’t know why</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""inference.py"", line 81, in &lt;module&gt;<NewLine>    output = infer(args.text, model)<NewLine>  File ""inference.py"", line 30, in infer<NewLine>    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)<NewLine>  File ""/media/tma/DATA/Khai-folder/Tacotron2-PyTorch/model/model.py"", line 542, in inference<NewLine>    encoder_outputs = self.encoder.inference(embedded_inputs)<NewLine>  File ""/media/tma/DATA/Khai-folder/Tacotron2-PyTorch/model/model.py"", line 219, in inference<NewLine>    self.lstm.flatten_parameters()<NewLine>  File ""/media/tma/DATA/miniconda3/envs/ttsv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 576, in __get<NewLine>attr__<NewLine>    type(self).__name__, name))<NewLine>AttributeError: 'LSTM' object has no attribute 'flatten_parameters'<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/khaidoan25,(Khaidoan25),khaidoan25,"April 19, 2020,  7:49am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which pytorch version are you using? I think this is a known problem and should go away with 1.5 release.<br/><NewLine>Could you wait for that or re-try with the nightly build and see if this issue goes away?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m using pytorch version 1.4.0<br/><NewLine>Hope next release will fix this issue.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/khaidoan25"">@khaidoan25</a>, I am facing the same issue with Pytorch version - 1.5.1 . Have you been able to solve it?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Actually, when you load your quantized model, you need to quantize your initial model first.</p><NewLine><pre><code class=""lang-auto"">quantized_model = torch.quantization.quantize_dynamic(<NewLine>    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8<NewLine>)         // Do s.t like this first before loading your quantized model<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/khaidoan25; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/asmita; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/khaidoan25; <NewLine> ,"REPLY_DATE 1: April 20, 2020,  4:40pm; <NewLine> REPLY_DATE 2: April 21, 2020,  8:09am; <NewLine> REPLY_DATE 3: July 15, 2020,  6:34am; <NewLine> REPLY_DATE 4: August 21, 2020, 10:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
90445,Bottleneck on data loading,2020-07-24T17:27:41.777Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have a bottleneck on the dataloading during training. I run cProfiler and these are the results:</p><NewLine><pre><code class=""lang-auto"">        1    0.012    0.012 1820.534 1820.534 models.py:15(fit)<NewLine>       56    0.001    0.000 1808.163   32.289 dataloader.py:775(__next__)<NewLine>       52    0.001    0.000 1807.264   34.755 dataloader.py:742(_get_data)<NewLine>      392    0.016    0.000 1807.263    4.610 dataloader.py:711(_try_get_data)<NewLine>      392    0.006    0.000 1807.178    4.610 queues.py:91(get)<NewLine>      392    0.002    0.000 1806.842    4.609 connection.py:253(poll)<NewLine>      392    0.002    0.000 1806.840    4.609 connection.py:413(_poll)<NewLine>      392    0.009    0.000 1806.837    4.609 connection.py:906(wait)<NewLine>      392    0.004    0.000 1806.810    4.609 selectors.py:402(select)<NewLine>      392 1806.805    4.609 1806.805    4.609 {method 'poll' of 'select.poll' objects}<NewLine>        4    0.000    0.000    6.452    1.613 dataloader.py:274(__iter__)<NewLine>        4    0.016    0.004    6.452    1.613 dataloader.py:635(__init__)<NewLine>      128    0.007    0.000    5.553    0.043 process.py:101(start)<NewLine>      128    0.001    0.000    5.531    0.043 context.py:221(_Popen)<NewLine>      128    0.003    0.000    5.530    0.043 context.py:274(_Popen)<NewLine></code></pre><NewLine><p>I am using 32 workers (I have 40 cpus available).<br/><NewLine>Do you what is causing the dataloading to be slow? Do you know what are the files queues.py and connetctions.py? Functions there seem to be taking great part of the time.</p><NewLine><p>Cheers <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>",https://discuss.pytorch.org/u/marioo,(Mario Lino),marioo,"July 24, 2020,  5:28pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have a look at <a href=""https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19"">this post</a>, which explains some potential bottlenecks and workarounds. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: August 21, 2020, 10:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
90458,Quantized model inference error related to Mish activation function,2020-07-24T20:19:21.022Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m tinkering with post-training static quantization in PyTorch by trying out different activation functions on the same model, then I try to quantize it and run inference ( I want to see what are the activations that are supported). For example, I replaced ReLU with leakyReLU on ResNet50 then applied quantization. The inference ran just fine ( it was a bit slower with a 3% accuracy drop but this does not matter as I’m only experimenting). After that, I tried the Mish activation function, the conversion was successful, however, I got the following error during inference:</p><NewLine><pre><code class=""lang-auto"">---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine> in <NewLine>      7 helper.print_size_of_model(resnet)<NewLine>      8 <NewLine>----&gt; 9 top1, top5, time_elapsed= helper.evaluate(resnet, criterion, testloader, neval_batches=num_eval_batches)<NewLine>     10 print('Evaluation accuracy on %d images, top5: %2.2f, top1: %2.2f'%(num_eval_batches * eval_batch_size, top5.avg,top1.avg))<NewLine>     11 print('time slapsed: %s' % str(datetime.timedelta(seconds=time_elapsed)))<NewLine><NewLine>d:\github\PyTorch_CIFAR10\helper.py in evaluate(model, criterion, data_loader, neval_batches, device)<NewLine>     30         for image, target in data_loader:<NewLine>     31             image.to(device)<NewLine>---&gt; 32             output = model(image)<NewLine>     33             loss = criterion(output, target)<NewLine>     34             cnt += 1<NewLine><NewLine>~\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)<NewLine>    548             result = self._slow_forward(*input, **kwargs)<NewLine>    549         else:<NewLine>--&gt; 550             result = self.forward(*input, **kwargs)<NewLine>    551         for hook in self._forward_hooks.values():<NewLine>    552             hook_result = hook(self, input, result)<NewLine><NewLine>d:\github\PyTorch_CIFAR10\cifar10_models\resnetQ.py in forward(self, x)<NewLine>    227         x = self.conv1(x)<NewLine>    228         x = self.bn1(x)<NewLine>--&gt; 229         x = self.relu(x)<NewLine>    230         x = self.maxpool(x)<NewLine>    231         x = self.layer1(x)<NewLine><NewLine>~\anaconda3\envs\PFE_env\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)<NewLine>    548             result = self._slow_forward(*input, **kwargs)<NewLine>    549         else:<NewLine>--&gt; 550             result = self.forward(*input, **kwargs)<NewLine>    551         for hook in self._forward_hooks.values():<NewLine>    552             hook_result = hook(self, input, result)<NewLine><NewLine>d:\github\PyTorch_CIFAR10\cifar10_models\resnetQ.py in forward(self, x)<NewLine>     24 <NewLine>     25     def forward(self, x):<NewLine>---&gt; 26         x = x * (torch.tanh(torch.nn.functional.softplus(x)))<NewLine>     27         return x<NewLine>     28 <NewLine><NewLine>RuntimeError: Could not run 'aten::empty.memory_format' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::empty.memory_format' is only available for these backends: [CPUTensorId, CUDATensorId, MkldnnCPUTensorId, SparseCPUTensorId, SparseCUDATensorId, BackendSelect, VariableTensorId].<NewLine></code></pre><NewLine><p>the mish layer is defined by:</p><NewLine><pre><code class=""lang-auto"">class Mish(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = x * (torch.tanh(torch.nn.functional.softplus(x)))<NewLine>        return x<NewLine></code></pre><NewLine><p>Any help in this matter would be greatly appreciated because ultimately, I want to apply quantization on YOLOv4 which relies on Mish as an activation function.</p><NewLine></div>",https://discuss.pytorch.org/u/Anouar_LAOUICHI,(Anouar LAOUICHI),Anouar_LAOUICHI,"July 24, 2020,  8:23pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Softplus currently does not have a quantized implementation.  This error also usually means that a quantized tensor is being passed to a non-quantized function.</p><NewLine><p>For a quick fix, you could add a <code>torch.quantization.DeQuantStub()</code> and <code>torch.quantization.QuantStub()</code> around the areas of the network which cannot be quantized such as Softplus.</p><NewLine><p>The longer term fix would be to add quantization support for Softplus to PyTorch.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The quick fix seems reasonable enough. Hopefully it won’t have a significant impact on inference time. I guess I’ll just have to try it out and see. Thank you for your help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Anouar_LAOUICHI; <NewLine> ,"REPLY_DATE 1: August 21, 2020, 10:57pm; <NewLine> REPLY_DATE 2: July 24, 2020, 11:35pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
67158,The packing format of quantized parameters after jitting,2020-01-20T09:47:01.813Z,1,297,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, following <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">the static quantization tutorial,</a>, I am trying to extract parameters of quantized, and jitted model. It seems after jitting, parameters are packed in a way that I don’t understand. For example, if I run the snippet below after the tutorial script, I get the output below.</p><NewLine><pre><code class=""lang-auto"">input_size = (1, 3, 224, 224)<NewLine>inp = np.random.randn(*input_size).astype(""float32"")<NewLine>trace = torch.jit.trace(per_channel_quantized_model, torch.from_numpy(inp))<NewLine>state_dict = trace.state_dict()<NewLine>for (k, v) in state_dict.items():<NewLine>    print(k, v.size())<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">features.0.0._packed_params torch.Size([128])<NewLine>features.1.conv.0.0._packed_params torch.Size([128])<NewLine>features.1.conv.1._packed_params torch.Size([128])<NewLine>features.2.conv.0.0._packed_params torch.Size([128])<NewLine>features.2.conv.1.0._packed_params torch.Size([128])<NewLine>features.2.conv.2._packed_params torch.Size([128])<NewLine>features.3.conv.0.0._packed_params torch.Size([128])<NewLine>features.3.conv.1.0._packed_params torch.Size([128])<NewLine>features.3.conv.2._packed_params torch.Size([128])<NewLine>features.4.conv.0.0._packed_params torch.Size([128])<NewLine>features.4.conv.1.0._packed_params torch.Size([128])<NewLine>features.4.conv.2._packed_params torch.Size([128])<NewLine>features.5.conv.0.0._packed_params torch.Size([128])<NewLine>features.5.conv.1.0._packed_params torch.Size([128])<NewLine>features.5.conv.2._packed_params torch.Size([128])<NewLine>features.6.conv.0.0._packed_params torch.Size([128])<NewLine>features.6.conv.1.0._packed_params torch.Size([128])<NewLine>features.6.conv.2._packed_params torch.Size([128])<NewLine>features.7.conv.0.0._packed_params torch.Size([128])<NewLine>features.7.conv.1.0._packed_params torch.Size([128])<NewLine>features.7.conv.2._packed_params torch.Size([128])<NewLine>features.8.conv.0.0._packed_params torch.Size([128])<NewLine>features.8.conv.1.0._packed_params torch.Size([128])<NewLine>features.8.conv.2._packed_params torch.Size([128])<NewLine>features.9.conv.0.0._packed_params torch.Size([128])<NewLine>features.9.conv.1.0._packed_params torch.Size([128])<NewLine>features.9.conv.2._packed_params torch.Size([128])<NewLine>features.10.conv.0.0._packed_params torch.Size([128])<NewLine>features.10.conv.1.0._packed_params torch.Size([128])<NewLine>features.10.conv.2._packed_params torch.Size([128])<NewLine>features.11.conv.0.0._packed_params torch.Size([128])<NewLine>features.11.conv.1.0._packed_params torch.Size([128])<NewLine>features.11.conv.2._packed_params torch.Size([128])<NewLine>features.12.conv.0.0._packed_params torch.Size([128])<NewLine>features.12.conv.1.0._packed_params torch.Size([128])<NewLine>features.12.conv.2._packed_params torch.Size([128])<NewLine>features.13.conv.0.0._packed_params torch.Size([128])<NewLine>features.13.conv.1.0._packed_params torch.Size([128])<NewLine>features.13.conv.2._packed_params torch.Size([128])<NewLine>features.14.conv.0.0._packed_params torch.Size([128])<NewLine>features.14.conv.1.0._packed_params torch.Size([128])<NewLine>features.14.conv.2._packed_params torch.Size([128])<NewLine>features.15.conv.0.0._packed_params torch.Size([128])<NewLine>features.15.conv.1.0._packed_params torch.Size([128])<NewLine>features.15.conv.2._packed_params torch.Size([128])<NewLine>features.16.conv.0.0._packed_params torch.Size([128])<NewLine>features.16.conv.1.0._packed_params torch.Size([128])<NewLine>features.16.conv.2._packed_params torch.Size([128])<NewLine>features.17.conv.0.0._packed_params torch.Size([128])<NewLine>features.17.conv.1.0._packed_params torch.Size([128])<NewLine>features.17.conv.2._packed_params torch.Size([128])<NewLine>features.18.0._packed_params torch.Size([128])<NewLine>quant.scale torch.Size([1])<NewLine>quant.zero_point torch.Size([1])<NewLine>classifier.1._packed_params._packed_params torch.Size([104])<NewLine></code></pre><NewLine><p>I have no idea what is going on in this format and I have many questions. But for now let me ask you these:</p><NewLine><ul><NewLine><li>Is there a documentation of the packing format?</li><NewLine><li>How can I extract the original floating point tensors along with scale and zero point? I confirmed that they are available before tracing.</li><NewLine><li>Or even better, is there a way to prevent packing?</li><NewLine><li>During tracing, where in the code base does this packing happen?</li><NewLine></ul><NewLine><p>I’m trying to translate jitted, quantized PyTorch model to TVM IR. For that I need floating point tensors with scale and zero point. That is the reason I’m asking here.</p><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a></p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"January 20, 2020,  9:53am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok torch.ops.quantized.conv2d_unpack did the job.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I met the same problem. Could you show me the detail of the “torch.ops.quantized.conv2d_unpack”? And how to deal with classifier.1._packed_params?<br/><NewLine>Thanks!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>See the implementation in TVM I added:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/apache/incubator-tvm/blob/06e9542ee0bfd014bd06a4dd4fdb3af9d2d29eb0/python/tvm/relay/frontend/qnn_torch.py#L50-L100"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/apache/incubator-tvm/blob/06e9542ee0bfd014bd06a4dd4fdb3af9d2d29eb0/python/tvm/relay/frontend/qnn_torch.py#L50-L100"" rel=""nofollow noopener"" target=""_blank"">apache/incubator-tvm/blob/06e9542ee0bfd014bd06a4dd4fdb3af9d2d29eb0/python/tvm/relay/frontend/qnn_torch.py#L50-L100</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""50"" style=""counter-reset: li-counter 49 ;""><NewLine><li>def _unpack_quant_params(param_name, packed_params, unpack_func):</li><NewLine><li>    # Torch stores quantized params in a custom packed format,</li><NewLine><li>    # need to unpack and retrieve them as numpy arrays</li><NewLine><li>    qweight, bias = unpack_func(packed_params)</li><NewLine><li>    weight_np = qweight.dequantize().numpy()</li><NewLine><li><NewLine></li><NewLine><li>    import torch</li><NewLine><li>    if qweight.qscheme() == torch.per_tensor_affine:</li><NewLine><li>        param = QNNParam(weight_np, bias, qweight.q_scale(),</li><NewLine><li>                         int(qweight.q_zero_point()), param_name)</li><NewLine><li>    else:</li><NewLine><li>        scales = qweight.q_per_channel_scales().numpy()</li><NewLine><li>        zero_points = qweight.q_per_channel_zero_points().numpy()</li><NewLine><li>        # This is an assumption posed by QNN</li><NewLine><li>        msg = ""The values of zero points should be all zero for per channel""</li><NewLine><li>        assert np.all(zero_points == 0), msg</li><NewLine><li>        param = QNNParam(weight_np, bias, scales, 0, param_name)</li><NewLine><li><NewLine></li><NewLine><li>    return param</li><NewLine><li><NewLine></li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/apache/incubator-tvm/blob/06e9542ee0bfd014bd06a4dd4fdb3af9d2d29eb0/python/tvm/relay/frontend/qnn_torch.py#L50-L100"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>From the name <code>classifier.1._packed_params</code> I guess it comes from nn.Linear. In that case, you need to use <code>torch.ops.quantized.linear_unpack</code>.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi I am working with a quantized model in C++, I wonder if I can parse the jitted model parameters like this in C++ ? I could not find any unpacking modules in  torch::jit::script::Module . I have trained and quantized my model in Python and loaded to C++. I am using version 1.6.0+</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/johnzhou1996; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/k.osama; <NewLine> ,"REPLY_DATE 1: January 20, 2020, 10:31am; <NewLine> REPLY_DATE 2: March 11, 2020,  8:32am; <NewLine> REPLY_DATE 3: March 11, 2020,  9:15am; <NewLine> REPLY_DATE 4: July 22, 2020,  7:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89086,"_combine_histograms histogram_with_output_range = torch.zeros((Nbins * downsample_rate), device=orig_hist.device) RuntimeError: Trying to create tensor with negative dimension -4398046511104: [-4398046511104]",2020-07-14T04:29:34.877Z,5,177,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone.<br/><NewLine>This is a followup question concerning  this <a href=""https://discuss.pytorch.org/t/how-can-i-incorporate-prelu-in-a-quantized-model/89080"">one</a><br/><NewLine>The issue is everything goes just fine expect at some point in time, this weird error occurs when running this specific block! :</p><NewLine><pre><code class=""lang-python"">class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine><NewLine>        self.fc = nn.Sequential(<NewLine>                                nn.Linear(channel, channel // reduction),<NewLine>                                nn.PReLU(),<NewLine>                                # nn.ReLU(),<NewLine>                                nn.Linear(channel // reduction, channel),<NewLine>                                nn.Sigmoid()<NewLine>                                )<NewLine>        self.fc1 = self.fc[0]<NewLine>        self.prelu = self.fc[1]<NewLine>        self.fc2 = self.fc[2]<NewLine>        self.sigmoid = self.fc[3]<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine><NewLine>    def forward(self, x):<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        # y = self.fc(y).view(b, c, 1, 1)<NewLine>        y = self.fc1(y)<NewLine>        y = self.prelu_q(y)<NewLine>        y = self.fc2(y)<NewLine>        y = self.sigmoid(y).view(b, c, 1, 1)<NewLine><NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine><NewLine></code></pre><NewLine><p>It runs several times fine, but at some point it fails with the following error message :</p><NewLine><pre><code class=""lang-python"">Traceback (most recent call last):<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 248, in &lt;module&gt;<NewLine>    quantize_test()<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 230, in quantize_test<NewLine>    evaluate(model, dtloader, neval_batches=num_calibration_batches)<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 145, in evaluate<NewLine>    features = model(image.unsqueeze(0))<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 473, in forward<NewLine>    x = self.layer3(x)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 387, in forward<NewLine>    out = self.se(out)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 345, in forward<NewLine>    y = self.prelu_q(y)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 221, in forward<NewLine>    inputs = self.quantized_op.add(tmax, weight_min_res).unsqueeze(0)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\quantized\modules\functional_modules.py"", line 43, in add<NewLine>    r = self.activation_post_process(r)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 833, in forward<NewLine>    self.bins)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 789, in _combine_histograms<NewLine>    histogram_with_output_range = torch.zeros((Nbins * downsample_rate), device=orig_hist.device)<NewLine>RuntimeError: Trying to create tensor with negative dimension -4398046511104: [-4398046511104]<NewLine></code></pre><NewLine><p>what am I missing here ?</p><NewLine><p>Any help is geatly appreciated</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"July 14, 2020,  1:43pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Can you check what are the inputs to the add operation at <code>""d:\codes\org\python\FV\quantized_models.py"", line 221</code><br/><NewLine>It looks like it is not handling these properly.</p><NewLine><p>If you could give us a set of inputs that reproduces this issue so that we can reproduce on our side, that would be very helpful!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>This is the latest error I get (after updating the PReLU_Quantized (link to implementation is <a href=""https://discuss.pytorch.org/t/how-can-i-incorporate-prelu-in-a-quantized-model/89080/4"">here</a> by the way):<br/><NewLine>The inputs are included in the log below (as <strong>X</strong>) and the error only happens in <strong>SEBlock</strong> module which its definition is also given below all other modules that use the <code>PReLU_Quantzied</code> module run fine except <strong>SEBlock</strong>!:</p><NewLine><pre><code class=""lang-python"">Size (MB): 89.297826<NewLine>QConfig(activation=functools.partial(&lt;class 'torch.quantization.observer.HistogramObserver'&gt;, reduce_range=True), weight=functools.partial(&lt;class 'torch.quantization.observer.PerChannelMinMaxObserver'&gt;, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))<NewLine>Post Training Quantization Prepare: Inserting Observers<NewLine><NewLine> Inverted Residual Block:After observer insertion<NewLine><NewLine> Conv2d(<NewLine>  3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>  (activation_post_process): HistogramObserver()<NewLine>)<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5691, -0.7516, -0.7360, -0.6458]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 3.6605e-01,  3.3855e+00, -5.0032e-19, -9.0280e-19]])<NewLine>Traceback (most recent call last):<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 266, in &lt;module&gt;<NewLine>    quantize_test()<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 248, in quantize_test<NewLine>    evaluate(model, dtloader, neval_batches=num_calibration_batches)<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 152, in evaluate<NewLine>    features = model(image.unsqueeze(0))<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 576, in forward<NewLine>    x = self.layer1(x)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 489, in forward<NewLine>    out = self.se(out)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 447, in forward<NewLine>    y = self.prelu_q(y)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 322, in forward<NewLine>    inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\quantized\modules\functional_modules.py"", line 43, in add<NewLine>    r = self.activation_post_process(r)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 833, in forward<NewLine>    self.bins)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 789, in _combine_histograms<NewLine>    histogram_with_output_range = torch.zeros((Nbins * downsample_rate), device=orig_hist.device)<NewLine>RuntimeError: Trying to create tensor with negative dimension -4398046511104: [-4398046511104]<NewLine></code></pre><NewLine><p>and this is how the SE block looks like :</p><NewLine><pre><code class=""lang-python"">class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine><NewLine>        self.fc = nn.Sequential(<NewLine>                                nn.Linear(channel, channel // reduction),<NewLine>                                nn.PReLU(),<NewLine>                                # nn.ReLU(),<NewLine>                                nn.Linear(channel // reduction, channel),<NewLine>                                nn.Sigmoid()<NewLine>                                )<NewLine>        self.fc1 = self.fc[0]<NewLine>        self.prelu = self.fc[1]<NewLine>        self.fc2 = self.fc[2]<NewLine>        self.sigmoid = self.fc[3]<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine><NewLine>    def forward(self, x):<NewLine>        print(f'&lt;inside se forward:&gt;')<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        # y = self.fc(y).view(b, c, 1, 1)<NewLine>        y = self.fc1(y)<NewLine>        print(f'X: {y}')<NewLine>        y = self.prelu_q(y)<NewLine>        y = self.fc2(y)<NewLine>        y = self.sigmoid(y).view(b, c, 1, 1)<NewLine>        print('--------------------------')<NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine></code></pre><NewLine><p>amd  this is the output when I use PReLU instead of PReLU_Quantized in the SE block only (all other instance of PReLU is replaced with PReLU_Quantized in other modulels of ResNet) :</p><NewLine><details><NewLine><summary><NewLine>Summary</summary><NewLine><pre><code class=""lang-python"">Size (MB): 89.29209<NewLine>QConfig(activation=functools.partial(&lt;class 'torch.quantization.observer.HistogramObserver'&gt;, reduce_range=True), weight=functools.partial(&lt;class 'torch.quantization.observer.PerChannelMinMaxObserver'&gt;, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))<NewLine>Post Training Quantization Prepare: Inserting Observers<NewLine><NewLine> Inverted Residual Block:After observer insertion<NewLine><NewLine> Conv2d(<NewLine>  3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>  (activation_post_process): HistogramObserver()<NewLine>)<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5691, -0.7516, -0.7360, -0.6458]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 3.6605e-01,  3.3855e+00, -5.0032e-19, -9.0280e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0513, -0.0656, -0.4529,  0.0653, -0.4762, -0.6304, -1.5043, -0.9484]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8730,  1.6650, -0.5135, -0.6811, -0.0392, -0.4689, -0.1496,  0.0717]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8759, -0.8886, -1.3295, -0.5375,  0.7598, -0.8526, -1.9066,  0.0985,<NewLine>         -0.1461, -0.5857,  0.1513, -0.3050,  0.1955, -0.8470,  0.4528,  0.9689]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6184e+00, -2.2714e-18,  2.8052e+00,  1.0378e+01,  4.6361e-05,<NewLine>          1.0644e+01,  1.4302e-02,  2.6143e-02,  2.4926e-05,  6.2237e+00,<NewLine>          8.8411e-05,  6.4360e+00,  3.3530e+00,  3.9302e-05,  8.1652e+00,<NewLine>          8.7950e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1687e+00,  3.1469e+00, -1.1788e+01,  4.9410e-02,  1.7272e+00,<NewLine>         -3.0913e+00,  1.1572e+00, -6.7104e+00,  1.1371e+01,  4.8926e+00,<NewLine>         -1.3102e+00, -4.9774e+00, -4.1444e+00, -6.3367e-01, -1.5672e+00,<NewLine>          4.2629e+00,  3.2491e+00, -4.6632e+00,  5.9241e-01, -2.4883e+00,<NewLine>          5.2599e+00, -7.1710e+00,  4.7197e+00,  7.2724e+00, -2.3363e+00,<NewLine>         -2.2564e+00,  5.4431e+00, -2.2832e-12,  1.9732e+00,  1.1682e+00,<NewLine>          6.1555e+00,  6.3574e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2785e-01,  1.1057e+00,  3.1581e-07,  9.7595e-01,  9.7386e-03,<NewLine>          8.4260e-07,  2.4243e-01,  2.1749e+00,  4.5704e-01,  2.9307e+00,<NewLine>          3.2384e+00,  2.6099e+00,  1.7640e-01,  4.3206e-04,  9.9380e-18,<NewLine>          1.3450e-11,  1.5721e-09,  2.7632e-07,  3.6721e-04,  2.1237e-07,<NewLine>          1.8839e-10,  1.8423e-02,  1.8514e-13,  4.3584e+00,  1.0972e-01,<NewLine>          7.5909e-03,  4.3828e-02,  2.9285e-02,  8.3840e-07, -2.6420e-19,<NewLine>          3.6933e-01,  1.0561e+00]])<NewLine>--------------------------<NewLine>0-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5517, -0.8007, -0.7286, -0.6478]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 5.0945e-01,  3.2514e+00, -5.2950e-19, -9.1256e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0556, -0.1015, -0.4792,  0.0956, -0.4782, -0.6346, -1.4946, -0.9745]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8254,  1.6459, -0.4613, -0.6462, -0.0376, -0.4217, -0.0865,  0.0773]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8807, -0.8899, -1.3275, -0.5305,  0.7527, -0.8557, -1.9068,  0.1042,<NewLine>         -0.1444, -0.5798,  0.1493, -0.3055,  0.1952, -0.8383,  0.4532,  0.9664]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6193e+00, -2.2732e-18,  2.8069e+00,  1.0384e+01,  4.6389e-05,<NewLine>          1.0650e+01,  1.4310e-02,  2.6159e-02,  2.4941e-05,  6.2275e+00,<NewLine>          8.8464e-05,  6.4398e+00,  3.3551e+00,  3.9326e-05,  8.1701e+00,<NewLine>          8.8003e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1444e+00,  3.1584e+00, -1.1794e+01,  4.9510e-02,  1.7366e+00,<NewLine>         -3.0976e+00,  1.1594e+00, -6.7127e+00,  1.1380e+01,  4.9035e+00,<NewLine>         -1.3231e+00, -4.9740e+00, -4.1439e+00, -6.3774e-01, -1.5777e+00,<NewLine>          4.2655e+00,  3.2341e+00, -4.6753e+00,  6.1677e-01, -2.4898e+00,<NewLine>          5.2556e+00, -7.1508e+00,  4.7271e+00,  7.2643e+00, -2.3301e+00,<NewLine>         -2.2546e+00,  5.4412e+00, -2.2872e-12,  1.9668e+00,  1.1764e+00,<NewLine>          6.1590e+00,  6.3575e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2778e-01,  1.1051e+00,  3.1564e-07,  9.7544e-01,  9.7335e-03,<NewLine>          8.4216e-07,  2.4230e-01,  2.1737e+00,  4.5681e-01,  2.9292e+00,<NewLine>          3.2367e+00,  2.6086e+00,  1.7631e-01,  4.3183e-04,  9.9393e-18,<NewLine>          1.3443e-11,  1.5713e-09,  2.7617e-07,  3.6702e-04,  2.1226e-07,<NewLine>          1.8829e-10,  1.8414e-02,  1.8504e-13,  4.3561e+00,  1.0967e-01,<NewLine>          7.5869e-03,  4.3805e-02,  2.9270e-02,  8.3797e-07, -2.6259e-19,<NewLine>          3.6914e-01,  1.0555e+00]])<NewLine>--------------------------<NewLine>1-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.6008, -0.7627, -0.7418, -0.6562]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.6180e-01,  3.2969e+00, -5.1091e-19, -8.5673e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0860, -0.0888, -0.4410,  0.0515, -0.4853, -0.6203, -1.4854, -0.9521]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8713,  1.6702, -0.5249, -0.6848, -0.0393, -0.4817, -0.1603,  0.0686]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8888, -0.8991, -1.3308, -0.5351,  0.7626, -0.8547, -1.9075,  0.1075,<NewLine>         -0.1457, -0.5770,  0.1518, -0.3068,  0.2023, -0.8418,  0.4610,  0.9654]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6180e+00, -2.2720e-18,  2.8046e+00,  1.0376e+01,  4.6351e-05,<NewLine>          1.0642e+01,  1.4299e-02,  2.6138e-02,  2.4921e-05,  6.2225e+00,<NewLine>          8.8393e-05,  6.4347e+00,  3.3524e+00,  3.9294e-05,  8.1636e+00,<NewLine>          8.7932e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1925e+00,  3.1589e+00, -1.1792e+01,  4.9472e-02,  1.7246e+00,<NewLine>         -3.0884e+00,  1.1586e+00, -6.7112e+00,  1.1375e+01,  4.8954e+00,<NewLine>         -1.3047e+00, -4.9715e+00, -4.1392e+00, -6.4653e-01, -1.5772e+00,<NewLine>          4.2795e+00,  3.2537e+00, -4.6607e+00,  5.9939e-01, -2.4853e+00,<NewLine>          5.2615e+00, -7.1921e+00,  4.7311e+00,  7.2626e+00, -2.3221e+00,<NewLine>         -2.2574e+00,  5.4390e+00, -2.2799e-12,  1.9636e+00,  1.1820e+00,<NewLine>          6.1593e+00,  6.3554e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2775e-01,  1.1048e+00,  3.1557e-07,  9.7521e-01,  9.7312e-03,<NewLine>          8.4196e-07,  2.4225e-01,  2.1732e+00,  4.5670e-01,  2.9285e+00,<NewLine>          3.2360e+00,  2.6079e+00,  1.7627e-01,  4.3173e-04,  9.9377e-18,<NewLine>          1.3440e-11,  1.5710e-09,  2.7611e-07,  3.6693e-04,  2.1221e-07,<NewLine>          1.8825e-10,  1.8409e-02,  1.8500e-13,  4.3551e+00,  1.0964e-01,<NewLine>          7.5851e-03,  4.3795e-02,  2.9263e-02,  8.3777e-07, -2.6075e-19,<NewLine>          3.6905e-01,  1.0553e+00]])<NewLine>--------------------------<NewLine>2-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5790, -0.8100, -0.7292, -0.6440]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 5.0116e-01,  3.2659e+00, -5.2126e-19, -8.5920e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0427, -0.0929, -0.4953,  0.0674, -0.4784, -0.6115, -1.4972, -0.9645]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8374,  1.6551, -0.4788, -0.6555, -0.0380, -0.4393, -0.1045,  0.0742]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8727, -0.8932, -1.3280, -0.5371,  0.7591, -0.8533, -1.8998,  0.1003,<NewLine>         -0.1452, -0.5813,  0.1475, -0.3055,  0.2016, -0.8411,  0.4535,  0.9559]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6189e+00, -2.2717e-18,  2.8060e+00,  1.0381e+01,  4.6375e-05,<NewLine>          1.0647e+01,  1.4306e-02,  2.6151e-02,  2.4933e-05,  6.2256e+00,<NewLine>          8.8438e-05,  6.4379e+00,  3.3541e+00,  3.9314e-05,  8.1676e+00,<NewLine>          8.7976e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1427e+00,  3.1480e+00, -1.1763e+01,  4.9449e-02,  1.7342e+00,<NewLine>         -3.0890e+00,  1.1581e+00, -6.7127e+00,  1.1348e+01,  4.8951e+00,<NewLine>         -1.3154e+00, -4.9691e+00, -4.1414e+00, -6.4151e-01, -1.5783e+00,<NewLine>          4.2688e+00,  3.2439e+00, -4.6649e+00,  6.0231e-01, -2.4855e+00,<NewLine>          5.2647e+00, -7.1494e+00,  4.7290e+00,  7.2520e+00, -2.3288e+00,<NewLine>         -2.2466e+00,  5.4410e+00, -2.2847e-12,  1.9777e+00,  1.1817e+00,<NewLine>          6.1588e+00,  6.3552e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2778e-01,  1.1050e+00,  3.1563e-07,  9.7541e-01,  9.7331e-03,<NewLine>          8.4213e-07,  2.4229e-01,  2.1736e+00,  4.5679e-01,  2.9291e+00,<NewLine>          3.2366e+00,  2.6084e+00,  1.7631e-01,  4.3181e-04,  9.9368e-18,<NewLine>          1.3443e-11,  1.5713e-09,  2.7616e-07,  3.6700e-04,  2.1225e-07,<NewLine>          1.8828e-10,  1.8413e-02,  1.8503e-13,  4.3559e+00,  1.0966e-01,<NewLine>          7.5866e-03,  4.3804e-02,  2.9269e-02,  8.3793e-07, -2.6231e-19,<NewLine>          3.6912e-01,  1.0555e+00]])<NewLine>--------------------------<NewLine>3-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.6226, -0.7605, -0.6854, -0.5836]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 2.6039e-01,  3.4835e+00, -4.8167e-19, -8.5980e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0699, -0.0526, -0.4319, -0.0069, -0.4890, -0.6087, -1.4835, -0.9184]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8828,  1.6724, -0.5539, -0.7054, -0.0402, -0.5061, -0.2002,  0.0661]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8790, -0.8969, -1.3365, -0.5384,  0.7664, -0.8571, -1.9043,  0.1059,<NewLine>         -0.1459, -0.5847,  0.1542, -0.3094,  0.2076, -0.8439,  0.4567,  0.9642]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6174e+00, -2.2710e-18,  2.8035e+00,  1.0371e+01,  4.6333e-05,<NewLine>          1.0638e+01,  1.4293e-02,  2.6128e-02,  2.4911e-05,  6.2200e+00,<NewLine>          8.8358e-05,  6.4321e+00,  3.3510e+00,  3.9279e-05,  8.1603e+00,<NewLine>          8.7897e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1583e+00,  3.1523e+00, -1.1765e+01,  4.9511e-02,  1.7292e+00,<NewLine>         -3.0851e+00,  1.1595e+00, -6.7154e+00,  1.1350e+01,  4.9005e+00,<NewLine>         -1.3040e+00, -4.9675e+00, -4.1433e+00, -6.3643e-01, -1.5745e+00,<NewLine>          4.2669e+00,  3.2492e+00, -4.6569e+00,  6.0002e-01, -2.4789e+00,<NewLine>          5.2519e+00, -7.1619e+00,  4.7275e+00,  7.2465e+00, -2.3229e+00,<NewLine>         -2.2525e+00,  5.4448e+00, -2.2806e-12,  1.9732e+00,  1.1739e+00,<NewLine>          6.1550e+00,  6.3576e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2778e-01,  1.1050e+00,  3.1563e-07,  9.7540e-01,  9.7331e-03,<NewLine>          8.4212e-07,  2.4229e-01,  2.1736e+00,  4.5679e-01,  2.9291e+00,<NewLine>          3.2366e+00,  2.6084e+00,  1.7630e-01,  4.3181e-04,  9.9369e-18,<NewLine>          1.3443e-11,  1.5712e-09,  2.7616e-07,  3.6700e-04,  2.1225e-07,<NewLine>          1.8828e-10,  1.8413e-02,  1.8503e-13,  4.3559e+00,  1.0966e-01,<NewLine>          7.5866e-03,  4.3803e-02,  2.9269e-02,  8.3793e-07, -2.6177e-19,<NewLine>          3.6912e-01,  1.0555e+00]])<NewLine>--------------------------<NewLine>4-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5559, -0.7016, -0.7545, -0.6793]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.6992e-01,  3.2951e+00, -5.1868e-19, -8.9299e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0106, -0.0831, -0.5151,  0.0650, -0.4869, -0.6094, -1.5116, -0.9355]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8588,  1.6723, -0.4774, -0.6520, -0.0379, -0.4428, -0.0917,  0.0721]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8655, -0.8893, -1.3313, -0.5367,  0.7590, -0.8533, -1.9023,  0.1008,<NewLine>         -0.1428, -0.5834,  0.1448, -0.3016,  0.2040, -0.8361,  0.4534,  0.9494]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6194e+00, -2.2728e-18,  2.8070e+00,  1.0384e+01,  4.6391e-05,<NewLine>          1.0651e+01,  1.4311e-02,  2.6160e-02,  2.4942e-05,  6.2277e+00,<NewLine>          8.8468e-05,  6.4401e+00,  3.3552e+00,  3.9328e-05,  8.1704e+00,<NewLine>          8.8006e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1170e+00,  3.1500e+00, -1.1769e+01,  4.9446e-02,  1.7362e+00,<NewLine>         -3.0951e+00,  1.1581e+00, -6.7183e+00,  1.1354e+01,  4.8964e+00,<NewLine>         -1.3110e+00, -4.9689e+00, -4.1461e+00, -6.4890e-01, -1.5875e+00,<NewLine>          4.2782e+00,  3.2361e+00, -4.6685e+00,  6.0150e-01, -2.4799e+00,<NewLine>          5.2726e+00, -7.1287e+00,  4.7384e+00,  7.2532e+00, -2.3235e+00,<NewLine>         -2.2367e+00,  5.4463e+00, -2.2915e-12,  1.9780e+00,  1.1893e+00,<NewLine>          6.1668e+00,  6.3629e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2774e-01,  1.1047e+00,  3.1554e-07,  9.7513e-01,  9.7304e-03,<NewLine>          8.4189e-07,  2.4223e-01,  2.1730e+00,  4.5666e-01,  2.9283e+00,<NewLine>          3.2357e+00,  2.6077e+00,  1.7626e-01,  4.3169e-04,  9.9353e-18,<NewLine>          1.3439e-11,  1.5708e-09,  2.7609e-07,  3.6690e-04,  2.1219e-07,<NewLine>          1.8823e-10,  1.8408e-02,  1.8498e-13,  4.3547e+00,  1.0963e-01,<NewLine>          7.5845e-03,  4.3792e-02,  2.9261e-02,  8.3770e-07, -2.6081e-19,<NewLine>          3.6902e-01,  1.0552e+00]])<NewLine>--------------------------<NewLine>5-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5922, -0.7833, -0.8099, -0.7581]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 6.0425e-01,  3.1537e+00, -5.2917e-19, -8.2412e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0295, -0.1079, -0.5239,  0.1099, -0.4906, -0.6187, -1.5178, -0.9515]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.9047,  1.7059, -0.4654, -0.6338, -0.0371, -0.4419, -0.0531,  0.0689]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8792, -0.8972, -1.3274, -0.5352,  0.7649, -0.8542, -1.9078,  0.1055,<NewLine>         -0.1455, -0.5737,  0.1437, -0.3026,  0.2050, -0.8408,  0.4609,  0.9527]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6192e+00, -2.2734e-18,  2.8065e+00,  1.0383e+01,  4.6383e-05,<NewLine>          1.0649e+01,  1.4309e-02,  2.6156e-02,  2.4938e-05,  6.2268e+00,<NewLine>          8.8454e-05,  6.4391e+00,  3.3547e+00,  3.9321e-05,  8.1692e+00,<NewLine>          8.7993e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1462e+00,  3.1594e+00, -1.1777e+01,  4.9445e-02,  1.7250e+00,<NewLine>         -3.0903e+00,  1.1580e+00, -6.6971e+00,  1.1362e+01,  4.8978e+00,<NewLine>         -1.3202e+00, -4.9701e+00, -4.1377e+00, -6.3982e-01, -1.5717e+00,<NewLine>          4.2688e+00,  3.2314e+00, -4.6666e+00,  6.1283e-01, -2.4762e+00,<NewLine>          5.2739e+00, -7.1517e+00,  4.7211e+00,  7.2673e+00, -2.3338e+00,<NewLine>         -2.2474e+00,  5.4291e+00, -2.2837e-12,  1.9676e+00,  1.1787e+00,<NewLine>          6.1559e+00,  6.3495e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2772e-01,  1.1046e+00,  3.1549e-07,  9.7498e-01,  9.7289e-03,<NewLine>          8.4176e-07,  2.4219e-01,  2.1727e+00,  4.5659e-01,  2.9278e+00,<NewLine>          3.2352e+00,  2.6073e+00,  1.7623e-01,  4.3163e-04,  9.9370e-18,<NewLine>          1.3437e-11,  1.5706e-09,  2.7604e-07,  3.6684e-04,  2.1216e-07,<NewLine>          1.8820e-10,  1.8405e-02,  1.8495e-13,  4.3541e+00,  1.0962e-01,<NewLine>          7.5833e-03,  4.3785e-02,  2.9256e-02,  8.3757e-07, -2.6052e-19,<NewLine>          3.6896e-01,  1.0550e+00]])<NewLine>--------------------------<NewLine>6-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5156, -0.5839, -0.7718, -0.6881]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 6.3789e-01,  3.1470e+00, -5.4607e-19, -8.8140e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0068, -0.1239, -0.5419,  0.1311, -0.4739, -0.6220, -1.5159, -1.0039]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.7764,  1.6289, -0.3860, -0.5940, -0.0352, -0.3554,  0.0103,  0.0848]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8759, -0.8883, -1.3219, -0.5339,  0.7527, -0.8555, -1.9051,  0.0963,<NewLine>         -0.1418, -0.5765,  0.1501, -0.2970,  0.1911, -0.8370,  0.4527,  0.9548]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6203e+00, -2.2739e-18,  2.8086e+00,  1.0390e+01,  4.6417e-05,<NewLine>          1.0657e+01,  1.4319e-02,  2.6175e-02,  2.4956e-05,  6.2312e+00,<NewLine>          8.8518e-05,  6.4437e+00,  3.3571e+00,  3.9350e-05,  8.1750e+00,<NewLine>          8.8056e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1541e+00,  3.1531e+00, -1.1772e+01,  4.9404e-02,  1.7326e+00,<NewLine>         -3.0931e+00,  1.1571e+00, -6.6943e+00,  1.1357e+01,  4.8937e+00,<NewLine>         -1.3274e+00, -4.9758e+00, -4.1305e+00, -6.4647e-01, -1.5764e+00,<NewLine>          4.2726e+00,  3.2396e+00, -4.6719e+00,  6.0704e-01, -2.4865e+00,<NewLine>          5.2721e+00, -7.1595e+00,  4.7218e+00,  7.2695e+00, -2.3445e+00,<NewLine>         -2.2482e+00,  5.4221e+00, -2.2827e-12,  1.9751e+00,  1.1886e+00,<NewLine>          6.1566e+00,  6.3400e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2782e-01,  1.1054e+00,  3.1574e-07,  9.7576e-01,  9.7366e-03,<NewLine>          8.4243e-07,  2.4238e-01,  2.1744e+00,  4.5695e-01,  2.9301e+00,<NewLine>          3.2378e+00,  2.6094e+00,  1.7637e-01,  4.3197e-04,  9.9450e-18,<NewLine>          1.3448e-11,  1.5718e-09,  2.7626e-07,  3.6714e-04,  2.1232e-07,<NewLine>          1.8835e-10,  1.8419e-02,  1.8510e-13,  4.3575e+00,  1.0970e-01,<NewLine>          7.5893e-03,  4.3819e-02,  2.9279e-02,  8.3823e-07, -2.6201e-19,<NewLine>          3.6925e-01,  1.0558e+00]])<NewLine>--------------------------<NewLine>7-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5567, -0.7524, -0.7620, -0.6805]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 5.3279e-01,  3.2445e+00, -5.2411e-19, -8.5973e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0248, -0.1011, -0.5172,  0.0823, -0.4737, -0.6192, -1.4961, -0.9762]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8410,  1.6705, -0.4254, -0.6104, -0.0360, -0.4001, -0.0183,  0.0759]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8740, -0.8943, -1.3243, -0.5337,  0.7550, -0.8610, -1.9063,  0.1108,<NewLine>         -0.1408, -0.5770,  0.1506, -0.3089,  0.1984, -0.8347,  0.4544,  0.9591]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6191e+00, -2.2732e-18,  2.8065e+00,  1.0383e+01,  4.6383e-05,<NewLine>          1.0649e+01,  1.4309e-02,  2.6156e-02,  2.4938e-05,  6.2267e+00,<NewLine>          8.8453e-05,  6.4390e+00,  3.3546e+00,  3.9321e-05,  8.1691e+00,<NewLine>          8.7992e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1553e+00,  3.1582e+00, -1.1776e+01,  4.9516e-02,  1.7335e+00,<NewLine>         -3.0909e+00,  1.1595e+00, -6.7080e+00,  1.1362e+01,  4.9002e+00,<NewLine>         -1.3237e+00, -4.9679e+00, -4.1376e+00, -6.4026e-01, -1.5758e+00,<NewLine>          4.2652e+00,  3.2360e+00, -4.6691e+00,  6.1957e-01, -2.4899e+00,<NewLine>          5.2536e+00, -7.1605e+00,  4.7257e+00,  7.2488e+00, -2.3271e+00,<NewLine>         -2.2548e+00,  5.4335e+00, -2.2811e-12,  1.9611e+00,  1.1809e+00,<NewLine>          6.1551e+00,  6.3494e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2783e-01,  1.1055e+00,  3.1575e-07,  9.7577e-01,  9.7367e-03,<NewLine>          8.4244e-07,  2.4238e-01,  2.1744e+00,  4.5696e-01,  2.9302e+00,<NewLine>          3.2378e+00,  2.6094e+00,  1.7637e-01,  4.3197e-04,  9.9456e-18,<NewLine>          1.3448e-11,  1.5718e-09,  2.7626e-07,  3.6714e-04,  2.1233e-07,<NewLine>          1.8835e-10,  1.8420e-02,  1.8510e-13,  4.3576e+00,  1.0970e-01,<NewLine>          7.5894e-03,  4.3820e-02,  2.9280e-02,  8.3824e-07, -2.6233e-19,<NewLine>          3.6926e-01,  1.0559e+00]])<NewLine>--------------------------<NewLine>8-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.6060, -0.9100, -0.7711, -0.7195]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 5.6481e-01,  3.2033e+00, -5.2471e-19, -8.0308e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0948, -0.1106, -0.4654,  0.0768, -0.5028, -0.6202, -1.4778, -0.9581]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.9064,  1.6963, -0.5052, -0.6644, -0.0385, -0.4721, -0.1160,  0.0672]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8868, -0.8981, -1.3322, -0.5298,  0.7566, -0.8556, -1.9039,  0.1134,<NewLine>         -0.1447, -0.5744,  0.1480, -0.3113,  0.2017, -0.8359,  0.4564,  0.9658]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6177e+00, -2.2718e-18,  2.8040e+00,  1.0373e+01,  4.6342e-05,<NewLine>          1.0640e+01,  1.4296e-02,  2.6133e-02,  2.4916e-05,  6.2212e+00,<NewLine>          8.8375e-05,  6.4333e+00,  3.3517e+00,  3.9286e-05,  8.1619e+00,<NewLine>          8.7914e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1471e+00,  3.1531e+00, -1.1779e+01,  4.9447e-02,  1.7370e+00,<NewLine>         -3.0912e+00,  1.1580e+00, -6.7101e+00,  1.1363e+01,  4.9010e+00,<NewLine>         -1.3083e+00, -4.9699e+00, -4.1370e+00, -6.3986e-01, -1.5794e+00,<NewLine>          4.2680e+00,  3.2415e+00, -4.6646e+00,  6.0562e-01, -2.4862e+00,<NewLine>          5.2591e+00, -7.1519e+00,  4.7275e+00,  7.2529e+00, -2.3203e+00,<NewLine>         -2.2537e+00,  5.4380e+00, -2.2843e-12,  1.9685e+00,  1.1793e+00,<NewLine>          6.1543e+00,  6.3497e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2776e-01,  1.1049e+00,  3.1558e-07,  9.7525e-01,  9.7316e-03,<NewLine>          8.4199e-07,  2.4226e-01,  2.1733e+00,  4.5672e-01,  2.9286e+00,<NewLine>          3.2361e+00,  2.6080e+00,  1.7628e-01,  4.3175e-04,  9.9388e-18,<NewLine>          1.3441e-11,  1.5710e-09,  2.7612e-07,  3.6695e-04,  2.1221e-07,<NewLine>          1.8825e-10,  1.8410e-02,  1.8500e-13,  4.3553e+00,  1.0965e-01,<NewLine>          7.5854e-03,  4.3797e-02,  2.9264e-02,  8.3780e-07, -2.6109e-19,<NewLine>          3.6906e-01,  1.0553e+00]])<NewLine>--------------------------<NewLine>9-feature dims: torch.Size([1, 512])<NewLine>Post Training Quantization: Calibration done<NewLine>C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py:845: UserWarning: must run observer before calling calculate_qparams.<NewLine>     Returning default scale and zero point<NewLine>  Returning default scale and zero point ""<NewLine>Post Training Quantization: Convert done<NewLine><NewLine> Inverted Residual Block: After fusion and quantization, note fused modules:<NewLine><NewLine> QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011990774422883987, zero_point=80)<NewLine>Size of model after quantization<NewLine>Size (MB): 24.397458<NewLine></code></pre><NewLine></details><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which version of PyTorch are you currently using? We recently fixed a bug in the histogram observer that should be available in 1.6. You can also use nightlies to see if it fixes your issue.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m using 1.5.0!<br/><NewLine>Ok, I’ll give that a try and report back</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Updated to the latest nighly(<code>1.7.0.dev20200714+cpu</code> and <code>torchvision-0.8.0.dev20200714+cpu</code>) just now , it got a bit further, but ultimately crashed with the same error :</p><NewLine><pre><code class=""lang-python"">Size (MB): 89.322487<NewLine>QConfig(activation=functools.partial(&lt;class 'torch.quantization.observer.HistogramObserver'&gt;, reduce_range=True), weight=functools.partial(&lt;class 'torch.quantization.observer.PerChannelMinMaxObserver'&gt;, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))<NewLine>Post Training Quantization Prepare: Inserting Observers<NewLine><NewLine> Inverted Residual Block:After observer insertion<NewLine><NewLine> Conv2d(<NewLine>  3, 64, kernel_size=(3, 3), stride=(1, 1)<NewLine>  (activation_post_process): HistogramObserver()<NewLine>)<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5691, -0.7516, -0.7360, -0.6458]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 3.6604e-01,  3.3855e+00, -5.0032e-19, -9.0280e-19]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.0513, -0.0656, -0.4529,  0.0653, -0.4762, -0.6304, -1.5043, -0.9484]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 4.8730,  1.6650, -0.5135, -0.6811, -0.0392, -0.4689, -0.1496,  0.0717]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.8759, -0.8886, -1.3295, -0.5375,  0.7598, -0.8526, -1.9066,  0.0985,<NewLine>         -0.1461, -0.5857,  0.1513, -0.3050,  0.1955, -0.8470,  0.4528,  0.9689]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.6184e+00, -2.2714e-18,  2.8052e+00,  1.0378e+01,  4.6361e-05,<NewLine>          1.0644e+01,  1.4302e-02,  2.6143e-02,  2.4926e-05,  6.2237e+00,<NewLine>          8.8411e-05,  6.4360e+00,  3.3530e+00,  3.9302e-05,  8.1652e+00,<NewLine>          8.7950e-07]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 9.1687e+00,  3.1469e+00, -1.1788e+01,  4.9410e-02,  1.7272e+00,<NewLine>         -3.0913e+00,  1.1572e+00, -6.7104e+00,  1.1371e+01,  4.8926e+00,<NewLine>         -1.3102e+00, -4.9773e+00, -4.1444e+00, -6.3367e-01, -1.5672e+00,<NewLine>          4.2629e+00,  3.2491e+00, -4.6632e+00,  5.9241e-01, -2.4883e+00,<NewLine>          5.2599e+00, -7.1710e+00,  4.7197e+00,  7.2724e+00, -2.3363e+00,<NewLine>         -2.2564e+00,  5.4431e+00, -2.2832e-12,  1.9732e+00,  1.1682e+00,<NewLine>          6.1555e+00,  6.3574e+00]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 1.2785e-01,  1.1057e+00,  3.1581e-07,  9.7595e-01,  9.7386e-03,<NewLine>          8.4260e-07,  2.4243e-01,  2.1749e+00,  4.5704e-01,  2.9307e+00,<NewLine>          3.2384e+00,  2.6099e+00,  1.7640e-01,  4.3206e-04,  9.9380e-18,<NewLine>          1.3450e-11,  1.5721e-09,  2.7632e-07,  3.6721e-04,  2.1237e-07,<NewLine>          1.8839e-10,  1.8423e-02,  1.8514e-13,  4.3584e+00,  1.0972e-01,<NewLine>          7.5909e-03,  4.3828e-02,  2.9285e-02,  8.3840e-07, -2.6420e-19,<NewLine>          3.6933e-01,  1.0561e+00]])<NewLine>--------------------------<NewLine>0-feature dims: torch.Size([1, 512])<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[-1.5517, -0.8007, -0.7286, -0.6478]])<NewLine>--------------------------<NewLine>&lt;inside se forward:&gt;<NewLine>X: tensor([[ 5.0945e-01,  3.2514e+00, -5.2950e-19, -9.1256e-19]])<NewLine>Traceback (most recent call last):<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 266, in &lt;module&gt;<NewLine>    quantize_test()<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 248, in quantize_test<NewLine>    evaluate(model, dtloader, neval_batches=num_calibration_batches)<NewLine>  File ""d:\Codes\org\python\Quantization\quantizer.py"", line 152, in evaluate<NewLine>    features = model(image.unsqueeze(0))<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 576, in forward<NewLine>    x = self.layer1(x)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\container.py"", line 117, in forward<NewLine>    input = module(input)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 489, in forward<NewLine>    out = self.se(out)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 447, in forward<NewLine>    y = self.prelu_q(y)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""d:\codes\org\python\FV\quantized_models.py"", line 322, in forward<NewLine>    inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\quantized\modules\functional_modules.py"", line 46, in add<NewLine>    r = self.activation_post_process(r)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\nn\modules\module.py"", line 726, in _call_impl<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 862, in forward<NewLine>    self.bins)<NewLine>  File ""C:\Users\User\Anaconda3\Lib\site-packages\torch\quantization\observer.py"", line 813, in _combine_histograms<NewLine>    histogram_with_output_range = torch.zeros((Nbins * downsample_rate), device=orig_hist.device)<NewLine>RuntimeError: Trying to create tensor with negative dimension -4398046511104: [-4398046511104]<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> Any ideas whats the problem here?<br/><NewLine>its greatly appreciated</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>The initial error was due to the histogram observer getting a tensor with same values or all zero values. But since that was fixed I am not quite sure of the cause of this error.<br/><NewLine>Could you provide a small repro for us to take a look? Along with the input tensor data for which this error shows up. Thanks!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>seems updating to 1.7 solved this issue! Hoever, the Unimplemented type and native bn related issues are still present.<br/><NewLine>I created a minimal self contained example with Resnet18 and a simple 2 layered Network from quantizing the model to testing it using fake data.<br/><NewLine>By setting the  two variables at the top (use_relu, disable_single_bn) you can see different behaviors(most of the code is biolerplates and resnet18 definitions)<br/><NewLine>you are free to test this both with the <code>ResNet18</code> or the <code>SimpleNetwork</code>:</p><NewLine><pre><code class=""lang-python"">import os<NewLine>from os.path import abspath, dirname, join<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.utils.data import DataLoader<NewLine>from torchvision.datasets import FakeData<NewLine>import torchvision.transforms as transforms<NewLine>from torch.quantization import fuse_modules<NewLine>use_relu = False<NewLine>disable_single_bns = False<NewLine><NewLine>class PReLU_Quantized(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine>        self.quantized_op = nn.quantized.FloatFunctional()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        # inputs = max(0, inputs) + alpha * min(0, inputs) <NewLine>        # this is how we do it <NewLine>        # pos = torch.relu(inputs)<NewLine>        # neg = -alpha * torch.relu(-inputs)<NewLine>        # res3 = pos + neg<NewLine>        self.weight = self.quant(self.weight)<NewLine>        weight_min_res = self.quantized_op.mul(-self.weight, torch.relu(-inputs))<NewLine>        inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>        inputs = self.dequant(inputs)<NewLine>        self.weight = self.dequant(self.weight)<NewLine>        return inputs<NewLine><NewLine>def conv3x3(in_planes, out_planes, stride=1):<NewLine>    """"""3x3 convolution with padding""""""<NewLine>    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,<NewLine>                     padding=1, bias=False)<NewLine><NewLine>class BasicBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super().__init__()<NewLine>        self.conv1 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.relu = nn.ReLU(inplace=True)<NewLine>        self.conv2 = conv3x3(planes, planes)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.add_relu = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu(out)<NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],<NewLine>                                               ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class Bottleneck(nn.Module):<NewLine>    expansion = 4<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None):<NewLine>        super(Bottleneck, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(planes)<NewLine>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)<NewLine>        self.bn3 = nn.BatchNorm2d(planes * 4)<NewLine>        self.relu1 = nn.ReLU(inplace=False)<NewLine>        self.relu2 = nn.ReLU(inplace=False)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.skip_add_relu = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu1(out)<NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        out = self.relu2(out)<NewLine>        out = self.conv3(out)<NewLine>        out = self.bn3(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.relu(out)<NewLine>        out = self.skip_add_relu.add_relu(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'relu1'],<NewLine>                            ['conv2', 'bn2', 'relu2'],<NewLine>                            ['conv3', 'bn3']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class SEBlock(nn.Module):<NewLine>    def __init__(self, channel, reduction=16):<NewLine>        super().__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.mult_xy = nn.quantized.FloatFunctional()<NewLine>        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction),<NewLine>                                nn.PReLU(),<NewLine>                                nn.Linear(channel // reduction, channel),<NewLine>                                nn.Sigmoid())<NewLine>        self.fc1 = self.fc[0]<NewLine>        self.prelu = self.fc[1]<NewLine>        self.fc2 = self.fc[2]<NewLine>        self.sigmoid = self.fc[3]<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>            self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>    def forward(self, x):<NewLine>        # print(f'&lt;inside se forward:&gt;')<NewLine>        b, c, _, _ = x.size()<NewLine>        y = self.avg_pool(x).view(b, c)<NewLine>        # y = self.fc(y).view(b, c, 1, 1)<NewLine>        y = self.fc1(y)<NewLine>        y = self.prelu_q_or_relu(y)<NewLine>        y = self.fc2(y)<NewLine>        y = self.sigmoid(y).view(b, c, 1, 1)<NewLine>        # print('--------------------------')<NewLine>        # out = x*y <NewLine>        out = self.mult_xy.mul(x, y)<NewLine>        return out<NewLine><NewLine>class IRBlock(nn.Module):<NewLine>    expansion = 1<NewLine>    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):<NewLine>        super().__init__()<NewLine>        self.bn0 = nn.BatchNorm2d(inplanes)<NewLine>        if disable_single_bns:<NewLine>            self.bn0_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn0_or_identity = self.bn0<NewLine><NewLine>        self.conv1 = conv3x3(inplanes, inplanes)<NewLine>        self.bn1 = nn.BatchNorm2d(inplanes)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        <NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>            self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>        self.conv2 = conv3x3(inplanes, planes, stride)<NewLine>        self.bn2 = nn.BatchNorm2d(planes)<NewLine>        self.downsample = downsample<NewLine>        self.stride = stride<NewLine>        self.use_se = use_se<NewLine>        # if self.use_se:<NewLine>        self.se = SEBlock(planes)<NewLine>        self.add_residual = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        residual = x<NewLine>        # TODO:<NewLine>        # this needs to be quantized as well!<NewLine>        out = self.bn0_or_identity(x)<NewLine><NewLine>        out = self.conv1(out)<NewLine>        out = self.bn1(out)<NewLine>        # out = self.prelu(out)<NewLine>        out = self.prelu_q_or_relu(out)<NewLine><NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        if self.use_se:<NewLine>            out = self.se(out)<NewLine>        if self.downsample is not None:<NewLine>            residual = self.downsample(x)<NewLine>        # out += residual<NewLine>        # out = self.prelu(out)<NewLine>        out = self.prelu_q_or_relu(out)<NewLine>        # we may need to change prelu into relu and instead of add, use add_relu here<NewLine>        out = self.add_residual.add(out, residual)<NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [# ['bn0'],<NewLine>                            ['conv1', 'bn1'],<NewLine>                            ['conv2', 'bn2']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine>class ResNet(nn.Module):<NewLine><NewLine>    def __init__(self, block, layers, use_se=True):<NewLine>        self.inplanes = 64<NewLine>        self.use_se = use_se<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, bias=False)<NewLine>        self.bn1 = nn.BatchNorm2d(64)<NewLine>        self.prelu = nn.PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>        # This is to only get rid of the unimplemented CPUQuantization type error<NewLine>        # when we use PReLU_Quantized during test time<NewLine>        if use_relu:<NewLine>            self.prelu_q_or_relu = torch.relu<NewLine>        else:<NewLine>             self.prelu_q_or_relu = self.prelu_q<NewLine><NewLine>        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)<NewLine>        self.layer1 = self._make_layer(block, 64, layers[0])<NewLine>        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)<NewLine>        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)<NewLine>        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)<NewLine>        self.bn2 = nn.BatchNorm2d(512)<NewLine>        # This is to get around the single BatchNorms not getting fused and thus causing <NewLine>        # a RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPU' backend.<NewLine>        # 'aten::native_batch_norm' is only available for these backends: [CPU, MkldnnCPU, BackendSelect, Named, Autograd, Profiler, Tracer, Autocast, Batched].<NewLine>        # during test time<NewLine>        if disable_single_bns:<NewLine>            self.bn2_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn2_or_identity = self.bn2<NewLine><NewLine>        self.dropout = nn.Dropout()<NewLine>        self.fc = nn.Linear(512 * 7 * 7, 512)<NewLine>        self.bn3 = nn.BatchNorm1d(512)<NewLine>        if disable_single_bns:<NewLine>            self.bn3_or_identity = torch.nn.Identity()<NewLine>        else:<NewLine>            self.bn3_or_identity = self.bn3<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.xavier_normal_(m.weight)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine>    def _make_layer(self, block, planes, blocks, stride=1):<NewLine>        downsample = None<NewLine>        if stride != 1 or self.inplanes != planes * block.expansion:<NewLine>            downsample = nn.Sequential(<NewLine>                nn.Conv2d(self.inplanes, planes * block.expansion,<NewLine>                          kernel_size=1, stride=stride, bias=False),<NewLine>                nn.BatchNorm2d(planes * block.expansion),<NewLine>            )<NewLine><NewLine>        layers = []<NewLine>        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))<NewLine>        self.inplanes = planes<NewLine>        for i in range(1, blocks):<NewLine>            layers.append(block(self.inplanes, planes, use_se=self.use_se))<NewLine><NewLine>        return nn.Sequential(*layers)<NewLine><NewLine>    def forward(self, x):<NewLine>        <NewLine>        x = self.quant(x)<NewLine>        x = self.conv1(x)<NewLine>        # TODO: single bn needs to be fused<NewLine>        x = self.bn1(x)<NewLine><NewLine>        # x = self.prelu(x)<NewLine>        x = self.prelu_q_or_relu(x)<NewLine><NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.bn2_or_identity(x)<NewLine>        x = self.dropout(x)<NewLine>        # x = x.view(x.size(0), -1)<NewLine>        x = x.reshape(x.size(0), -1)<NewLine>        x = self.fc(x)<NewLine>        # TODO: single bn needs to be fused<NewLine>        x = self.bn3_or_identity(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine>        fuse_modules(self, ['conv1', 'bn1'], inplace=True)<NewLine>        for m in self.modules():<NewLine>            if type(m) == Bottleneck or type(m) == BasicBlock or type(m) == IRBlock:<NewLine>                m.fuse_model()<NewLine><NewLine>class SimpleNetwork(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1)<NewLine>        self.bn1 = nn.BatchNorm2d(10)<NewLine>        self.relu1 = nn.ReLU()<NewLine><NewLine>        self.prelu_q = PReLU_Quantized(nn.PReLU())<NewLine>        self.bn = nn.BatchNorm2d(10)<NewLine><NewLine>        self.prelu_q_or_relu = torch.relu if use_relu else self.prelu_q<NewLine>        self.bn_or_identity = nn.Identity() if disable_single_bns else self.bn    <NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine>    <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine><NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu1(x)<NewLine><NewLine>        x = self.prelu_q_or_relu(x)<NewLine>        x = self.bn_or_identity(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>def resnet18(use_se=True, **kwargs):<NewLine>    return ResNet(IRBlock, [2, 2, 2, 2], use_se=use_se, **kwargs)<NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"")/1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>def evaluate(model, data_loader, eval_batches):<NewLine>    model.eval()<NewLine>    with torch.no_grad():<NewLine>        for i, (image, target) in enumerate(data_loader):<NewLine>            features = model(image)<NewLine>            print(f'{i})feature dims: {features.shape}')<NewLine>            if i &gt;= eval_batches:<NewLine>                return<NewLine><NewLine>def load_quantized(model, quantized_checkpoint_file_path):<NewLine>    model.eval()<NewLine>    if type(model) == ResNet:<NewLine>        model.fuse_model()<NewLine>    # Specify quantization configuration<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    checkpoint = torch.load(quantized_checkpoint_file_path, map_location=torch.device('cpu'))<NewLine>    model.load_state_dict(checkpoint)<NewLine>    print_size_of_model(model)<NewLine>    return model<NewLine><NewLine>def test_the_model(model, dtloader):<NewLine>    current_dir = abspath(dirname(__file__))<NewLine>    model = load_quantized(model, join(current_dir, 'data', 'model_quantized_jit.pth'))<NewLine>    model.eval()<NewLine>    img, _ = next(iter(dtloader))<NewLine>    embd1 = model(img)<NewLine><NewLine>def quantize_model(model, dtloader):<NewLine>    calibration_batches = 10 <NewLine>    saved_model_dir = 'data'<NewLine>    scripted_quantized_model_file = 'model_quantized_jit.pth'<NewLine>    # model = resnet18()<NewLine>    model.eval()<NewLine>    if type(model) == ResNet:<NewLine>        model.fuse_model()<NewLine>    print_size_of_model(model)<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>    print(model.qconfig)<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine><NewLine>    print(f'Model after fusion(prepared): {model}')<NewLine><NewLine>    # Calibrate first<NewLine>    print('Post Training Quantization Prepare: Inserting Observers')<NewLine>    print('\n Inverted Residual Block:After observer insertion \n\n', model.conv1)<NewLine><NewLine>    # Calibrate with the training set<NewLine>    evaluate(model, dtloader, eval_batches=calibration_batches)<NewLine>    print('Post Training Quantization: Calibration done')<NewLine><NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    print('Post Training Quantization: Convert done')<NewLine>    print('\n Inverted Residual Block: After fusion and quantization, note fused modules: \n\n', model.conv1)<NewLine><NewLine>    print(""Size of model after quantization"")<NewLine>    print_size_of_model(model)<NewLine>    script = torch.jit.script(model)<NewLine>    path_tosave = join(dirname(abspath(__file__)), saved_model_dir, scripted_quantized_model_file)<NewLine>    print(f'path to save: {path_tosave}')<NewLine>    with open(path_tosave, 'wb') as f:<NewLine>        torch.save(model.state_dict(), f)<NewLine><NewLine>    print(f'model after quantization (prepared and converted:) {model}')<NewLine>    # torch.jit.save(script, path_tosave)<NewLine><NewLine>dataset = FakeData(1000, image_size=(3, 112, 112), num_classes=5, transform=transforms.ToTensor())<NewLine>data_loader = DataLoader(dataset, batch_size=1)<NewLine><NewLine># quantize the model <NewLine>model = resnet18()<NewLine># model = SimpleNetwork()<NewLine>quantize_model(model, data_loader)<NewLine><NewLine># and load and test the quantized model<NewLine>model = resnet18()<NewLine># model = SimpleNetwork()<NewLine>test_the_model(model, data_loader)<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  1:45pm; <NewLine> REPLY_DATE 2: July 15, 2020,  2:36am; <NewLine> REPLY_DATE 3: July 15, 2020,  5:23am; <NewLine> REPLY_DATE 4: July 15, 2020,  5:26am; <NewLine> REPLY_DATE 5: July 15, 2020,  5:34am; <NewLine> REPLY_DATE 6: July 18, 2020,  1:59am; <NewLine> REPLY_DATE 7: July 21, 2020, 11:53pm; <NewLine> REPLY_DATE 8: July 22, 2020,  7:36am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
76884,ONNX export of quantized model,2020-04-15T15:14:40.744Z,9,1212,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve tried to export simple model using ONNX export and faced an error that ask me to report a bug.</p><NewLine><pre><code class=""lang-auto"">import torch <NewLine>import onnx <NewLine>import io<NewLine><NewLine>import torch._C as _C<NewLine>OperatorExportTypes = _C._onnx.OperatorExportTypes<NewLine><NewLine>class Net(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.cnn = torch.nn.Conv2d(1,1,1)<NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        return self.cnn(x)<NewLine><NewLine>model = Net()<NewLine>model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>torch.backends.quantized.engine = 'fbgemm'<NewLine>model = torch.quantization.prepare(model, inplace=False)<NewLine>torch.quantization.convert(model, inplace=True)<NewLine>print(model)<NewLine>inputs = torch.ones((1,10,224,224))<NewLine>with torch.no_grad():<NewLine>    with io.BytesIO() as f:<NewLine>        torch.onnx.export(<NewLine>            model,<NewLine>            inputs,<NewLine>            f,<NewLine>            operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,<NewLine>#             verbose=True,  # NOTE: uncomment this for debugging<NewLine>#             export_params=True,<NewLine>        )<NewLine>        onnx_model = onnx.load_from_string(f.getvalue())<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Net(<NewLine>  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)<NewLine>  (cnn): QuantizedConv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)<NewLine>)<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-42-9f9e68519c44&gt; in &lt;module&gt;<NewLine>     27             model,<NewLine>     28             inputs,<NewLine>---&gt; 29             f,<NewLine>     30 #             operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,<NewLine>     31 #             verbose=True,  # NOTE: uncomment this for debugging<NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>    170                         do_constant_folding, example_outputs,<NewLine>    171                         strip_doc_string, dynamic_axes, keep_initializers_as_inputs,<NewLine>--&gt; 172                         custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>    173 <NewLine>    174 <NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)<NewLine>     90             dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs,<NewLine>     91             custom_opsets=custom_opsets, enable_onnx_checker=enable_onnx_checker,<NewLine>---&gt; 92             use_external_data_format=use_external_data_format)<NewLine>     93 <NewLine>     94 <NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format)<NewLine>    508                                                             example_outputs, propagate,<NewLine>    509                                                             _retain_param_name, val_do_constant_folding,<NewLine>--&gt; 510                                                             fixed_batch_size=fixed_batch_size)<NewLine>    511 <NewLine>    512             # TODO: Don't allocate a in-memory string for the protobuf<NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size)<NewLine>    348             model.graph, tuple(in_vars), False, propagate)<NewLine>    349     else:<NewLine>--&gt; 350         graph, torch_out = _trace_and_get_graph_from_model(model, args)<NewLine>    351         state_dict = _unique_state_dict(model)<NewLine>    352         params = list(state_dict.values())<NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/onnx/utils.py in _trace_and_get_graph_from_model(model, args)<NewLine>    305 <NewLine>    306     trace_graph, torch_out, inputs_states = \<NewLine>--&gt; 307         torch.jit._get_trace_graph(model, args, _force_outplace=False, _return_inputs_states=True)<NewLine>    308     warn_on_static_input_change(inputs_states)<NewLine>    309 <NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py in _get_trace_graph(f, args, kwargs, _force_outplace, return_inputs, _return_inputs_states)<NewLine>    275     if not isinstance(args, tuple):<NewLine>    276         args = (args,)<NewLine>--&gt; 277     outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)<NewLine>    278     return outs<NewLine>    279 <NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    556             result = self._slow_forward(*input, **kwargs)<NewLine>    557         else:<NewLine>--&gt; 558             result = self.forward(*input, **kwargs)<NewLine>    559         for hook in self._forward_hooks.values():<NewLine>    560             hook_result = hook(self, input, result)<NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py in forward(self, *args)<NewLine>    358             in_vars + module_state,<NewLine>    359             _create_interpreter_name_lookup_fn(),<NewLine>--&gt; 360             self._force_outplace,<NewLine>    361         )<NewLine>    362 <NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py in wrapper(*args)<NewLine>    342             trace_inputs = _unflatten(args[:len(in_vars)], in_desc)<NewLine>    343 <NewLine>--&gt; 344             ret_inputs.append(tuple(x.clone(memory_format=torch.preserve_format) for x in args))<NewLine>    345             if self._return_inputs_states:<NewLine>    346                 inputs_states.append(_unflatten(args[:len(in_vars)], in_desc))<NewLine><NewLine>~/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/jit/__init__.py in &lt;genexpr&gt;(.0)<NewLine>    342             trace_inputs = _unflatten(args[:len(in_vars)], in_desc)<NewLine>    343 <NewLine>--&gt; 344             ret_inputs.append(tuple(x.clone(memory_format=torch.preserve_format) for x in args))<NewLine>    345             if self._return_inputs_states:<NewLine>    346                 inputs_states.append(_unflatten(args[:len(in_vars)], in_desc))<NewLine><NewLine>RuntimeError: self.qscheme() == at::kPerTensorAffine INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1586761698468/work/aten/src/ATen/native/quantized/QTensor.cpp:190, please report a bug to PyTorch. clone for quantized Tensor only works for PerTensorAffine scheme right now<NewLine></code></pre><NewLine><p>What do I do incorrectly?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"April 15, 2020,  3:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to convert quantized model with to ONNX than to Caffe2?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure if it helps but here <a href=""https://pytorch.org/docs/stable/tensor_attributes.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/tensor_attributes.html</a> I have found this “<code>Quantized and complex types are not yet supported.</code>”</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is about <code>torch.Tensor</code> instantiating. Which types are supported as <code>dtype</code> parameter</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Converting quantized model to ONNX, isn’t supported yet.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a>: Can you take a look at this issue?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I forgot to mention that I used pytorch verdion 1.6.0 from nightly build via conda</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/zetyquickly"">@zetyquickly</a>, it is currently only possible to convert quantized model to Caffe2 using ONNX. The onnx file generated in the process is specific to Caffe2.<br/><NewLine>If this is something you are still interested in, then you need to run a traced model through the onnx export flow. You can use the following code for reference</p><NewLine><pre><code class=""lang-auto"">        class ConvModel(torch.nn.Module):<NewLine>            def __init__(self):<NewLine>                super(ConvModel, self).__init__()<NewLine>                self.qconfig = torch.quantization.default_qconfig<NewLine>                self.fc1 = torch.quantization.QuantWrapper(torch.nn.Conv2d(3, 5, 2, bias=True).to(dtype=torch.float))<NewLine><NewLine>            def forward(self, x):<NewLine>                x = self.fc1(x)<NewLine>                return x<NewLine>        torch.backends.quantized.engine = ""qnnpack""<NewLine>        qconfig = torch.quantization.default_qconfig<NewLine>        model = ConvModel()<NewLine>        model.qconfig = qconfig<NewLine>        model = torch.quantization.prepare(model)<NewLine>        model = torch.quantization.convert(model)<NewLine><NewLine>        x_numpy = np.random.rand(1, 3, 6, 6).astype(np.float32)<NewLine>        x = torch.from_numpy(x_numpy).to(dtype=torch.float)<NewLine>        outputs = model(x)<NewLine>        input_names = [""x""]<NewLine>        outputs = model(x)<NewLine><NewLine>        traced = torch.jit.trace(model, x)<NewLine>        buf = io.BytesIO()<NewLine>        torch.jit.save(traced, buf)<NewLine>        buf.seek(0)<NewLine><NewLine>        model = torch.jit.load(buf)<NewLine>        f = io.BytesIO()<NewLine>        torch.onnx.export(model, x, f, input_names=input_names, example_outputs=outputs,<NewLine>                          operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)<NewLine>        f.seek(0)<NewLine><NewLine>        onnx_model = onnx.load(f)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> thank you very much for your answer.</p><NewLine><p>You’re right I am interested in conversion to Caffe2.<br/><NewLine>There are some moments in example that confuse me. Could please reveal it for us?</p><NewLine><ol><NewLine><li><NewLine><code>torch.jit.trace</code> and <code>torch.onnx.export</code>. I thought that they are mutually exclusive functionalities: one for TorchScript and the second for ONNX conversion. While ONNX model needs backend to be executed, TorchScript is standalone. Why do we need TorchScript conversion here before ONNX export? Previously I saw opinions like that <a href=""https://github.com/pytorch/pytorch/issues/27569#issuecomment-539738922"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/27569#issuecomment-539738922</a><NewLine></li><NewLine><li>In general words how are connected Pytorch JIT, TorchScript and ONNX? Why do we still need to convert anything from PyTorch to Caffe2 if TorchScript model is created?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>The flow is slightly different for quantized ops (so the regular pytorch -&gt; onnx conversion flow rule doesn’t directly apply).<br/><NewLine>We tried to re-use some of the existing functionality of converting traced ops from pytorch to onnx for quantized models hence it is necessary to first trace it. Similarly it is also necessary to set <code>operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK</code> during the conversion flow for quantized ops.<br/><NewLine>TorchScript models are not directly runnable on Caffe2 backend. So we need to convert it to the expected backend using onnx.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply <a class=""mention"" href=""/u/supriyar"">@supriyar</a> ,</p><NewLine><ol><NewLine><li>Does it mean that we can convert to onnx scripted parts of network (using <code>torch.jit.script</code>) ?</li><NewLine><li>What if our network contains of operators that aren’t available in TorchScript but available in Caffe2 (e.g. RoIAlign)?</li><NewLine><li>Optionally is it possible to use quantized layers with TorchScript backend on mobile (I mean without additional conversion to Caffe2 using ONNX)?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Does it mean that we can convert to onnx scripted parts of network (using  <code>torch.jit.script</code> ) ?</li><NewLine></ol><NewLine><p>I haven’t tried torch.jit.script for quantized pytorch network to onnx to Caffe2. But torch.jit.trace should work.</p><NewLine><ol start=""2""><NewLine><li>What if our network contains of operators that aren’t available in TorchScript but available in Caffe2 (e.g. RoIAlign)?</li><NewLine></ol><NewLine><p>At this point this is only limited to operators present in both quantized Pytorch and quantized Caffe2 framework.</p><NewLine><ol start=""3""><NewLine><li>Optionally is it possible to use quantized layers with TorchScript backend on mobile (I mean without additional conversion to Caffe2 using ONNX)?</li><NewLine></ol><NewLine><p>You can directly run quantized pytorch network on mobile using PyTorch Mobile which is highly recommended over converting to Caffe2. Check out <a href=""https://pytorch.org/mobile/home/"" rel=""nofollow noopener"">https://pytorch.org/mobile/home/</a>.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a><br/><NewLine>Dose it now support converting quantized model to ONNX in dev-version or stable version?</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>General export of quantized models to ONNX isn’t currently supported. We only support conversion to ONNX for Caffe2 backend. This thread has additional context on what we currently support - <a href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/8"">ONNX export of quantized model</a></p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is generic onnx export support for quantized models (eg for import with onnx runtime) on the roadmap?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/albertotono; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ebarsoum; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/LMerCy; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/RicCu; <NewLine> ,"REPLY_DATE 1: April 16, 2020,  3:24pm; <NewLine> REPLY_DATE 2: April 16, 2020,  3:31pm; <NewLine> REPLY_DATE 3: April 16, 2020,  3:40pm; <NewLine> REPLY_DATE 4: April 16, 2020,  5:56pm; <NewLine> REPLY_DATE 5: April 16, 2020,  9:31pm; <NewLine> REPLY_DATE 6: April 16, 2020, 10:15pm; <NewLine> REPLY_DATE 7: April 17, 2020,  4:25pm; <NewLine> REPLY_DATE 8: April 17, 2020,  9:00pm; <NewLine> REPLY_DATE 9: April 20, 2020,  4:36pm; <NewLine> REPLY_DATE 10: April 20, 2020,  4:53pm; <NewLine> REPLY_DATE 11: April 20, 2020,  5:13pm; <NewLine> REPLY_DATE 12: July 21, 2020, 10:36am; <NewLine> REPLY_DATE 13: July 21, 2020, 11:42pm; <NewLine> REPLY_DATE 14: July 22, 2020,  1:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: 1 Like; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: 3 Likes; <NewLine> 
89501,Quantized model not provide performance improvements,2020-07-16T21:00:58.401Z,1,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I try to run quantization benchmark:<br/><NewLine></p><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""16""/><NewLine><a href=""https://github.com/z-a-f/quantization_benchmarks"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars3.githubusercontent.com/u/4216323?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/z-a-f/quantization_benchmarks"" rel=""nofollow noopener"" target=""_blank"">z-a-f/quantization_benchmarks</a></h3><NewLine><p>PyTorch quantization benchmarks. Contribute to z-a-f/quantization_benchmarks development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>But I didn’t see any speed up with quantized model.<br/><NewLine>For example:<br/><NewLine>googlenet:<br/><NewLine>Train time:<br/><NewLine>q: 192.940<br/><NewLine>f: 192.940<br/><NewLine>Test time:<br/><NewLine>q: 193.114<br/><NewLine>f:  193.114</p><NewLine><p>On the third model I got:<br/><NewLine>Downloading: “<a href=""https://download.pytorch.org/models/mobilenet_v2-b0353104.pth"" rel=""nofollow noopener"">https://download.pytorch.org/models/mobilenet_v2-b0353104.pth</a>” to<br/><NewLine>…/.cache\torch\hub\checkpoints\mobilenet_v2-b0353104.pth<br/><NewLine>100%|█████████████████████████████████████| 13.6M/13.6M [00:01&lt;00:00, 8.20MB/s]</p><NewLine><p>File “…Anaconda3\envs\torch1.5\lib\site-packages\torchvision\mod<br/><NewLine>els\quantization\utils.py”, line 22, in quantize_model<br/><NewLine>raise RuntimeError(""Quantized backend not supported "")<br/><NewLine>RuntimeError: Quantized backend not supported</p><NewLine><p>Why does the quantized model not provide performance improvements?<br/><NewLine>How can I activate quantized backend?</p><NewLine><p>My environment:<br/><NewLine>conda            4.8.3<br/><NewLine>torch              1.7.0.dev20200716<br/><NewLine>torchvision     0.8.0.dev20200716<br/><NewLine>Python           3.6.10</p><NewLine><p>CPU:             i5-4670</p><NewLine></div>",https://discuss.pytorch.org/u/fel88,(Felix),fel88,"July 16, 2020,  9:00pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><blockquote><NewLine><blockquote><NewLine><p>print(torch.backends.quantized.supported_engines)<br/><NewLine>[‘none’, ‘fbgemm’]</p><NewLine></blockquote><NewLine></blockquote><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""89501"" data-username=""fel88""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/f/6a8cbe/40.png"" width=""20""/> fel88:</div><NewLine><blockquote><NewLine><p>Quantized backend not supported</p><NewLine></blockquote><NewLine></aside><NewLine><p>one thing to try would be:</p><NewLine><pre><code class=""lang-auto"">torch.backends.quantized.engine = 'fbgemm'<NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It still doesn’t work, but<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a><br/><NewLine>works well. It is enough for my purposes.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/fel88; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/fel88; <NewLine> ,"REPLY_DATE 1: July 20, 2020,  2:51pm; <NewLine> REPLY_DATE 2: July 20, 2020,  3:46pm; <NewLine> REPLY_DATE 3: July 21, 2020,  2:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
84855,Converting quantized models from PyTorch to ONNX,2020-06-10T07:39:38.299Z,1,565,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to export a quantized int8 PyTorch model to ONNX from the following tutorial.</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><p>However, PyTorch to ONNX conversion of quantized models is not supported. Various types of quantized models will either explicitly say their conversion is not supported or they will throw an attribute error.</p><NewLine><p>My question is — how do we do the conversion manually? Specifically, how do we define a custom mapping of ONNX operations for PyTorch classes? I assume the logic is the same for non-quantized layers, whose conversion needed to be defined until it was built-in, but I am having trouble finding an example.</p><NewLine></div>",https://discuss.pytorch.org/u/Joseph_Konan,(Joseph Konan),Joseph_Konan,"June 10, 2020,  7:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/supriyar"">@supriyar</a> might be able to help.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently only support conversion to ONNX for Caffe2 backend. This thread has additional context on what we currently support - <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/8"">ONNX export of quantized model</a></p><NewLine><p>If you would like to add custom conversion logic to onnx operators for quantized pytorch ops you can follow the code in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_caffe2.py</a> which adds the mapping for the Caffe2 ops in ONNX.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>＠ Joseph_Konan Hello, can you now convert the quantified model to ONNX, thank you!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the update — I’ll look into this!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Joseph_Konan; <NewLine> ,"REPLY_DATE 1: June 16, 2020, 11:42pm; <NewLine> REPLY_DATE 2: July 21, 2020,  1:39am; <NewLine> REPLY_DATE 3: July 21, 2020,  2:33am; <NewLine> REPLY_DATE 4: July 21, 2020,  2:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89042,Loading Quantized Model from State_Dict with Version==None,2020-07-13T19:59:52.515Z,0,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi PyTorch community,</p><NewLine><p><strong>TLDR;</strong> DistilBert’s <code>nn.quantized.Linear</code> encounters <code>KeyError</code> when loading from <code>state_dict</code>. Saving from <code>state_dict</code> uses version <code>3</code> format, but loading evaluates <code>local_metadata.get('version', None) == None</code> which defaults to using version <code>1</code> format.</p><NewLine><p>I have a problem with loading DistilBert classifier. I would load it from a pre-trained model, fine-tune it, quantize it, then save its <code>state_dict</code>. The issue happens when saving and reloading this quantized version. When DynamicQuantizedLinear generates keys, it uses this format:<br/><NewLine><code>_distilbert.transformer.layer.0.attention.q_lin._packed_params.weight</code></p><NewLine><pre><code class=""lang-auto""># Version 3<NewLine>    #   self<NewLine>    #   |--- _packed_params : (Tensor, Tensor) representing (weight, bias)<NewLine>    #                         of LinearPackedParams<NewLine>    #   |--- dtype : torch.dtype<NewLine></code></pre><NewLine><p>Printing the state_dict in that key:</p><NewLine><pre><code class=""lang-auto""># print(state_dict['_distilbert.transformer.layer.0.attention.q_lin._packed_params.weight'])<NewLine>tensor([[ 0.0357,  0.0365,  0.0119,  ..., -0.0230,  0.0199,  0.0397],<NewLine>        [ 0.0119, -0.0349,  0.0048,  ...,  0.0294, -0.0127, -0.0119],<NewLine>        [ 0.0540,  0.0159, -0.0032,  ...,  0.0008, -0.0183, -0.0016],<NewLine>        ...,<NewLine>        [ 0.0064, -0.0079,  0.0302,  ..., -0.0199,  0.0008, -0.0095],<NewLine>        [ 0.0024, -0.0056,  0.0183,  ...,  0.0008,  0.0175,  0.0270],<NewLine>        [-0.0024, -0.0119, -0.0238,  ...,  0.0294,  0.0199,  0.0175]],<NewLine>       size=(768, 768), dtype=torch.qint8,<NewLine>       quantization_scheme=torch.per_tensor_affine, scale=0.0007942558731883764,<NewLine>       zero_point=0)<NewLine></code></pre><NewLine><p>However, when loading the model using the same Python environment and on the same machine, the de-serialization fails with the following error:</p><NewLine><pre><code class=""lang-auto"">File ""/home/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 827, in load<NewLine>    state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)<NewLine>  File ""/home/.venv/lib/python3.6/site-packages/torch/nn/quantized/modules/linear.py"", line 207, in _load_from_state_dict<NewLine>    weight = state_dict.pop(prefix + 'weight')<NewLine>KeyError: '_distilbert.transformer.layer.0.attention.q_lin.weight'<NewLine></code></pre><NewLine><p>Here’s what the de-serialization method that fails looks like:</p><NewLine><pre><code class=""lang-auto""># file: torch/nn/quantized/modules/linear.py <NewLine>    # ===== Deserialization methods =====<NewLine>    # Counterpart to the serialization methods, we must pack the serialized QTensor<NewLine>    # weight into its packed format for use by the FBGEMM ops.<NewLine>    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,<NewLine>                              missing_keys, unexpected_keys, error_msgs):<NewLine>        self.scale = float(state_dict[prefix + 'scale'])<NewLine>        state_dict.pop(prefix + 'scale')<NewLine><NewLine>        self.zero_point = int(state_dict[prefix + 'zero_point'])<NewLine>        state_dict.pop(prefix + 'zero_point')<NewLine><NewLine>        version = local_metadata.get('version', None)<NewLine>        if version is None or version == 1:<NewLine>            # We moved the parameters into a LinearPackedParameters submodule<NewLine>            weight = state_dict.pop(prefix + 'weight')<NewLine>            bias = state_dict.pop(prefix + 'bias')<NewLine>            state_dict.update({prefix + '_packed_params.weight': weight,<NewLine>                               prefix + '_packed_params.bias': bias})<NewLine><NewLine>        super(Linear, self)._load_from_state_dict(state_dict, prefix, local_metadata, False,<NewLine>                                                  missing_keys, unexpected_keys, error_msgs)<NewLine></code></pre><NewLine><p>The issue seems to be that the backward compatibility if-statement defaults to assuming the model was serialized using an earlier version if <code>version is None</code>. This fails in my case. Changing <code>if version is None or version == 1:</code> to <code>if version == 1:</code> fixes the issue for me but I’d like a more sustainable solution.</p><NewLine><p>How do I make sure my model’s version evaluates to the correct value?</p><NewLine><p>Thanks in advance for any help!</p><NewLine></div>",https://discuss.pytorch.org/u/salimmj,(Salim),salimmj,"July 13, 2020,  7:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/salimmj"">@salimmj</a>,</p><NewLine><p>if you are seeing this on a recent version of PyTorch (v1.5 or nightlies), would you mind filing a github issue?</p><NewLine><p>for a quick local fix, you can also modify the checkpoint data.  Here is a code snippet (for a different case) which is doing something similar:</p><NewLine><pre><code class=""lang-auto"">    def adjust_convbn_metadata(mod, prefix, old_state_dict):<NewLine>        for name, child in mod._modules.items():<NewLine>            new_prefix = prefix + '.' + name if prefix != '' else name<NewLine>            if isinstance(child, torch.nn.intrinsic.qat.ConvBn2d):<NewLine>                old_state_dict._metadata[new_prefix]['version'] = 2<NewLine>            adjust_convbn_metadata(child, new_prefix, old_state_dict)<NewLine>    adjust_convbn_metadata(model, '', checkpoint['model'])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a>,</p><NewLine><p>Thanks for your help, I was actually just working on this. I ended up fixing it like this:</p><NewLine><pre><code class=""lang-auto""># This is a temporary fix for https://discuss.pytorch.org/t/loading-quantized-model-from-state-dict-with-version-none/89042<NewLine>model_checkpoint['state_dict'] = OrderedDict(model_checkpoint['state_dict'])<NewLine>if not hasattr(model_checkpoint['state_dict'], '_metadata'):<NewLine>     setattr(model_checkpoint['state_dict'], '_metadata', OrderedDict({'version': 2}))<NewLine></code></pre><NewLine><p>Your code seems more specific, I wonder if mine could break. For now I only really need to change the version for the Linear layer so I don’t know if doing it like this is going to break something else.</p><NewLine><p>I will file a Github issue!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/salimmj; <NewLine> ,"REPLY_DATE 1: July 20, 2020,  4:25pm; <NewLine> REPLY_DATE 2: July 20, 2020,  4:45pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
88553,Network pruning error,2020-07-09T11:57:47.483Z,10,140,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am very new to this topic but I am trying to prune the model I am working with. For reference, I am using <a href=""https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"" rel=""nofollow noopener"">this</a> page. The model is quite big, containing different encoders, ResNet modules, and decoders. So, I’m guessing that I have to prune each network individually (I couldn’t find a reference where the whole model is being pruned together, but please attach some links where it’s being done). The list of different modules are like:</p><NewLine><pre><code class=""lang-auto"">module.model_enc1.1.weight<NewLine>module.model_enc1.1.bias<NewLine>module.model_enc1.2.weight<NewLine>module.model_enc1.2.bias<NewLine>module.model_enc1.4.weight<NewLine>module.model_enc1.4.bias<NewLine>module.model_enc1.5.weight<NewLine>.<NewLine>.<NewLine>.<NewLine></code></pre><NewLine><p>So I’m only taking the <code>module.model_enc1.1.weight</code> using the following code:</p><NewLine><pre><code class=""lang-auto"">test = netM.module.model_enc1<NewLine></code></pre><NewLine><p>where <code>netM</code> contains the model weights (<code>&lt;class 'torch.nn.parallel.data_parallel.DataParallel'&gt; </code>).</p><NewLine><p>So <code>test</code> contains the following model:</p><NewLine><pre><code class=""lang-auto"">Sequential(<NewLine>  (0): ReflectionPad2d((3, 3, 3, 3))<NewLine>  (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))<NewLine>  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>  (3): ReLU(inplace=True)<NewLine>  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))<NewLine>  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>  (6): ReLU(inplace=True)<NewLine>)<NewLine></code></pre><NewLine><p>And when I run the pruning method by pytorch<br/><NewLine><code>prune.random_unstructured(test, name='1.weight', amount=0.3)</code>, I get the following error:</p><NewLine><blockquote><NewLine><hr/><NewLine><p>AttributeError                            Traceback (most recent call last)<br/><NewLine> in <br/><NewLine>----&gt; 1 prune.random_unstructured(test, name=‘1.weight’, amount=0.3)</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/nn/utils/prune.py in random_unstructured(module, name, amount)<br/><NewLine>851<br/><NewLine>852     “”""<br/><NewLine>–&gt; 853     RandomUnstructured.apply(module, name, amount)<br/><NewLine>854     return module<br/><NewLine>855</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/nn/utils/prune.py in apply(cls, module, name, amount)<br/><NewLine>473         “”""<br/><NewLine>474         return super(RandomUnstructured, cls).apply(<br/><NewLine>–&gt; 475             module, name, amount=amount<br/><NewLine>476         )<br/><NewLine>477</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/nn/utils/prune.py in apply(cls, module, name, *args, **kwargs)<br/><NewLine>155         # starting from the state it is found in prior to this iteration of<br/><NewLine>156         # pruning<br/><NewLine>–&gt; 157         orig = getattr(module, name)<br/><NewLine>158<br/><NewLine>159         # If this is the first time pruning is applied, take care of moving</p><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in <strong>getattr</strong>(self, name)<br/><NewLine>592                 return modules[name]<br/><NewLine>593         raise AttributeError(""’{}’ object has no attribute ‘{}’"".format(<br/><NewLine>–&gt; 594             type(self).<strong>name</strong>, name))<br/><NewLine>595<br/><NewLine>596     def <strong>setattr</strong>(self, name, value):</p><NewLine><p>AttributeError: ‘Sequential’ object has no attribute ‘1.weight’</p><NewLine></blockquote><NewLine><p>How do I fix this? Is there any better way to prune these networks?</p><NewLine></div>",https://discuss.pytorch.org/u/Flock1,(Flock Anizak),Flock1,"July 9, 2020, 11:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88553"" data-username=""Flock1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/flock1/40/12220_2.png"" width=""20""/> Flock1:</div><NewLine><blockquote><NewLine><p>del is quite big, containing different encoders, ResNet modules, and decoders. So, I’m guessing that I have to prune each network individually (I couldn’t find a reference where the whole model is being pruned together, but please attach some links where it’s being done). The list of different modules are like:</p><NewLine></blockquote><NewLine></aside><NewLine><p>print out test and look at the structure of the module. You’ll need to index test and use name=‘weight’. ‘1.weight’ is not an acceptable parameter name.</p><NewLine><p>Try prune.random_unstructured(test[0], name=‘weight’, amount=0.3) or any other index in place of 0.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88553"" data-username=""Flock1""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/flock1/40/12220_2.png"" width=""20""/> Flock1:</div><NewLine><blockquote><NewLine><p>I am very new to this topic but I am trying to prune the model I am working with. For reference, I am using <a href=""https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"" rel=""nofollow noopener"">this</a> page. The model is quite big, containing different encoders, ResNet modules, and decoders. So, I’m guessing that I have to prune each network individually (I couldn’t find a reference where the whole model is being pruned together, but please attach some links where it’s being done). The list of different modules are like:</p><NewLine><pre><code class=""lang-auto""><NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/michela"">@Michela</a> could you take a look?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. I got <code>1.weight</code> by running this <code>print(list(netM.module.model_enc1.named_parameters()))</code>. The output is:</p><NewLine><pre><code class=""lang-auto"">[('1.weight', Parameter containing:<NewLine>tensor([[[[ 2.5521e-02,  5.2238e-02,  4.7848e-04,  ...,  5.6985e-02,<NewLine>            5.1901e-02,  5.1235e-02],<NewLine>.<NewLine>.<NewLine>.<NewLine>.<NewLine></code></pre><NewLine><p>And then we have values for <code>1.bias</code>, <code>2.weight</code>, <code>2.bias</code> as mentioned above.<br/><NewLine>This is the error I got for the code you mentioned:</p><NewLine><blockquote><NewLine><p>AttributeError: ‘ReflectionPad2d’ object has no attribute ‘weight’</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Your first Conv layer in the Sequential module is at index 1. Try prune.random_unstructured(test[1], name=‘weight’, amount=0.3). I think that should work. Let me know if it doesn’t.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi. This worked. Thank you so much. Can you also tell me if there’s some way I can prune such a big network in one go? Or do I need to iterate through every layer and prune it individually?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Try this <a href=""https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#global-pruning"" rel=""nofollow noopener"">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#global-pruning</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much. I’ll check this out</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/flock1"">@Flock1</a> the issue was related to how you were accessing the parameter.  Either do <code>dict(netM.module.model_enc1.named_parameters())['1.weight']</code> or do <code>netM.module.model_enc1[1].weight</code>.<br/><NewLine><a class=""mention"" href=""/u/ani0075"">@ani0075</a>’s solution is correct (and related to the second option in the previous line of this answer) for when you want to refer to that module/parameter combination for the sake of pruning.</p><NewLine><p>Beyond that, what do you mean by pruning “in one go”? Global pruning will allow you to prune the network by pooling all parameters together and comparing them with each other while deciding which ones to prune. That’s not the same thing as pruning each layer individually, but in an efficient way, without mixing weights across layers. Which one of the two were you interested in?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/michela"">@Michela</a>. Firstly, big fan of your work when it comes to network pruning and ML applications in physics. I work on ML applications for quantum computing and astrophysics. I am so glad you replied.</p><NewLine><p>I think I am looking for Global pruning. I was kinda thinking that pruning each layer might not be an effective way compared to global pruning. I was trying one layer just to see how to go about it since it was my first time. But I want to prune the whole network. Moreover, do you recommend pruning the <code>BatchNorm</code> layer?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Global pruning is generally more flexible and has empirically been shown to have better performance – though be careful not to let it prune entire layers thus disconnecting your network!<br/><NewLine>Re: batch norm – that’s a more complicated issue, it depends what you want to achieve. Pruning batch norm params won’t really help you significantly reduce the number of params in the network. But if you prune an entire output, does it make any sense to keep its corresponding batch norm parameters? Btw, on the other hand, some even directly use batch norm to figure out which channels to prune in the respective layer. I’d recommend checking out the literature for this.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. I have one question. Can you elaborate on “entire output”? Do you mean the final layer output or something else. Please let me know.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I mean an entire output dimension in any of the hidden layers. Batch norm layers compute <code>y = γx + β</code> with parameters <code>γ,β</code> for each normalized <code>x</code> at that layer.<br/><NewLine>If you pruned the previous layer such that a specific <code>x</code> is now always 0, does it make sense to keep its corresponding <code>γ,β</code> around?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ani0075; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ani0075; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/ani0075; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Michela; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Michela; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Flock1; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/Michela; <NewLine> ,"REPLY_DATE 1: July 9, 2020,  5:20pm; <NewLine> REPLY_DATE 2: July 9, 2020,  7:03pm; <NewLine> REPLY_DATE 3: July 9, 2020,  7:27pm; <NewLine> REPLY_DATE 4: July 10, 2020,  4:53am; <NewLine> REPLY_DATE 5: July 10, 2020,  4:54am; <NewLine> REPLY_DATE 6: July 10, 2020,  1:20pm; <NewLine> REPLY_DATE 7: July 11, 2020,  9:19am; <NewLine> REPLY_DATE 8: July 14, 2020, 12:16am; <NewLine> REPLY_DATE 9: July 16, 2020, 10:48am; <NewLine> REPLY_DATE 10: July 16, 2020,  6:37pm; <NewLine> REPLY_DATE 11: July 18, 2020,  3:08pm; <NewLine> REPLY_DATE 12: July 18, 2020,  3:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
75607,Extending Quantization-Aware Training,2020-04-06T20:01:48.433Z,2,232,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I would like to extend QAT to support two below cases. Does anyone know how I can achieve these?</p><NewLine><ol><NewLine><li><NewLine><p>mixed-precision: being able to set precision for each layer separately (manually)</p><NewLine></li><NewLine><li><NewLine><p>lower precisions: being able to fake-quantize to lower than 8-bit (using a QConfig?)</p><NewLine></li><NewLine></ol><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/mg1371,,mg1371,"April 7, 2020, 12:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Raghu is adding support for sub 8 bit qat right now cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a><br/><NewLine>I think mixed precision is supported as long as you can have sub 8 bit observers, in eager mode quantization you’ll need to set the qconfig manually for each child module.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> and <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>.</p><NewLine><p>Is there a way to have fake 8-bit observers (scale factors and zero point) in current implementation?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>you’ll need to implement your own observer module (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py</a>) and fake quantize module(<a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/fake_quantize.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/fake_quantize.py</a>) to support 8-bit scale and zero_point</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi, have you implement your question? Now, I am learning QAT…</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mg1371; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/111179; <NewLine> ,"REPLY_DATE 1: April 8, 2020, 11:49pm; <NewLine> REPLY_DATE 2: April 14, 2020,  6:01pm; <NewLine> REPLY_DATE 3: April 17, 2020,  8:45pm; <NewLine> REPLY_DATE 4: July 17, 2020,  4:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
84700,Output tensor type is lost after serializing and loading back a quantized model,2020-06-09T01:13:22.765Z,1,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a></p><NewLine><p>It seems that after I serialize and load back a quantized model, the output type of quantized operators, <code>QUInt8</code>, is lost and instead it is replaced by float <code>Tensor</code> type. See below for a module with a single quantized conv layer.</p><NewLine><p>Before <code>torch.jit.save</code></p><NewLine><pre><code class=""lang-auto"">graph(%self.1 : __torch__.AnnotatedConvModel,<NewLine>      %X : Float(2, 3, 10, 10)):<NewLine>  ...<NewLine>  %input : QUInt8(2, 3, 10, 10) = aten::quantize_per_tensor(%X, %67, %68, %69), scope: __module.quant # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:43:0<NewLine>  ...<NewLine>  %Xq : QUInt8(2, 3, 8, 8) = quantized::conv2d(%input, %71, %74, %77, %80, %81, %82, %83), scope: __module.conv # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/conv.py:215:0<NewLine>  %85 : Float(2, 3, 8, 8) = aten::dequantize(%Xq), scope: __module.dequant # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:74:0<NewLine>  return (%85)<NewLine></code></pre><NewLine><p>After <code>torch.jit.load</code></p><NewLine><pre><code class=""lang-auto"">graph(%self.1 : __torch__.AnnotatedConvModel,<NewLine>      %X.1 : Tensor):<NewLine>  ...<NewLine>  %input.1 : Tensor = aten::quantize_per_tensor(%X.1, %9, %10, %11) # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:43:0<NewLine>  %Xq.1 : Tensor = quantized::conv2d(%input.1, %15, %17, %18, %19, %16, %20, %21) # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/conv.py:215:0<NewLine>  ...<NewLine>  %24 : Tensor = aten::dequantize(%Xq.1) # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:74:0<NewLine>  return (%24)<NewLine></code></pre><NewLine><p>The PyTorch frontend in TVM uses this tensor type information to decide if a torch op is invoked on a quantized tensor. See for example the case of converting adaptive avg pooling, which requires special care for quantized case, but in the Torch IR the same op <code>aten::adaptive_avg_pool2d</code> appears for both float and quantized input.</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/apache/incubator-tvm/blob/master/python/tvm/relay/frontend/pytorch.py#L600-L601"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/apache/incubator-tvm/blob/master/python/tvm/relay/frontend/pytorch.py#L600-L601"" rel=""nofollow noopener"" target=""_blank"">apache/incubator-tvm/blob/master/python/tvm/relay/frontend/pytorch.py#L600-L601</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""600"" style=""counter-reset: li-counter 599 ;""><NewLine><li>if input_types[0] == ""quint8"":</li><NewLine><li>    return qnn_torch.apply_with_upcast(data, func)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Without correct typing, we cannot convert serialized quantized PyTorch models. What happens right now is since Torch tells TVM that input tensor is float type, TVM incorrectly converts some quantized ops into float ops.</p><NewLine><p>A repro script, tested on v1.5</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch.quantization import QuantStub, DeQuantStub, default_qconfig<NewLine><NewLine><NewLine>class AnnotatedConvModel(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(AnnotatedConvModel, self).__init__()<NewLine>        self.qconfig = default_qconfig<NewLine>        self.conv = torch.nn.Conv2d(3, 3, 3, bias=False).to(dtype=torch.float)<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.conv(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine><NewLine>def quantize_model(model, inp):<NewLine>    model.qconfig = default_qconfig<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    model(inp)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine><NewLine>def test_conv():<NewLine>    inp = torch.rand(2, 3, 10, 10)<NewLine>    annotated_conv_model = AnnotatedConvModel()<NewLine>    quantize_model(annotated_conv_model, inp)<NewLine><NewLine>    trace = torch.jit.trace(annotated_conv_model, inp)<NewLine>    torch._C._jit_pass_inline(trace.graph)<NewLine>    print(trace.graph)<NewLine><NewLine>    torch.jit.save(trace, ""trace.pt"")<NewLine>    trace = torch.jit.load(""trace.pt"")<NewLine>    print(trace.graph)<NewLine><NewLine><NewLine>test_conv()<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"June 9, 2020,  1:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also posted on Github <a href=""https://github.com/pytorch/pytorch/issues/39690"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/39690</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>What if you run the graph on some sample data after you load the model?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""84700"" data-username=""masahi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/masahi/40/19725_2.png"" width=""20""/> masahi:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">  %input : QUInt8(2, 3, 10, 10) = aten::quantize_per_tensor(%X, %67, %68, %69), scope: __module.quant # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/quantized/modules/__init__.py:43:0<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>You can use the type, but don’t rely on the shape since it will probably change every time you run the model with input of different shape</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">def test_conv():<NewLine>    inp = torch.rand(2, 3, 10, 10)<NewLine>    annotated_conv_model = AnnotatedConvModel()<NewLine>    quantize_model(annotated_conv_model, inp)<NewLine><NewLine>    trace = torch.jit.trace(annotated_conv_model, inp)<NewLine>    torch._C._jit_pass_inline(trace.graph)<NewLine>    print(trace.graph)<NewLine><NewLine>    torch.jit.save(trace, ""trace.pt"")<NewLine>    loaded = torch.jit.load(""trace.pt"")<NewLine><NewLine>    for i in range(5):<NewLine>        out = loaded(torch.rand(2, 3, 10, 10))<NewLine><NewLine>    print(loaded.graph)<NewLine></code></pre><NewLine><p>Tried running a loaded graph with some inputs, it still says</p><NewLine><pre><code class=""lang-auto"">%Xq.1 : Tensor = quantized::conv2d(...)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> COrrect me if I am wrong, but I think that’s what jit does irrespective of it being quantized or not. I believe we should talk to the JIT team to somehow allow dtype to be exposed.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: June 9, 2020,  1:17am; <NewLine> REPLY_DATE 2: June 9, 2020,  1:27am; <NewLine> REPLY_DATE 3: June 9, 2020,  1:28am; <NewLine> REPLY_DATE 4: June 9, 2020,  2:37am; <NewLine> REPLY_DATE 5: July 16, 2020,  8:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
86805,Static quantizing and batch norm error (could not run aten::native_batch_norm with args from QuantCPUTensorid backend&rsquo;),2020-06-25T00:53:41.407Z,2,142,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>Working on static quantizing a few models and hitting this error on a basic Resnet18 - any insight into what is missing to complete the quantization?<br/><NewLine>Did not ‘fuse’ the BN but unclear if that is the core issue?<br/><NewLine>Any assistance would be appreciated - very hard to find much documentation.</p><NewLine><pre><code class=""lang-auto"">/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)<NewLine>   1921     return torch.batch_norm(<NewLine>   1922         input, weight, bias, running_mean, running_var,<NewLine>-&gt; 1923         training, momentum, eps, torch.backends.cudnn.enabled<NewLine>   1924     )<NewLine>   1925 <NewLine>RuntimeError: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::native_batch_norm' is only available for these backends: [CPUTensorId, CUDATensorId, MkldnnCPUTensorId, VariableTensorId].<NewLine><NewLine><NewLine></code></pre><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/lessw2020,(Less Wright),lessw2020,"June 25, 2020, 12:53am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think you need to fuse BN since there’s no quantized BN layer.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks very much - will try to fuse!  The docs implied it was more to boost accuracy vs a requirement but makes that it won’t otherwise quantize itself so to speak.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That was in fact the issue (lack of fusing).  Thanks very much <a class=""mention"" href=""/u/hx89"">@hx89</a>!</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>There is no convolutional layer in front, I want to know how to merge the bn layer，thanks!</p><NewLine><p>class MobileFaceNet(Module):<br/><NewLine>def <strong>init</strong>(self, embedding_size):<br/><NewLine>super(MobileFaceNet, self).<strong>init</strong>()<br/><NewLine>self.conv1 = Conv_Block(3, 64, kernel=3, stride=2, padding=1)<br/><NewLine>self.conv2 = Conv_Block(64, 64, kernel=3, stride=1, padding=1, groups=64)<br/><NewLine>self.conv3 = InvertedResidual(64, 64, kernel=3, stride=2, padding=1, groups=128)<br/><NewLine>self.conv4 = MakeBlocks(64, num_block=4, kernel=3, stride=1, padding=1, groups=128)<br/><NewLine>self.conv5 = InvertedResidual(64, 128, kernel=3, stride=2, padding=1, groups=256)<br/><NewLine>self.conv6 = MakeBlocks(128, num_block=6, kernel=3, stride=1, padding=1, groups=256)<br/><NewLine>self.conv7 = InvertedResidual(128, 128, kernel=3, stride=2, padding=1, groups=512)<br/><NewLine>self.conv8 = MakeBlocks(128, num_block=2, kernel=3, stride=1, padding=1, groups=256)<br/><NewLine>self.conv9 = Conv_Block(128, 512, kernel=1, stride=1, padding=0)<br/><NewLine>self.conv10 = Conv_Block(512, 512, kernel=7, stride=1, padding=0, groups=512, is_linear=True)<br/><NewLine>self.ft = Flatten()<br/><NewLine>self.ln = Linear(512, embedding_size, bias=False)<br/><NewLine>self.bn = BatchNorm1d(embedding_size)</p><NewLine><pre><code>    self.quant = QuantStub()<NewLine>    self.dequant = DeQuantStub()<NewLine><NewLine>    for m in self.modules():<NewLine>        if isinstance(m, Conv2d):<NewLine>            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels<NewLine>            m.weight.data.normal_(0, math.sqrt(2. / n))<NewLine>        elif isinstance(m, BatchNorm2d):<NewLine>            m.weight.data.fill_(1)<NewLine>            m.bias.data.zero_()<NewLine><NewLine>def forward(self, x):<NewLine>    x = self.quant(x)<NewLine>    x = self.conv1(x)<NewLine>    x = self.conv2(x)<NewLine>    x = self.conv3(x)<NewLine>    x = self.conv4(x)<NewLine>    x = self.conv5(x)<NewLine>    x = self.conv6(x)<NewLine>    x = self.conv7(x)<NewLine>    x = self.conv8(x)<NewLine>    x = self.conv9(x)<NewLine>    x = self.conv10(x)<NewLine>    x = self.ft(x)<NewLine>    x = self.ln(x)<NewLine>    x = self.bn(x)<NewLine>    x = self.dequant(x)<NewLine>    return x<NewLine><NewLine>def fuse_model(self):<NewLine>    for m in self.modules():<NewLine>        if type(m) == Conv_Block:<NewLine>            m.fuse_model()</code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lessw2020; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lessw2020; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/blueskywwc; <NewLine> ,"REPLY_DATE 1: June 25, 2020,  2:52pm; <NewLine> REPLY_DATE 2: June 25, 2020, 12:57am; <NewLine> REPLY_DATE 3: June 25, 2020,  2:52pm; <NewLine> REPLY_DATE 4: July 16, 2020,  3:14am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
88234,Backward prop per Batch of data or backward prop after one epoch,2020-07-07T13:11:02.459Z,3,100,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Given the it won’t be normal to feed a deep learning model the training data all at once (especially if the training data is 1000+) due the available RAM and other factors, I really want to know the effects of running the backward propagation after feeding all batches of data to the model per epoch versus running the backward propagation after feeding the model per batch.</p><NewLine><p>Thanks to anyone who answers this.</p><NewLine></div>",https://discuss.pytorch.org/u/Henry_Chibueze,(Henry Chibueze),Henry_Chibueze,"July 7, 2020,  1:17pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Generally larger batches will provide a more accurate estimate of the gradient, while smaller batches will introduce more noise (which is often seen as beneficial up to a certain degree).<br/><NewLine><a href=""https://www.deeplearningbook.org/contents/optimization.html"">Chapter 8.1.3 - DeepLearningBook</a> discusses these effects in more detail.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply, but I just wanted to know if backward propagation should be carried out after each batch within one epoch or after the entire batch has been fed the model (per epoch)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Usually, you would update the model after each batch, but that’s not a hard rule as explained.<br/><NewLine>Depending on your use case, you might want to accumulate the gradients and update the model after a couple of batches (or all of them).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks alot I think i get it now</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Henry_Chibueze; <NewLine> ,"REPLY_DATE 1: July 8, 2020, 10:09am; <NewLine> REPLY_DATE 2: July 8, 2020,  1:19pm; <NewLine> REPLY_DATE 3: July 9, 2020,  1:13am; <NewLine> REPLY_DATE 4: July 15, 2020, 10:41am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
89103,"Prediction , objective function and optimization in Python - AI",2020-07-14T06:57:55.124Z,0,43,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, What are the different methods i can build prediction models, link objective functions and do optimization using pytorch. Pl suggest</p><NewLine></div>",https://discuss.pytorch.org/u/hpsuresh,(Suresha Hp),hpsuresh,"July 14, 2020,  6:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure I understand the question correctly, but it seems you are looking for the complete support of PyTorch methods?<br/><NewLine>If that’s the case, the <a href=""https://pytorch.org/docs/stable/index.html"">docs</a> and <a href=""https://pytorch.org/tutorials/"">tutorials</a> might be a good starter. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: July 15, 2020,  9:35am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
89154,Is it planned to support nn.Embeddings quantization?,2020-07-14T13:19:44.747Z,0,59,"<div class=""post"" itemprop=""articleBody""><NewLine><p>First of all, I would like to thank you for the awesome  <code>torch.quantization</code> . But at the moment, the quantization of embeddings is not supported, although ususally it’s one of the biggest (in terms of size) parts of the model (in NLP).<br/><NewLine>I tried to use  <code>nn.Embeddings</code>  as  <code>nn.Linear</code>  because they have a very similar nature, but get the following error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'aten::index_select' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::index_select' is only available for these backends: [CPUTensorId, CUDATensorId, SparseCPUTensorId, SparseCUDATensorId, VariableTensorId].<NewLine></code></pre><NewLine><p>So I’m interested whether it’s planned to support  <code>nn.Embeddings</code>  quantization?</p><NewLine></div>",https://discuss.pytorch.org/u/skurzhanskyi,(Alex Skurzhanskyi),skurzhanskyi,"July 14, 2020,  1:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think <a class=""mention"" href=""/u/zafar"">@Zafar</a> is working on this</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  5:01pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
89080,How can I incorporate PReLU in a quantized model?,2020-07-14T02:40:07.361Z,2,119,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone.<br/><NewLine>This is a followup question concerning <a href=""https://discuss.pytorch.org/t/runtimeerror-could-not-run-aten-native-batch-norm-with-arguments-from-the-quantizedcputensorid-backend-aten-native-batch-norm-is-only-available-for-these-backends-cputensorid-mkldnncputensorid-variabletensorid/88975"">this</a> . The issue is in the Resnet model that I’m dealing with, I cant replace <code>PReLU</code> with <code>ReLU</code> as it drastically affects the network performance.<br/><NewLine>So my question is, what are my options here? what should I be doing in this case?<br/><NewLine>Would doing sth like this suffice?</p><NewLine><pre><code class=""lang-python"">class PReLU_Quantized(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.weight = prelu_object.weight<NewLine>        self.quantized_op = nn.quantized.FloatFunctional()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        # inputs = torch.max(0, inputs) + self.weight * torch.min(0, inputs)    <NewLine>        self.weight = self.quant(self.weight)<NewLine>        weight_min_res = self.quantized_op.mul(self.weight, torch.min(inputs)[0])<NewLine>        inputs = self.quantized_op.add(torch.max(inputs)[0], weight_min_res).unsqueeze(0)<NewLine>        self.weight = self.dequant(self.weight)<NewLine>        return inputs<NewLine></code></pre><NewLine><p>and for the replacement :</p><NewLine><pre><code class=""lang-auto"">class model(nn.Module):<NewLine>     def __init__(self)<NewLine>         super().__init__()<NewLine>         .... <NewLine>        self.prelu = PReLU()<NewLine>        self.prelu_q = PReLU_Quantized(self.prelu)<NewLine>         ....<NewLine></code></pre><NewLine><p>Thanks a lot in advance</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"July 14, 2020,  3:43am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>for some reason, the error between the actual PReLU and my implementation is  very large!<br/><NewLine>here are sample diffs in different layers:</p><NewLine><pre><code class=""lang-python"">diff : 1.1562038660049438<NewLine>diff : 0.02868632599711418<NewLine>diff : 0.3653906583786011<NewLine>diff : 1.6100226640701294<NewLine>diff : 0.8999372720718384<NewLine>diff : 0.03773299604654312<NewLine>diff : -0.5090572834014893<NewLine>diff : 0.1654307246208191<NewLine>diff : 1.161868691444397<NewLine>diff : 0.026089997962117195<NewLine>diff : 0.4205571115016937<NewLine>diff : 1.5337920188903809<NewLine>diff : 0.8799554705619812<NewLine>diff : 0.03827812895178795<NewLine>diff : -0.40296515822410583<NewLine>diff : 0.15618863701820374<NewLine></code></pre><NewLine><p>and the diff is calculated like this in the forward pass:</p><NewLine><pre><code class=""lang-python"">    def forward(self, x):<NewLine>        residual = x<NewLine>        out = self.bn0(x)<NewLine>        out = self.conv1(out)<NewLine>        out = self.bn1(out)<NewLine>        out = self.prelu(out)<NewLine>        <NewLine>        out2 = self.prelu2(out)<NewLine>        print(f'diff : {( out - out2).mean().item()}')<NewLine><NewLine>        out = self.conv2(out)<NewLine></code></pre><NewLine><p>This is the normal implementation which I used on ordinary model (i.e. not quantized!)  to assess whether it produces correct result and then move on to quantized version:</p><NewLine><pre><code class=""lang-python"">class PReLU_2(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine><NewLine>    def forward(self, inputs):<NewLine>        x = self.weight<NewLine>        tmin, _ = torch.min(inputs,dim=0)<NewLine>        tmax, _ = torch.max(inputs,dim=0)<NewLine>        weight_min_res = torch.mul(x, tmin)<NewLine>        inputs = torch.add(tmax, weight_min_res)<NewLine>        inputs = inputs.unsqueeze(0)<NewLine>        return inputs<NewLine><NewLine></code></pre><NewLine><p>what am I missing here?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK, I figured it out! I made a huge mistake in the very begining. I needed to calculate</p><NewLine><pre><code class=""lang-auto"">PReLU(x)=max(0,x)+a∗min(0,x)<NewLine></code></pre><NewLine><p>or<br/><NewLine><img alt=""image"" data-base62-sha1=""nspDyO6leATlzwvsEWOc3Qp1g15"" height=""88"" src=""https://discuss.pytorch.org/uploads/default/original/3X/a/4/a4683d696a2a7178e866b53b58efa84054b22f4b.png"" width=""337""/><br/><NewLine>and not the actual min! or max! which doesnt make sense!<br/><NewLine>now, can anyone do me a favor and tell me how I can vectorize this ? I’m kind of lost at the moment!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks to dear God its done!<br/><NewLine>Here is the final solution!:</p><NewLine><pre><code class=""lang-python"">class PReLU_2(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine><NewLine>    def forward(self, inputs):<NewLine>        pos = torch.relu(inputs)<NewLine>        neg = -self.weight * torch.relu(-inputs)<NewLine>        inputs = pos + neg<NewLine>        return inputs<NewLine></code></pre><NewLine><p>and t his is the quantized version :</p><NewLine><pre><code class=""lang-python"">class PReLU_Quantized(nn.Module):<NewLine>    def __init__(self, prelu_object):<NewLine>        super().__init__()<NewLine>        self.prelu_weight = prelu_object.weight<NewLine>        self.weight = self.prelu_weight<NewLine>        self.quantized_op = nn.quantized.FloatFunctional()<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, inputs):<NewLine>        # inputs = max(0, inputs) + alpha * min(0, inputs) <NewLine>        self.weight = self.quant(self.weight)<NewLine>        weight_min_res = self.quantized_op.mul(-self.weight, torch.relu(-inputs))<NewLine>        inputs = self.quantized_op.add(torch.relu(inputs), weight_min_res)<NewLine>        inputs = self.dequant(inputs)<NewLine>        self.weight = self.dequant(self.weight)<NewLine>        return inputs<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: July 14, 2020,  8:22am; <NewLine> REPLY_DATE 2: July 14, 2020, 11:52am; <NewLine> REPLY_DATE 3: July 15, 2020,  2:02am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
88486,QuantStub/DeQuantStubs for QAT confusion,2020-07-09T03:57:31.577Z,0,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’m kind of confused by the use of <code>Quant/DeQuantStubs</code> for Quantization Aware Training.</p><NewLine><p>From my understanding only layers in between the Quant/DequantStubs are supposed to be quantised (is that correct?) but for my model when I place quantstubs around just the backbone:</p><NewLine><pre><code class=""lang-auto"">x = self.quant0(x)<NewLine>x = self.backbone0(x)<NewLine>x = self.dequant(x)<NewLine>confidence = self.classification_headers0(x)<NewLine></code></pre><NewLine><p>and I look at the layers in classification_headers before and after preparation:</p><NewLine><pre><code class=""lang-auto"">print(model.classification_headers0)<NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model, inplace=True)<NewLine>print(model.classification_headers0)<NewLine></code></pre><NewLine><p>I get</p><NewLine><pre><code class=""lang-auto"">Sequential(<NewLine>  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)<NewLine>  (1): ReLU()<NewLine>  (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))<NewLine>)<NewLine>Sequential(<NewLine>  (0): Conv2d(<NewLine>    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>    )<NewLine>    (weight_fake_quant): FakeQuantize(<NewLine>      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])<NewLine>      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>    )<NewLine>  )<NewLine>  (1): ReLU(<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>    )<NewLine>  )<NewLine>  (2): Conv2d(<NewLine>    64, 6, kernel_size=(1, 1), stride=(1, 1)<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>    )<NewLine>    (weight_fake_quant): FakeQuantize(<NewLine>      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            scale=tensor([1.]), zero_point=tensor([0])<NewLine>      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>    )<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>Why are the layers in classification_headers0 prepared for quantisation too?</p><NewLine></div>",https://discuss.pytorch.org/u/kekpirat,(Joel),kekpirat,"July 9, 2020,  3:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88486"" data-username=""kekpirat""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/kekpirat/40/26428_2.png"" width=""20""/> kekpirat:</div><NewLine><blockquote><NewLine><p>From my understanding only layers in between the Quant/DequantStubs are supposed to be quantised (is that correct?)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Which layers to quantize is controlled by the qconfig, when we do <code>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')</code> it applies the defaults settings of which modules to swap to the entire model.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>the <code>mapping</code> argument to <code>prepare_qat</code> (<a href=""https://github.com/pytorch/pytorch/blob/733b8c23c436d906125c20f0a64692bf57bce040/torch/quantization/quantize.py#L289"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/733b8c23c436d906125c20f0a64692bf57bce040/torch/quantization/quantize.py#L289</a>) can be used to customize which layer’s you’d like to quantize</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: July 13, 2020,  7:26am; <NewLine> REPLY_DATE 2: July 13, 2020,  7:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64213,Loading of Quantized Model,2019-12-16T12:25:28.112Z,1,588,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have quantized model and I want to load it in pytorch but I am not able to do it.<br/><NewLine>After quantisation the definition of model is changing as fusion of BatchNormalization layer is happening.<br/><NewLine>But when I am loading the model I have previous definition which does not contain fused layer but other layers are there like quant and dequant layer.</p><NewLine><p>Is there a way  to load quantized model in pytorch?</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"December 16, 2019, 12:25pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Mohit,<br/><NewLine>Can you provide more details/code? You can load/save quantized models by saving a state_dict().  When you perform fusion, make sure you set inplace=True.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> I have saved the model correctly but I want to use it in pytorch so we must know the definition of model then we can load the state_dict from the saved model file.<br/><NewLine>But what I have is definition of model without the fusion of layer and that’s where the definition of model changing and I can’t load model.</p><NewLine><p>So Do I need to change the model definition according to the fused layers?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the expectation is to have the original model and go through the whole eager mode quantization flow again, and then load from the saved state_dict.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi mohit7,<br/><NewLine>Make sure you create the net using previous definition, and let the net go through process that was applied during quantization before (prepare_model, fuse_model, and convert), without rerun the calibration process.<br/><NewLine>After that you can load the quantized state_dict in. Hope it helps.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Giang_Dang; <NewLine> ,"REPLY_DATE 1: December 16, 2019,  7:49pm; <NewLine> REPLY_DATE 2: December 17, 2019,  4:15am; <NewLine> REPLY_DATE 3: December 18, 2019, 12:16am; <NewLine> REPLY_DATE 4: July 9, 2020, 10:22am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
87531,The accuracy after int8 is higher than before quantization,2020-07-01T03:51:52.845Z,2,129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Modify the amount of calibration data, the model accuracy after int8 quantization is actually higher than the original model accuracy</p><NewLine><p>def get_imagenet(dataset_dir=’…/dataset/CIFAR10’, batch_size=32):</p><NewLine><pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                 std=[0.229, 0.224, 0.225])<NewLine><NewLine>train_transform = transforms.Compose([<NewLine>    # transforms.Resize(256),<NewLine>    transforms.RandomResizedCrop(32),<NewLine>    transforms.RandomHorizontalFlip(),<NewLine>    transforms.ToTensor(),<NewLine>    normalize,<NewLine>])<NewLine><NewLine>test_transform = transforms.Compose([<NewLine>    transforms.ToTensor(),<NewLine>    normalize,<NewLine>])<NewLine><NewLine>train_dataset = datasets.CIFAR10(root=dataset_dir, train=True, transform=train_transform, download=True)<NewLine>test_dataset = datasets.CIFAR10(root=dataset_dir, train=False, transform=test_transform, download=True)<NewLine><NewLine>trainloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS,<NewLine>                                          pin_memory=True, shuffle=False)<NewLine>testloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=NUM_WORKERS,<NewLine>                                         pin_memory=True, shuffle=False)<NewLine>return trainloader, testloader<NewLine></code></pre><NewLine><p>class quantizeModel(object):</p><NewLine><pre><code>def __init__(self):<NewLine>    super(quantizeModel, self).__init__()<NewLine>    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')<NewLine>    self.train_loader, self.test_loader = get_imagenet()<NewLine>    self.quant()<NewLine><NewLine>def quant(self):<NewLine><NewLine>    model = self.load_model()<NewLine>    model.eval()<NewLine>    self.print_size_of_model(model)<NewLine>    self.validate(model, ""original_resnet18"", self.test_loader)<NewLine>    model.fuse_model()<NewLine><NewLine>    self.print_size_of_model(model)<NewLine>    self.quantize(model)<NewLine><NewLine>def load_model(self):<NewLine>    model = resnet18()<NewLine>    state_dict = torch.load(""CIFAR10_resnet18.pth"", map_location=self.device)<NewLine>    model.load_state_dict(state_dict)<NewLine>    model.to(self.device)<NewLine>    return model<NewLine><NewLine>def print_size_of_model(self, model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>def validate(self, model, name, data_loader):<NewLine>    with torch.no_grad():<NewLine>        correct = 0<NewLine>        total = 0<NewLine>        acc = 0<NewLine>        for data in data_loader:<NewLine>            images, labels = data<NewLine>            images, labels = images.to(self.device), labels.to(self.device)<NewLine>            output = model(images)<NewLine><NewLine>            _, predicted = torch.max(output, dim=1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine><NewLine>            if total == 1024:  ＃calibration data<NewLine>                break<NewLine><NewLine>        acc = round(100 * correct / total, 3)<NewLine>        print('{{""metric"": ""{}_val_accuracy"", ""value"": {}%}}'.format(name, acc))<NewLine>        return acc<NewLine><NewLine>def quantize(self, model):<NewLine>    #model.qconfig = torch.quantization.default_qconfig<NewLine>    #model.qconfig = torch.quantization.default_per_channel_qconfig<NewLine><NewLine>    model.qconfig = torch.quantization.QConfig(<NewLine>        activation=torch.quantization.observer.MinMaxObserver.with_args(reduce_range=True),<NewLine>        weight=torch.quantization.observer.PerChannelMinMaxObserver.with_args(dtype=torch.qint8,<NewLine>                                                                              qscheme=torch.per_channel_affine))<NewLine>    pmodel = torch.quantization.prepare(model)<NewLine><NewLine>    #calibration<NewLine>    self.validate(pmodel, ""quntize_per_channel_resent18_train"", self.train_loader)<NewLine>    qmodel = torch.quantization.convert(pmodel)<NewLine><NewLine>    self.validate(qmodel, ""quntize_per_chaannel_resent18_test"", self.test_loader)<NewLine>    self.print_size_of_model(qmodel)<NewLine><NewLine>    torch.jit.save(torch.jit.script(qmodel), ""quantization_per_channel_model18.pth"")<NewLine></code></pre><NewLine><p>Original model accuracy：<strong>71.76%</strong></p><NewLine><p>First quantification：batch_size:32    calibration data: <strong>2048</strong><br/><NewLine>Quantified model accuracy：<strong>71.51%</strong></p><NewLine><p>Second quantification：batch_size:32    calibration data: <strong>1024</strong><br/><NewLine>Quantified model accuracy：<strong>71.85%</strong></p><NewLine><p>Why the accuracy becomes higher after quantization?</p><NewLine><p>In addition, I think that the total number of calibration data remains unchanged, the maximum and minimum range of activation should be fixed, and the quantization accuracy should also be fixed. However, it is found that the total number of calibration data remains unchanged, and if batch_size is modified, the accuracy after quantization will change. What is the reason?</p><NewLine></div>",https://discuss.pytorch.org/u/blueskywwc,(Blueskywwc),blueskywwc,"July 1, 2020,  3:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""87531"" data-username=""blueskywwc""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/ecd19e/40.png"" width=""20""/> blueskywwc:</div><NewLine><blockquote><NewLine><p>However, it is found that the total number of calibration data remains unchanged, and if batch_size is modified, the accuracy after quantization will change. What is the reason?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Is there any randomness in which specific dataset slice is getting used for calibration?  Can you reproduce the accuracy changes if you set <code>torch.manual_seed(0)</code>?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>torch.manual_seed(191009)</p><NewLine><p>train_dataset = datasets.CIFAR10(root=dataset_dir, train=True, transform=train_transform, download=True)<br/><NewLine>test_dataset = datasets.CIFAR10(root=dataset_dir, train=False, transform=test_transform, download=True)</p><NewLine><pre><code>trainloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS,<NewLine>                                          pin_memory=True, shuffle=False)<NewLine>testloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=NUM_WORKERS,<NewLine>                                         pin_memory=True, shuffle=False)<NewLine></code></pre><NewLine><p>if batch_size is modified, the accuracy after quantization will change，no modification, the accuracy rate will not change.<br/><NewLine>Why the accuracy becomes higher after quantization?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87531"" data-username=""blueskywwc""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/ecd19e/40.png"" width=""20""/> blueskywwc:</div><NewLine><blockquote><NewLine><p>Why the accuracy becomes higher after quantization?</p><NewLine></blockquote><NewLine></aside><NewLine><p>We don’t expect accuracy to increase due to quantization, this is likely random variation. To test this theory, you could run evaluation on various slices of data unseen in training. I would expect the mean difference of accuracy on a set of slices would be a slight drop for the quantized model compared to the floating point model.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""87531"" data-username=""blueskywwc""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/b/ecd19e/40.png"" width=""20""/> blueskywwc:</div><NewLine><blockquote><NewLine><p>if batch_size is modified, the accuracy after quantization will change，no modification, the accuracy rate will not change.</p><NewLine></blockquote><NewLine></aside><NewLine><p>If MinMax observers are used, we do not expect the ordering or batch size of the calibration data to matter, as long as the same dataset gets seen via calibration. One thing to look into would be whether the way the evaluation score is measured depends on batch size, and if you are feeding exactly the same set of images through.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your reply</p><NewLine><p>model.qconfig = torch.quantization.QConfig(<br/><NewLine>activation=torch.quantization.observer.MinMaxObserver.with_args(dtype=torch.quint8,<br/><NewLine>qscheme=torch.per_channel_affine,<br/><NewLine>reduce_range=True),<br/><NewLine>weight=torch.quantization.observer.PerChannelMinMaxObserver.with_args(dtype=torch.qint8,<br/><NewLine>qscheme=torch.per_channel_affine,<br/><NewLine>reduce_range=False))</p><NewLine><p>The total number of calibration data sets remains unchanged, and different batch_sizes (8, 16, 32, 64) are tested, and the quantized accuracy rate will still slightly fluctuate.<br/><NewLine>batch_size has little effect on the quantization result, I can choose a group with the highest accuracy as the final quantization result.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/blueskywwc; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/blueskywwc; <NewLine> ,"REPLY_DATE 1: July 6, 2020,  9:44pm; <NewLine> REPLY_DATE 2: July 7, 2020,  3:29am; <NewLine> REPLY_DATE 3: July 7, 2020,  4:03pm; <NewLine> REPLY_DATE 4: July 7, 2020,  4:05pm; <NewLine> REPLY_DATE 5: July 9, 2020,  3:34am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
88190,Quantization.convert after QAT pickling issue,2020-07-07T06:43:57.585Z,1,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>I’ve been having an issue with <code>torch.quantization.convert</code> after performing QAT -</p><NewLine><p>I modified the <a href=""https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB"" rel=""nofollow noopener"">model (face detector)</a> to do QAT by adding the lines</p><NewLine><pre><code class=""lang-auto"">    net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>    torch.quantization.prepare_qat(net, inplace=True)<NewLine></code></pre><NewLine><p>in <code>train.py</code> and the QuantStub/DeStub in the forward() of Mb_Tiny_RFB() (<code>vision/nn/mb_tiny_rfb.py</code>) following <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">this tutorial</a>, and then saved the model via  <code>torch.save(self.state_dict(), path)</code></p><NewLine><details><NewLine><summary> train.py </summary><NewLine><pre><code class=""lang-auto"">DEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() and args.use_cuda else ""cpu"")<NewLine><NewLine>if __name__ == '__main__':<NewLine>    timer = Timer()<NewLine>    create_net = create_Mb_Tiny_RFB_fd<NewLine><NewLine>    train_transform = TrainAugmentation(config.image_size, config.image_mean, config.image_std)<NewLine>    target_transform = MatchPrior(config.priors, config.center_variance,<NewLine>                                  config.size_variance, args.overlap_threshold)<NewLine><NewLine>    test_transform = TestTransform(config.image_size, config.image_mean_test, config.image_std)<NewLine><NewLine>    datasets = []<NewLine>    for dataset_path in args.datasets:<NewLine>        if args.dataset_type == 'voc':<NewLine>            dataset = VOCDataset(dataset_path, transform=train_transform,<NewLine>                                 target_transform=target_transform, img_size = config.image_size)<NewLine>            label_file = os.path.join(args.checkpoint_folder, ""voc-model-labels.txt"")<NewLine>            store_labels(label_file, dataset.class_names)<NewLine>            num_classes = len(dataset.class_names)<NewLine>        else:<NewLine>            raise ValueError(f""Dataset type {args.dataset_type} is not supported."")<NewLine>        datasets.append(dataset)<NewLine>    train_dataset = ConcatDataset(datasets)<NewLine><NewLine>    train_loader = DataLoader(train_dataset, args.batch_size,<NewLine>                              num_workers=args.num_workers,<NewLine>                              shuffle=True, pin_memory=True)<NewLine>    val_dataset = VOCDataset(args.validation_dataset, transform=test_transform,<NewLine>                                 target_transform=target_transform, is_test=True)<NewLine>    val_loader = DataLoader(val_dataset, args.batch_size,<NewLine>                            num_workers=args.num_workers,<NewLine>                            shuffle=False)<NewLine><NewLine>    net = create_net(num_classes)<NewLine><NewLine>    min_loss = -10000.0<NewLine>    last_epoch = -1<NewLine><NewLine>    base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr<NewLine>    extra_layers_lr = args.extra_layers_lr if args.extra_layers_lr is not None else args.lr<NewLine> <NewLine>    params = [<NewLine>            {'params': net.base_net.parameters(), 'lr': base_net_lr},<NewLine>            {'params': itertools.chain(<NewLine>                net.source_layer_add_ons.parameters(),<NewLine>                net.extras.parameters()<NewLine>            ), 'lr': extra_layers_lr},<NewLine>            {'params': itertools.chain(<NewLine>                net.regression_headers.parameters(),<NewLine>                net.classification_headers.parameters()<NewLine>            )}<NewLine>        ]<NewLine><NewLine>    if args.resume:<NewLine>        logging.info(f""Resume from the model {args.resume}"")<NewLine>        net.load(args.resume)<NewLine><NewLine>    criterion = MultiboxLoss(config.priors, neg_pos_ratio=3,<NewLine>                             center_variance=0.1, size_variance=0.2, device=DEVICE)<NewLine>    optimizer = torch.optim.SGD(params, lr=args.lr, momentum=args.momentum,<NewLine>                                    weight_decay=args.weight_decay)<NewLine>    ...<NewLine>    net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>    torch.quantization.prepare_qat(net, inplace=True)<NewLine><NewLine>    net.to(DEVICE)<NewLine><NewLine>    for epoch in range(last_epoch + 1, args.num_epochs):<NewLine>        train(train_loader, net, criterion, optimizer,<NewLine>              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)<NewLine>        if epoch &gt; 3:<NewLine>            # Freeze quantizer parameters<NewLine>            net.apply(torch.quantization.disable_observer)<NewLine>        if epoch &gt; 2:<NewLine>            # Freeze batch norm mean and variance estimates towards the end of training to better match inference numerics.<NewLine>            net.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:<NewLine>            logging.info(""lr rate :{}"".format(optimizer.param_groups[0]['lr']))<NewLine>            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)<NewLine><NewLine>            net.eval()<NewLine>            quant_model = torch.quantization.convert(net.cpu(), inplace=False) # &lt;-- error happens here<NewLine><NewLine>            model_path = os.path.join(args.checkpoint_folder, f""{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth"")<NewLine>            net.save(model_path)<NewLine></code></pre><NewLine></details><NewLine><p>When I tried to call <code>quantization.convert()</code> before saving, I got the error:</p><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""train.py"", line 432, in &lt;module&gt;<NewLine>    quant_model = torch.quantization.convert(net.module.eval().cpu(), inplace=False)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/site-packages/torch/quantization/quantize.py"", line 299, in convert<NewLine>    module = copy.deepcopy(module)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/copy.py"", line 172, in deepcopy<NewLine>    y = _reconstruct(x, memo, *rv)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/copy.py"", line 270, in _reconstruct<NewLine>    state = deepcopy(state, memo)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/copy.py"", line 146, in deepcopy<NewLine>    y = copier(x, memo)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/copy.py"", line 230, in _deepcopy_dict<NewLine>    y[deepcopy(key, memo)] = deepcopy(value, memo)<NewLine>  File ""/home/user/anaconda3/envs/FaceDetector/lib/python3.8/copy.py"", line 161, in deepcopy<NewLine>    rv = reductor(4)<NewLine>TypeError: cannot pickle 'module' object<NewLine></code></pre><NewLine><p>So instead I tried to load the QAT’d parameters into a less confusing form of the model and then tried converting again, but got the same error:</p><NewLine><pre><code class=""lang-auto"">import torchvision<NewLine>from torch import nn<NewLine>from vision.utils import box_utils<NewLine>from vision.ssd.config.fd_config import define_img_size<NewLine>define_img_size(640)<NewLine>from vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd<NewLine>import torch.nn.functional as F<NewLine>import cv2<NewLine>import numpy as np<NewLine><NewLine>class_names = ['background', 'face']<NewLine>net_1 = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device='cpu')<NewLine>net_1.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(net_1, inplace=True)<NewLine><NewLine># load definition: self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))<NewLine>net_1.load(model_path) # load the previously QAT'd model (without quantisation conversion)<NewLine><NewLine>class SimpleNet(nn.Module):<NewLine>    def __init__(self, base_net, regression_headers, classification_headers, extras, priors, config):<NewLine>        super(SimpleNet, self).__init__()<NewLine>        self.backbone0 = base_net[:8]<NewLine>        self.backbone1 = base_net[8:11]<NewLine>        self.backbone2 = base_net[11:13]<NewLine>        self.last_chunk = base_net[13:] <NewLine>        self.regression_headers0 = regression_headers[0]<NewLine>        self.regression_headers1 = regression_headers[1]<NewLine>        self.regression_headers2 = regression_headers[2]<NewLine>        self.regression_headers3 = regression_headers[3]<NewLine>        self.classification_headers0 = classification_headers[0]<NewLine>        self.classification_headers1 = classification_headers[1]<NewLine>        self.classification_headers2 = classification_headers[2]<NewLine>        self.classification_headers3 = classification_headers[3]<NewLine>        self.extras = extras<NewLine>        self.num_classes = 2<NewLine>        self.priors = priors<NewLine>        self.config = config<NewLine>        self.last_op = nn.Softmax(dim=-1)<NewLine><NewLine>    def forward(self, x):<NewLine>        confidences = []<NewLine>        locations = []<NewLine>        x = self.backbone0(x)<NewLine>        confidence = self.classification_headers0(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers0(x)<NewLine>        location = location.permute(0, 2, 3, 1).contiguous()<NewLine>        location = location.view(location.size(0), -1, 4)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        x = self.backbone1(x)<NewLine>        confidence = self.classification_headers1(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers1(x)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        x = self.backbone2(x)<NewLine>        confidence = self.classification_headers2(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers2(x)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        x = self.last_chunk.forward(x)<NewLine><NewLine>        x = self.extras(x)<NewLine>        confidence = self.classification_headers3(x)<NewLine>        confidence = confidence.permute(0, 2, 3, 1).contiguous()<NewLine>        confidence = confidence.view(confidence.size(0), -1, self.num_classes)<NewLine>        location = self.regression_headers3(x)<NewLine>        confidences.append(confidence)<NewLine>        locations.append(location)<NewLine><NewLine>        confidences = torch.cat(confidences, 1)<NewLine>        confidences = self.last_op(confidences)<NewLine>        locations = torch.cat(locations, 1)<NewLine><NewLine>        boxes = box_utils.convert_locations_to_boxes(<NewLine>            locations, self.priors, torch.tensor([0.1]), torch.tensor([0.2]) #self.config.center_variance, self.config.size_variance<NewLine>        )<NewLine>        boxes = box_utils.center_form_to_corner_form(boxes)<NewLine>        return confidences, boxes<NewLine><NewLine><NewLine>model = SimpleNet(<NewLine>        net_1.base_net,<NewLine>        net_1.regression_headers,<NewLine>        net_1.classification_headers,<NewLine>        net_1.extras[0],<NewLine>        net_1.priors,<NewLine>        net_1.config)<NewLine><NewLine>model.eval()<NewLine>model = torch.quantization.convert(model, inplace=False) # error here<NewLine></code></pre><NewLine><details><NewLine><summary>Heres the `print(model)` output:</summary><NewLine><pre><code class=""lang-auto"">SimpleNet(<NewLine>  (backbone0): Sequential(<NewLine>    (0): Sequential(<NewLine>      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>    )<NewLine>    (1): Sequential(<NewLine>      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)<NewLine>      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (2): Sequential(<NewLine>      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (3): Sequential(<NewLine>      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (4): Sequential(<NewLine>      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (5): Sequential(<NewLine>      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)<NewLine>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (6): Sequential(<NewLine>      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)<NewLine>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (7): BasicRFB(<NewLine>      (branch0): Sequential(<NewLine>        (0): BasicConv(<NewLine>          (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>        (1): BasicConv(<NewLine>          (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>          (activation_fn): ReLU()<NewLine>        )<NewLine>        (2): BasicConv(<NewLine>          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (branch1): Sequential(<NewLine>        (0): BasicConv(<NewLine>          (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>        (1): BasicConv(<NewLine>          (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>          (activation_fn): ReLU()<NewLine>        )<NewLine>        (2): BasicConv(<NewLine>          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (branch2): Sequential(<NewLine>        (0): BasicConv(<NewLine>          (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>        (1): BasicConv(<NewLine>          (conv): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(12, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>          (activation_fn): ReLU()<NewLine>        )<NewLine>        (2): BasicConv(<NewLine>          (conv): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>          (activation_fn): ReLU()<NewLine>        )<NewLine>        (3): BasicConv(<NewLine>          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False)<NewLine>          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>        )<NewLine>      )<NewLine>      (ConvLinear): BasicConv(<NewLine>        (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>      )<NewLine>      (shortcut): BasicConv(<NewLine>        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)<NewLine>      )<NewLine>      (activation_fn): ReLU()<NewLine>    )<NewLine>  )<NewLine>  (backbone1): Sequential(<NewLine>    (8): Sequential(<NewLine>      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)<NewLine>      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (9): Sequential(<NewLine>      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)<NewLine>      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (10): Sequential(<NewLine>      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)<NewLine>      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>  )<NewLine>  (backbone2): Sequential(<NewLine>    (11): Sequential(<NewLine>      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)<NewLine>      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>    (12): Sequential(<NewLine>      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)<NewLine>      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (5): ReLU()<NewLine>    )<NewLine>  )<NewLine>  (last_chunk): Sequential()<NewLine>  (regression_headers0): Sequential(<NewLine>    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (regression_headers1): Sequential(<NewLine>    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (regression_headers2): Sequential(<NewLine>    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (regression_headers3): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<NewLine>  (classification_headers0): Sequential(<NewLine>    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (classification_headers1): Sequential(<NewLine>    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (classification_headers2): Sequential(<NewLine>    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)<NewLine>    (1): ReLU()<NewLine>    (2): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (classification_headers3): Conv2d(256, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<NewLine>  (extras): Sequential(<NewLine>    (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))<NewLine>    (1): ReLU()<NewLine>    (2): Sequential(<NewLine>      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)<NewLine>      (1): ReLU()<NewLine>      (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))<NewLine>    )<NewLine>    (3): ReLU()<NewLine>  )<NewLine>  (last_op): Softmax(dim=-1)<NewLine>)<NewLine><NewLine></code></pre><NewLine></details><NewLine><p>It appeared that something could not be pickled in the model - so I tried using <code>dill</code>:</p><NewLine><pre><code class=""lang-auto"">dill.detect.trace(True)<NewLine>dill.detect.errors(model)<NewLine></code></pre><NewLine><p>and the output is</p><NewLine><details><NewLine><summary>Output</summary><NewLine><pre><code class=""lang-auto"">T2: &lt;class '__main__.SimpleNet'&gt;<NewLine>F2: &lt;function _create_type at 0x7f574cb6a670&gt;<NewLine># F2<NewLine>T1: &lt;class 'type'&gt;<NewLine>F2: &lt;function _load_type at 0x7f574cb6a5e0&gt;<NewLine># F2<NewLine># T1<NewLine>T4: &lt;class 'torch.nn.modules.module.Module'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f574c9f4b40&gt;<NewLine>F1: &lt;function SimpleNet.__init__ at 0x7f574cb86f70&gt;<NewLine>F2: &lt;function _create_function at 0x7f574cb6a700&gt;<NewLine># F2<NewLine>Co: &lt;code object __init__ at 0x7f57af0eb240, file ""simple_ul.py"", line 38&gt;<NewLine>F2: &lt;function _create_code at 0x7f574cb6a790&gt;<NewLine># F2<NewLine># Co<NewLine>D1: &lt;dict object at 0x7f57af1d7f00&gt;<NewLine># D1<NewLine>Ce: &lt;cell at 0x7f574cb5c790: type object at 0x5637763f3840&gt;<NewLine>F2: &lt;function _create_cell at 0x7f574cb6ab80&gt;<NewLine># F2<NewLine>T5: &lt;class '__main__.SimpleNet'&gt;<NewLine># T5<NewLine># Ce<NewLine>D2: &lt;dict object at 0x7f574c9f48c0&gt;<NewLine># D2<NewLine># F1<NewLine>F1: &lt;function SimpleNet.forward at 0x7f574cb86ee0&gt;<NewLine>Co: &lt;code object forward at 0x7f57af0f9450, file ""simple_ul.py"", line 60&gt;<NewLine># Co<NewLine>D1: &lt;dict object at 0x7f57af1d7f00&gt;<NewLine># D1<NewLine>D2: &lt;dict object at 0x7f574c9f4a00&gt;<NewLine># D2<NewLine># F1<NewLine># D2<NewLine># T2<NewLine>D2: &lt;dict object at 0x7f574c9fa740&gt;<NewLine>T4: &lt;class 'collections.OrderedDict'&gt;<NewLine># T4<NewLine>T4: &lt;class 'torch.nn.modules.container.Sequential'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f574c9fa4c0&gt;<NewLine>D2: &lt;dict object at 0x7f574d8c7580&gt;<NewLine>T4: &lt;class 'torch.nn.qat.modules.conv.Conv2d'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f574ca8db40&gt;<NewLine>F2: &lt;function _rebuild_parameter at 0x7f57ad6594c0&gt;<NewLine># F2<NewLine>F2: &lt;function _rebuild_tensor_v2 at 0x7f57ad659280&gt;<NewLine># F2<NewLine>/home/user/anaconda3/envs/FaceDetector/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead<NewLine>  warnings.warn(""pickle support for Storage will be removed in 1.5. Use `torch.save` instead"", FutureWarning)<NewLine>F2: &lt;function _load_from_bytes at 0x7f5759b7b310&gt;<NewLine># F2<NewLine>T4: &lt;class 'torch.quantization.fake_quantize.FakeQuantize'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f5759343500&gt;<NewLine>T4: &lt;class 'torch.quantization.observer.MovingAverageMinMaxObserver'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f574d8b7ac0&gt;<NewLine># D2<NewLine># D2<NewLine>D2: &lt;dict object at 0x7f574ca8de00&gt;<NewLine>T4: &lt;class 'torch.quantization.observer.MovingAveragePerChannelMinMaxObserver'&gt;<NewLine># T4<NewLine>D2: &lt;dict object at 0x7f574ca8de40&gt;<NewLine># D2<NewLine># D2<NewLine>T6: &lt;class 'torch.quantization.qconfig.QConfig'&gt;<NewLine>F2: &lt;function _create_namedtuple at 0x7f574cb6f0d0&gt;<NewLine># F2<NewLine># T6<NewLine>T4: &lt;class 'torch.quantization.observer._with_args.&lt;locals&gt;._PartialWrapper'&gt;<NewLine></code></pre><NewLine></details><NewLine>but I don't really understand the output and how to solve this problem... Am I doing QAT -&gt; quantisation correctly? Is it correct to again set the qconfig and to prepare_qat before loading a QAT'd model? Any help would be greatly appreciated.<NewLine>        </div>",https://discuss.pytorch.org/u/kekpirat,(Joel),kekpirat,"July 7, 2020,  8:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""88190"" data-username=""kekpirat""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/k/ecccb3/40.png"" width=""20""/> kekpirat:</div><NewLine><blockquote><NewLine><p><code>cannot pickle 'module' object</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>looks like it’s failing to <code>copy.deepcopy(module)</code>.  Just to confirm, does <code>copy.deepcopy</code> work on your model instance before you do QAT?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, yea I realised that deepcopy did not work on my original model either, and found the issue - I had some unpicklable objects saved in the init of my model</p><NewLine><p>Thank you!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kekpirat; <NewLine> ,"REPLY_DATE 1: July 7, 2020,  4:14pm; <NewLine> REPLY_DATE 2: July 8, 2020,  3:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66106,Quantization awareness training multi-gpu suport?,2020-01-08T14:47:59.411Z,13,676,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Does pytorch support multi-GPU in quantization awareness training?<br/><NewLine>In this script <a href=""https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py#L73"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py#L73</a>, it seems that it has the logic of multi-GPU.</p><NewLine></div>",https://discuss.pytorch.org/u/robotcator123,(robotcator),robotcator123,"January 8, 2020,  2:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/robotcator123"">@robotcator123</a>,<br/><NewLine>Multi gpu training is orthogonal to quantization aware training. Code written with Pytorch’s quantization aware training modules will work whether you are using a single gpu or using Data parallel on multiple gpus. Hope this helps!</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/mazhar_shaikh"">@Mazhar_Shaikh</a>，</p><NewLine><p>Actually, I train the mobilenet using this command in cluster.<br/><NewLine><code>python -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --data-path=./imagenet_1k</code> , it seems that the code works fine.</p><NewLine><p>But I change the training script to<br/><NewLine><code>python -m torch.distributed.launch --nproc_per_node=8 --use_env train_quantization.py --data-path=./imagenet_1k</code></p><NewLine><p>it will raise error like this after print lost of unknown data:<br/><NewLine>`Namespace(backend=‘qnnpack’, batch_size=32, cache_dataset=False, data_path=’~/test/imagenet_1k’, device=‘cuda’, dist_backend=‘nccl’, dist_url=‘env://’, distributed=True, epochs=90, eval_batch_size=128, gpu=0, lr=0.0001, lr_gamma=0.1, lr_step_size=30, model=‘mobilenet_v2’, momentum=0.9, num_batch_norm_update_epochs=3, num_calibration_batches=32, num_observer_update_epochs=4, output_dir=’.’, post_training_quantize=False, print_freq=10, rank=0, resume=’’, start_epoch=0, test_only=False, weight_decay=0.0001, workers=16, world_size=8)<br/><NewLine>Loading data<br/><NewLine>Loading data<br/><NewLine>Loading training data<br/><NewLine>Took 0.27007627487182617<br/><NewLine>Loading validation data<br/><NewLine>Creating data loaders<br/><NewLine>Creating model mobilenet_v2<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “train_quantization.py”, line 258, in <br/><NewLine>main(args)<br/><NewLine>File “train_quantization.py”, line 77, in main<br/><NewLine>model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])<br/><NewLine>File “xxx/.conda/envs/pytorch1.3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py”, line 298, in <strong>init</strong><br/><NewLine>self.broadcast_bucket_size)<br/><NewLine>File “xxx/.conda/envs/pytorch1.3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py”, line 480, in _distributed_broadcast_coalesced<br/><NewLine>dist._broadcast_coalesced(self.process_group, tensors, buffer_size)<br/><NewLine>TypeError: _broadcast_coalesced(): incompatible function arguments. The following argument types are supported:<br/><NewLine>1. (process_group: torch.distributed.ProcessGroup, tensors: List[at::Tensor], buffer_size: int) -&gt; None</p><NewLine><p>Invoked with: &lt;torch.distributed.ProcessGroupNCCL object at 0x7f943f78dd18&gt;, [tensor([[[[ 1.3185e-02, -4.3213e-03,  1.4823e-02],<br/><NewLine>…<br/><NewLine>…<br/><NewLine>…<br/><NewLine>subprocess.CalledProcessError: Command ‘[’/xxxx/pytorch1.3/bin/python’, ‘-u’, ‘train_quantization.py’, ‘–data-path=./imagenet_1k’]’ returned non-zero exit status 1.`<br/><NewLine>sorry for hiding some personal information.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that broadcast can not support None tensor, is there anybody know this problem?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/robotcator123"">@robotcator123</a>, If you believe broadcast of None doesn’t work as expected, please open an issue against PyTorch with a minimal reproducible example.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>, Thank you for your response. I can give some reproduction steps. For more detailed producible example, maybe I will do it at weekends.</p><NewLine><p>1: Download the imagenet1k dataset.<br/><NewLine>2: pip install torchvision==0.5.0, this will upgrade the torch into 1.4.0.<br/><NewLine>3: Use the script with the commands<br/><NewLine><code>python -m torch.distributed.launch --nproc_per_node=8 --use_env train_quant.py --data-path=./imagenet_1k </code></p><NewLine><p>The train_quant.py script is borrowed from torchvision reference code.</p><NewLine><details><NewLine><summary><NewLine>Summary</summary><NewLine><pre><code class=""lang-auto"">from __future__ import print_function<NewLine>import datetime<NewLine>import os<NewLine>import time<NewLine>import sys<NewLine>import copy<NewLine><NewLine>import torch<NewLine>import torch.utils.data<NewLine>from torch import nn<NewLine>import torchvision<NewLine>import torch.quantization<NewLine>import train_utils as utils<NewLine>from train import train_one_epoch, evaluate, load_data<NewLine><NewLine>def main(args):<NewLine>    if args.output_dir:<NewLine>        utils.mkdir(args.output_dir)<NewLine><NewLine>    utils.init_distributed_mode(args)<NewLine><NewLine>    print(args)<NewLine><NewLine>    if args.post_training_quantize and args.distributed:<NewLine>        raise RuntimeError(""Post training quantization example should not be performed ""<NewLine>                           ""on distributed mode"")<NewLine><NewLine>    # Set backend engine to ensure that quantized model runs on the correct kernels<NewLine>    if args.backend not in torch.backends.quantized.supported_engines:<NewLine>        raise RuntimeError(""Quantized backend not supported: "" + str(args.backend))<NewLine>    torch.backends.quantized.engine = args.backend<NewLine><NewLine>    device = torch.device(args.device)<NewLine>    torch.backends.cudnn.benchmark = True<NewLine><NewLine>    # Data loading code<NewLine>    print(""Loading data"")<NewLine>    train_dir = os.path.join(args.data_path, 'train')<NewLine>    val_dir = os.path.join(args.data_path, 'val')<NewLine><NewLine>    dataset, dataset_test, train_sampler, test_sampler = load_data(train_dir, val_dir,<NewLine>                                                                   args.cache_dataset, args.distributed)<NewLine>    data_loader = torch.utils.data.DataLoader(<NewLine>        dataset, batch_size=args.batch_size,<NewLine>        sampler=train_sampler, num_workers=args.workers, pin_memory=True)<NewLine><NewLine>    data_loader_test = torch.utils.data.DataLoader(<NewLine>        dataset_test, batch_size=args.eval_batch_size,<NewLine>        sampler=test_sampler, num_workers=args.workers, pin_memory=True)<NewLine><NewLine>    print(""Creating model"", args.model)<NewLine>    # when training quantized models, we always start from a pre-trained fp32 reference model<NewLine>    model = torchvision.models.quantization.__dict__[args.model](pretrained=True, quantize=args.test_only)<NewLine>    model.to(device)<NewLine><NewLine>    if not (args.test_only or args.post_training_quantize):<NewLine>        model.fuse_model()<NewLine>        model.qconfig = torch.quantization.get_default_qat_qconfig(args.backend)<NewLine>        torch.quantization.prepare_qat(model, inplace=True)<NewLine><NewLine>        optimizer = torch.optim.SGD(<NewLine>            model.parameters(), lr=args.lr, momentum=args.momentum,<NewLine>            weight_decay=args.weight_decay)<NewLine><NewLine>        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,<NewLine>                                                       step_size=args.lr_step_size,<NewLine>                                                       gamma=args.lr_gamma)<NewLine><NewLine>    criterion = nn.CrossEntropyLoss()<NewLine>    model_without_ddp = model<NewLine>    if args.distributed:<NewLine>        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])<NewLine>        model_without_ddp = model.module<NewLine>        print (model.module)<NewLine><NewLine>    model.apply(torch.quantization.enable_observer)<NewLine>    model.apply(torch.quantization.enable_fake_quant)<NewLine>    start_time = time.time()<NewLine>    for epoch in range(args.start_epoch, args.epochs):<NewLine>        if args.distributed:<NewLine>            train_sampler.set_epoch(epoch)<NewLine>        print('Starting training for epoch', epoch)<NewLine>        train_one_epoch(model, criterion, optimizer, data_loader, device, epoch,<NewLine>                        args.print_freq)<NewLine>        lr_scheduler.step()<NewLine>        with torch.no_grad():<NewLine>            if epoch &gt;= args.num_observer_update_epochs:<NewLine>                print('Disabling observer for subseq epochs, epoch = ', epoch)<NewLine>                model.apply(torch.quantization.disable_observer)<NewLine>            if epoch &gt;= args.num_batch_norm_update_epochs:<NewLine>                print('Freezing BN for subseq epochs, epoch = ', epoch)<NewLine>                model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine>            print('Evaluate QAT model')<NewLine><NewLine>            evaluate(model, criterion, data_loader_test, device=device)<NewLine>            quantized_eval_model = copy.deepcopy(model)<NewLine>            quantized_eval_model.eval()<NewLine>            quantized_eval_model.to(torch.device('cpu'))<NewLine>            torch.quantization.convert(quantized_eval_model, inplace=True)<NewLine><NewLine>            print('Evaluate Quantized model')<NewLine>            evaluate(quantized_eval_model, criterion, data_loader_test,<NewLine>                     device=torch.device('cpu'))<NewLine><NewLine>        model.train()<NewLine><NewLine>        print('Saving models after epoch ', epoch)<NewLine><NewLine>    total_time = time.time() - start_time<NewLine>    total_time_str = str(datetime.timedelta(seconds=int(total_time)))<NewLine>    print('Training time {}'.format(total_time_str))<NewLine><NewLine>def parse_args():                                                                                                                                                                                   <NewLine>    import argparse<NewLine>    parser = argparse.ArgumentParser(description='PyTorch Classification Training')<NewLine><NewLine>    parser.add_argument('--data-path',<NewLine>                        default='/datasets01/imagenet_full_size/061417/',<NewLine>                        help='dataset')<NewLine>    parser.add_argument('--model',<NewLine>                        default='mobilenet_v2',<NewLine>                        help='model')<NewLine>    parser.add_argument('--backend',<NewLine>                        default='qnnpack',<NewLine>                        help='fbgemm or qnnpack')<NewLine>    parser.add_argument('--device',<NewLine>                        default='cuda',<NewLine>                        help='device')<NewLine><NewLine>    parser.add_argument('-b', '--batch-size', default=32, type=int,<NewLine>                        help='batch size for calibration/training')<NewLine>    parser.add_argument('--eval-batch-size', default=128, type=int,<NewLine>                        help='batch size for evaluation')<NewLine>    parser.add_argument('--epochs', default=90, type=int, metavar='N',<NewLine>                        help='number of total epochs to run')<NewLine>    parser.add_argument('--num-observer-update-epochs',<NewLine>                        default=4, type=int, metavar='N',<NewLine>                        help='number of total epochs to update observers')<NewLine>    parser.add_argument('--num-batch-norm-update-epochs', default=3,<NewLine>                        type=int, metavar='N',<NewLine>                        help='number of total epochs to update batch norm stats')<NewLine>    parser.add_argument('--num-calibration-batches',<NewLine>                        default=32, type=int, metavar='N',<NewLine>                        help='number of batches of training set for \<NewLine>                              observer calibration ')<NewLine><NewLine>    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',<NewLine>                        help='number of data loading workers (default: 16)')<NewLine>    parser.add_argument('--lr',<NewLine>                        default=0.0001, type=float,<NewLine>                        help='initial learning rate')<NewLine>    parser.add_argument('--momentum',<NewLine>                        default=0.9, type=float, metavar='M',<NewLine>                        help='momentum')<NewLine>    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',<NewLine>                        help='number of data loading workers (default: 16)')<NewLine>    parser.add_argument('--lr',<NewLine>                        default=0.0001, type=float,<NewLine>                        help='initial learning rate')<NewLine>    parser.add_argument('--momentum',<NewLine>                        default=0.9, type=float, metavar='M',<NewLine>                        help='momentum')<NewLine>    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,<NewLine>                        metavar='W', help='weight decay (default: 1e-4)',<NewLine>                        dest='weight_decay')<NewLine>    parser.add_argument('--lr-step-size', default=30, type=int,<NewLine>                        help='decrease lr every step-size epochs')<NewLine>    parser.add_argument('--lr-gamma', default=0.1, type=float,<NewLine>                        help='decrease lr by a factor of lr-gamma')<NewLine>    parser.add_argument('--print-freq', default=10, type=int,<NewLine>                        help='print frequency')<NewLine>    parser.add_argument('--output-dir', default='.', help='path where to save')<NewLine>    parser.add_argument('--resume', default='', help='resume from checkpoint')<NewLine>    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',<NewLine>                        help='start epoch')<NewLine><NewLine>    parser.add_argument(<NewLine>        ""--cache-dataset"",<NewLine>        dest=""cache_dataset"",<NewLine>        help=""Cache the datasets for quicker initialization. \<NewLine>             It also serializes the transforms"",<NewLine>        action=""store_true"",<NewLine>    )<NewLine>    parser.add_argument(<NewLine>        ""--test-only"",<NewLine>        dest=""test_only"",<NewLine>        help=""Only test the model"",<NewLine>        action=""store_true"",<NewLine>    )<NewLine>    parser.add_argument(<NewLine>        ""--post-training-quantize"",<NewLine>        dest=""post_training_quantize"",<NewLine>        help=""Post training quantize the model"",<NewLine>        action=""store_true"",<NewLine>    )<NewLine>    # distributed training parameters<NewLine>    parser.add_argument('--world-size', default=1, type=int,<NewLine>                        help='number of distributed processes')<NewLine>    parser.add_argument('--dist-url',<NewLine>                        default='env://',<NewLine>                        help='url used to set up distributed training')<NewLine><NewLine>    args = parser.parse_args()<NewLine><NewLine>    return args<NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    args = parse_args()<NewLine>    main(args)<NewLine></code></pre><NewLine></details><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>The torchvision reference script (train_quantization.py) has not been tested for multiGPU support yet. Recently, we landed fixes to the code that should solve this issue:<br/><NewLine><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/33626"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/33626"" rel=""nofollow noopener"" target=""_blank"">[quant] Regsiter fake_quant and observer attributes as buffers</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:gh/supriyar/56/base</code> ← <code>pytorch:gh/supriyar/56/head</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-02-21"" data-format=""ll"" data-time=""19:16:06"" data-timezone=""UTC"">07:16PM - 21 Feb 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/supriyar"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""supriyar"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/2657489?v=4"" width=""20""/><NewLine>          supriyar<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""5 commits changed 2 files with 18 additions and 13 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/33626/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+18</span><NewLine><span class=""removed"">-13</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Note that syncBN is not yet supported for quantization aware training.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have a try with the changed files, but it remains a bug.</p><NewLine><pre><code class=""lang-auto"">    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)<NewLine>RuntimeError: Tensors must be CUDA and dense<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hit the same issue by using pytorch 1.5.0.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Creating a github issue to track this problem.<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/37270"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/37270"" rel=""nofollow noopener"" target=""_blank"">Broadcasting does not work for Quantization aware training with multiple GPUs</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-25"" data-format=""ll"" data-time=""00:44:07"" data-timezone=""UTC"">12:44AM - 25 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/raghuramank100"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""raghuramank100"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/48835916?v=4"" width=""20""/><NewLine>          raghuramank100<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Repro code and error info are at:<NewLine>https://discuss.pytorch.org/t/quantization-awareness-training-multi-gpu-suport/66106<NewLine>Snippet of error at:<NewLine>Traceback (most recent call last):<NewLine>File “train_quantization.py”, line 258, in<NewLine>main(args)<NewLine>File “train_quantization.py”, line 77,...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">quantization</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can try to set the default value of scale and zero point, because it can not broadcast none tensor.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Great to see this merge request.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tried to set default values of scale, zero point, quant_min and quant_max, and I could see the same error:<br/><NewLine>“dist._broadcast_coalesced(self.process_group, tensors, buffer_size)<br/><NewLine>RuntimeError: Tensors must be CUDA and dense”.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>what’s the default value you set?</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>scale = torch.FloatTensor([1])</p><NewLine><p>zero_point = torch.FloatTensor([0])<br/><NewLine>min_val = torch.FloatTensor([0])</p><NewLine><p>max_val = torch.FloatTensor([255])</p><NewLine><p>robotcator via PyTorch Forums &lt;<a href=""mailto:noreply@discuss.pytorch.org"">noreply@discuss.pytorch.org</a>&gt; 於 2020年5月20日 週三 上午11:05寫道：</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have now added multi-GPU support for Quantization aware training in the nightly build, let us know if you see any issues</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>Get the information “There is still work to do on verifying that BN is working correctly in<br/><NewLine>QAT + DDP, but saving that for a separate PR.” from <a href=""https://github.com/pytorch/vision/pull/2230"" rel=""nofollow noopener"">https://github.com/pytorch/vision/pull/2230</a>.<br/><NewLine>Could you provide the PR for tracking? Thanks.</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>the PR to make BN work correctly with QAT+DDP is here: <a href=""https://github.com/pytorch/pytorch/pull/38478"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/38478</a> .  This enables SyncBatchNorm to be swapped in to a fused QAT Conv-BN.  I will update the issue.  There were also a couple of bug fixes landed, such as <a href=""https://github.com/pytorch/pytorch/pull/38368"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/38368</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Mazhar_Shaikh; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/AliceLeeHX; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/slippers_HUANG; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/slippers_HUANG; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/slippers_HUANG; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/slippers_HUANG; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: January 9, 2020,  9:15am; <NewLine> REPLY_DATE 2: January 10, 2020,  2:13pm; <NewLine> REPLY_DATE 3: January 11, 2020,  4:23am; <NewLine> REPLY_DATE 4: January 13, 2020,  8:00pm; <NewLine> REPLY_DATE 5: January 17, 2020,  3:44am; <NewLine> REPLY_DATE 6: April 16, 2020, 10:24pm; <NewLine> REPLY_DATE 7: April 20, 2020,  6:13am; <NewLine> REPLY_DATE 8: April 24, 2020,  6:08am; <NewLine> REPLY_DATE 9: April 25, 2020, 12:44am; <NewLine> REPLY_DATE 10: April 25, 2020,  2:27pm; <NewLine> REPLY_DATE 11: April 25, 2020,  2:28pm; <NewLine> REPLY_DATE 12: April 25, 2020,  3:35pm; <NewLine> REPLY_DATE 13: May 20, 2020,  2:55am; <NewLine> REPLY_DATE 14: May 20, 2020,  3:52pm; <NewLine> REPLY_DATE 15: May 29, 2020, 10:13pm; <NewLine> REPLY_DATE 16: June 2, 2020,  8:00am; <NewLine> REPLY_DATE 17: July 8, 2020,  2:01am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
86708,No Difference in Model size of BERT fine-tuned with amp and without amp,2020-06-24T11:32:19.006Z,6,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Everyone,</p><NewLine><p>I have fine-tuned <strong>bert-base model with amp</strong> and <strong>without amp</strong> using <code>MAX_SEQ_LEN=512</code>. I compared the performance among these models in terms of:</p><NewLine><ol><NewLine><li>Fine-tuning time</li><NewLine><li>Inference time on CPU/GPU</li><NewLine><li>Model size</li><NewLine></ol><NewLine><p>While conducting first experiment, I observed that in terms of <strong>Fine-tuning time</strong> , <code>bert model with amp</code> performs better as compare to <code>without amp</code>.</p><NewLine><p>However, when I compare the inference time and model size, both models have same inference time and model size.</p><NewLine><p>Could anyone please explain why this is the case?</p><NewLine></div>",https://discuss.pytorch.org/u/Ramesh_Kumar,(Ramesh Kumar),Ramesh_Kumar,"June 24, 2020, 11:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using PyTorch dynamic quantization for the model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>The model size regarding its parameters won’t be changed, as the operations (and intermediates) will be casted to FP16 for “safe” ops.<br/><NewLine>So while you might be able to increase the batch size during training or inference, the <code>state_dict</code> won’t be smaller in size.</p><NewLine><p>Which batch size are you using for inference? If you are seeing a speedup during training, you should also see it during inference. However, if your batch size is low (e.g. a single sample), the performance gain might be too small compared to the overheads of launching all kernels.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> thanks for your answer.</p><NewLine><p>I have tried using different batch sizes e.g (8, 16, 64, 128). But I am not finding any difference.</p><NewLine><p>Regarding the code, I am following examples here: <a href=""https://pytorch.org/docs/stable/notes/amp_examples.html"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/amp_examples.html</a></p><NewLine><p>Update: I am able to see the difference in inference time on <code>GPU</code> using</p><NewLine><pre><code class=""lang-auto"">with autocast():<NewLine>        with torch.no_grad():<NewLine>            outputs = model(**inputs)<NewLine></code></pre><NewLine><p>But when I compare the inference time on CPU, I do not notice any difference.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""86708"" data-username=""Ramesh_Kumar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ramesh_kumar/40/12432_2.png"" width=""20""/> Ramesh_Kumar:</div><NewLine><blockquote><NewLine><p>But when I compare the inference time on CPU, I do not notice any difference.</p><NewLine></blockquote><NewLine></aside><NewLine><p>Automatic mixed precision is implemented for CUDA operations (and is thus in the <code>torch.cuda</code> namespace). By appying amp your GPU could use TensorCores for certain operations, which would yield a speedup. I don’t know if anything like that is implemented for CPU operations (and if I’m not mistaken not all operations are implemented for <code>HalfTensors</code> on the CPU).</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks for your response. No, I am not using dynamic quantization. But, since I cannot do mixed precision on cpu. Hence, I guess for CPU I have to switch to dynamic quantization. Am I right?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>So do you think, post-training quantization is better idea ? If we want to reduce inference time on cpu?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve unfortunately never profiled the quantized models, so unsure what the expected speedup is.<br/><NewLine>However, please let us know once if you are using the post-training quantized models and how large the performance gain is. <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> just to give you update  regarding dynamic quantization on cpu. There is issue with quantized bert model which I and many others are facing. Here is the github issue link : <a href=""https://github.com/huggingface/transformers/issues/2542"" rel=""nofollow noopener"">https://github.com/huggingface/transformers/issues/2542</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Ramesh_Kumar; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  6:14pm; <NewLine> REPLY_DATE 2: June 25, 2020,  6:22am; <NewLine> REPLY_DATE 3: June 26, 2020,  2:16pm; <NewLine> REPLY_DATE 4: June 27, 2020,  8:19am; <NewLine> REPLY_DATE 5: June 29, 2020,  8:19am; <NewLine> REPLY_DATE 6: June 29, 2020,  8:19am; <NewLine> REPLY_DATE 7: June 30, 2020,  1:30am; <NewLine> REPLY_DATE 8: July 7, 2020,  8:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
87319,Quantization not Decreasing Model Size (Static and QAT),2020-06-29T15:50:42.522Z,4,225,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am trying to quantize a text detection model based on Mobilenet (model definition <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/model.py"" rel=""nofollow noopener"">here</a> )</p><NewLine><p>After inserting the quant and dequant stub, fusing all the conv+bn+relu and conv+relu, replacing cat with skip_add.cat() . I perform the static quantization (script - <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py"" rel=""nofollow noopener"">https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py</a> )</p><NewLine><p>After performing quantization, the model size doesn’t go down (in fact it increases )</p><NewLine><pre><code class=""lang-auto"">Original Size:<NewLine>Size (MB): 6.623636<NewLine><NewLine>Fused model Size:<NewLine>Size (MB): 6.638188<NewLine><NewLine>Quantized model Size:<NewLine>Size (MB): 7.928258<NewLine></code></pre><NewLine><p>I have even printed the final quantized model <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt"" rel=""nofollow noopener"">here</a></p><NewLine><p>I changed the qconfig to <code>fused_model.qconfig = torch.quantization.default_qconfig</code> but still quantized_model size is <code>Size (MB): 6.715115</code></p><NewLine><p>Why doesn’t the model size reduce ?</p><NewLine></div>",https://discuss.pytorch.org/u/Raghav_Gurbaxani,(Raghav Gurbaxani),Raghav_Gurbaxani,"June 30, 2020,  1:37am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looking at the model def you posted, it looks like it is not yet quantized.  One missing thing is calibration. You can add a calibration step after you call prepare and before you call convert:</p><NewLine><pre><code>torch.quantization.prepare(fused_model, inplace=True)<NewLine><NewLine># calibrate your model by feeding it example inputs<NewLine>for inputs in your_dataset:        <NewLine>    fused_model(inputs)<NewLine><NewLine>print('Quantized model Size:')<NewLine>quantized = torch.quantization.convert(fused_model, inplace=False)<NewLine>print_size_of_model(quantized)</code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a></p><NewLine><p>Thank you for your input, I have updated my script to pass in a few images into the fused model as inputs for calibration.</p><NewLine><p>Please see the updated script here<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py</a></h4><NewLine><pre><code class=""lang-py"">import os<NewLine>import config as cfg<NewLine>from model import East<NewLine>import torch<NewLine>import utils<NewLine>import preprossing<NewLine>import cv2<NewLine>import numpy as np<NewLine>import time<NewLine><NewLine>def uninplace(model):<NewLine>    if hasattr(model, 'inplace'):<NewLine>        model.inplace = False<NewLine>    if not model.children():<NewLine>        return<NewLine>    for child in model.children():<NewLine>        uninplace(child)<NewLine>        <NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_quantization.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>But still the quantized model size is bigger than the original model -</p><NewLine><pre><code class=""lang-auto"">Original Size:<NewLine>Size (MB): 6.623636<NewLine><NewLine>Fused model Size:<NewLine>Size (MB): 6.638188<NewLine><NewLine>Quantized model Size:<NewLine>Size (MB): 6.712286<NewLine></code></pre><NewLine><p>there seems to be some improvement due to the calibration, but the quantized model size is still not satisfactory compared to the original size  <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine><p>Could you suggest what’s going wrong here ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a></p><NewLine><p>I also tried a script with Quantized Aware Training -<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py</a></h4><NewLine><pre><code class=""lang-py"">import os<NewLine>import config as cfg<NewLine>from model import East<NewLine>import torch<NewLine>import utils<NewLine>import preprossing<NewLine>import cv2<NewLine>import numpy as np<NewLine>import time<NewLine>import loss<NewLine><NewLine>def uninplace(model):<NewLine>    if hasattr(model, 'inplace'):<NewLine>        model.inplace = False<NewLine>    if not model.children():<NewLine>        return<NewLine>    for child in model.children():<NewLine>        uninplace(child)<NewLine>        <NewLine>def print_size_of_model(model):<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>But still the quantized model is bigger than the original model <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/> <img alt="":no_mouth:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/no_mouth.png?v=9"" title="":no_mouth:""/></p><NewLine><p>I don’t know what’s going wrong here</p><NewLine><pre><code class=""lang-auto"">Original Size:<NewLine>Size (MB): 6.623636<NewLine><NewLine>Fused model Size:<NewLine>Size (MB): 6.638188<NewLine><NewLine>Quantized model Size:<NewLine>Size (MB): 6.712286<NewLine><NewLine>QAT model Size:<NewLine>Size (MB): 6.712286<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>in the paste here (<a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt"" rel=""nofollow noopener"">https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt</a>), the model doesn’t look quantized.  One would expect to see <code>QuantizedConv</code> instead of <code>Conv</code> and <code>QuantizedLinear</code> instead of <code>Linear</code>.  One thing to try could be to make sure to run the convert script and ensure that you see the quantized module equivalents afterwards.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a></p><NewLine><p>Please check the updated quantized_model now -</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt</a></h4><NewLine><pre><code class=""lang-txt"">Size (MB): 6.712286<NewLine>DataParallel(<NewLine>  (module): East(<NewLine>    (mobilenet): MobileNetV2(<NewLine>      (features): Sequential(<NewLine>        (0): Sequential(<NewLine>          (0): ConvBnReLU2d(<NewLine>            (0): Conv2d(<NewLine>              3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>              (activation_post_process): MinMaxObserver(min_val=-694.3411254882812, max_val=765.30712890625)<NewLine>            )<NewLine>            (1): BatchNorm2d(<NewLine>              32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<NewLine>              (activation_post_process): MinMaxObserver(min_val=-4.2157487869262695, max_val=4.755300998687744)<NewLine>            )<NewLine>            (2): ReLU(<NewLine>              (activation_post_process): MinMaxObserver(min_val=0.0, max_val=4.755300998687744)<NewLine>            )<NewLine>          )<NewLine>          (1): Identity()<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/quantized_model.txt"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>it seems to have quantized covolutions (line 100 onwards). I don’t know why the layers before line 100 do not have quantized modules.</p><NewLine><p>Do you think my quantstub and dequantstub placement is incorrect ?<br/><NewLine>Here’s the model (with quant and dequant stub)<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/Quantization_Experiments/blob/master/model.py</a></h4><NewLine><pre><code class=""lang-py"">import torch.nn as nn<NewLine>import math<NewLine>import torch<NewLine>import config as cfg<NewLine>import utils<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>def conv_bn(inp, oup, stride):<NewLine>    return nn.Sequential(<NewLine>        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),<NewLine>        nn.BatchNorm2d(oup),<NewLine>        nn.ReLU(inplace=True)<NewLine>    )<NewLine><NewLine><NewLine>class InvertedResidual(nn.Module):<NewLine>    def __init__(self, inp, oup, stride, expand_ratio):<NewLine>        super(InvertedResidual, self).__init__()<NewLine>        self.stride = stride<NewLine>        assert stride in [1, 2]<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/model.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>Main script here - <a href=""https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py"" rel=""nofollow noopener"">https://github.com/raghavgurbaxani/Quantization_Experiments/blob/master/try_qat.py</a></p><NewLine><p>I suspect maybe my quant and dequant stub may be incorrect but apart from that I’ve followed all the steps as posted in the static quantization tutorial.</p><NewLine><p>Reallly appreciate your help</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a>  any update on this ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Raghav,</p><NewLine><p>For post training quantization, we want the model to be in eval mode (see <a href=""https://github.com/pytorch/pytorch/blob/530d48e93a3f04a5ec63a1b789c19a5f775bf497/torch/quantization/fuse_modules.py#L63"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/530d48e93a3f04a5ec63a1b789c19a5f775bf497/torch/quantization/fuse_modules.py#L63</a>).  So, you can add a <code>model.eval()</code> call before you fuse modules:</p><NewLine><pre><code class=""lang-auto"">model.eval()<NewLine>torch.quantization.fuse_modules(...)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: June 29, 2020,  5:01pm; <NewLine> REPLY_DATE 2: June 29, 2020, 10:40pm; <NewLine> REPLY_DATE 3: June 30, 2020,  1:43am; <NewLine> REPLY_DATE 4: June 30, 2020,  3:48pm; <NewLine> REPLY_DATE 5: June 30, 2020,  4:46pm; <NewLine> REPLY_DATE 6: July 2, 2020,  3:33pm; <NewLine> REPLY_DATE 7: July 8, 2020,  9:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> 
87807,How to convert a 32-bit operation to a 4-bit or 8-bit operation on cpu?,2020-07-03T07:51:14.765Z,0,76,"<div class=""post"" itemprop=""articleBody""><NewLine><p>To the best of my knowledge, the existing quantization method is operating on 32-bit.<br/><NewLine>In order to quantize weight of CNN as well as reduce memory footprint and then port the quantized model into the mobile device, how to convert a 32-bit operation to a 4-bit or 8-bit operation on cpu?</p><NewLine></div>",https://discuss.pytorch.org/u/learningsteady0J0,(Learningsteady0 J0),learningsteady0J0,"July 3, 2020,  7:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>PyTorch quantization supports int8 (but not int4), with fast kernels for CPU on mobile via QNNPACK.  <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> has some information to get started, and you would want to set the backend to <code>qnnpack</code> to target mobile CPUs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> ,"REPLY_DATE 1: July 6, 2020,  6:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
74895,Quantize the CRNN model,2020-03-31T11:14:31.204Z,3,161,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I read the quantization paper in pytorch website and realize the post dynamic quantization and static quantization.dynamic quantization is good for LSTM and Linear, and static quantization is good for CNNs, I wanna ask: when I use the CRNN model，the model  is like: CNN + LSTM + Linear, what is the best way to quantize my model, or is there some tricks to mix the two quantization methods?</p><NewLine><p>I’d appreciate if anybody can help me! Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"March 31, 2020, 11:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think it’s possible, you may apply static quantization to the CNN part of the model and dynamic quantization on LSTM + Linear part of the model, since both of them will have float data in the input and output, the combined model should work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>1.fix rnn and linear layers, quantize cnn layers (post-training static quantization)<br/><NewLine>2.fix rnn and linear layers, quantize cnn layers (quantization-aware training, this step is optional)<br/><NewLine>3.fix quantized cnn layers, quantize rnn and linear layers(post-training dynamic quantization)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, how to fix rnn and linear layers when quantize cnn layers?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Quantization is controlled by the qconfig, so when quantize cnn layers you can remove the qconfig of rnn layer, this way rnn layer will not be quantized.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thk, I will try it soon!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/wizardk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/THU-cui; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/THU-cui; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  5:08pm; <NewLine> REPLY_DATE 2: April 7, 2020,  1:33pm; <NewLine> REPLY_DATE 3: July 2, 2020,  9:45am; <NewLine> REPLY_DATE 4: July 3, 2020,  6:50am; <NewLine> REPLY_DATE 5: July 4, 2020,  2:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
87901,Export fp16 model to ONNX,2020-07-04T01:59:18.547Z,0,70,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Exporting fp16 Pytorch model to ONNX via the exporter fails. How to solve this?</p><NewLine></div>",https://discuss.pytorch.org/u/Hiperdyne19012,(Hiperdyne19012),Hiperdyne19012,"July 4, 2020,  1:59am",,,,,
87395,"Int8 quantization of the resnet18 model, the results of each quantization are inconsistent",2020-06-30T07:03:56.639Z,0,94,"<div class=""post"" itemprop=""articleBody""><NewLine><p>class quantizeModel(object):<br/><NewLine>“”“docstring for quantizePytorchModel”""""</p><NewLine><pre><code>def __init__(self):<NewLine>    super(quantizeModel, self).__init__()<NewLine>    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')<NewLine>    self.train_loader, self.test_loader = get_imagenet()<NewLine>    self.quant()<NewLine><NewLine>def quant(self):<NewLine><NewLine>    model = self.load_model()<NewLine>    model.eval()<NewLine>    self.print_size_of_model(model)<NewLine>    self.validate(model, ""original_resnet18"", self.test_loader)<NewLine>    model.fuse_model()<NewLine><NewLine>    self.print_size_of_model(model)<NewLine>    self.quantize(model)<NewLine><NewLine>def load_model(self):<NewLine>    model = resnet18()<NewLine>    state_dict = torch.load(""CIFAR10_resnet18.pth"", map_location=self.device)<NewLine>    model.load_state_dict(state_dict)<NewLine>    model.to(self.device)<NewLine>    return model<NewLine><NewLine>def print_size_of_model(self, model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>def validate(self, model, name, data_loader):<NewLine>    with torch.no_grad():<NewLine>        correct = 0<NewLine>        total = 0<NewLine>        acc = 0<NewLine>        for data in data_loader:<NewLine>            images, labels = data<NewLine>            images, labels = images.to(self.device), labels.to(self.device)<NewLine>            output = model(images)<NewLine><NewLine>            _, predicted = torch.max(output, dim=1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine>            if total == 1024: <NewLine>                break<NewLine>        acc = round(100 * correct / total, 3)<NewLine>        print('{{""metric"": ""{}_val_accuracy"", ""value"": {}%}}'.format(name, acc))<NewLine>        return acc<NewLine><NewLine>def quantize(self, model):<NewLine>    #model.qconfig = torch.quantization.default_qconfig<NewLine>    #model.qconfig = torch.quantization.default_per_channel_qconfig<NewLine><NewLine>    model.qconfig = torch.quantization.QConfig(<NewLine>        activation=torch.quantization.observer.MinMaxObserver.with_args(reduce_range=True),<NewLine>        weight=torch.quantization.observer.PerChannelMinMaxObserver.with_args(dtype=torch.qint8,<NewLine>                                                                              qscheme=torch.per_channel_affine))<NewLine>    pmodel = torch.quantization.prepare(model)<NewLine><NewLine>    #calibration<NewLine>    self.validate(pmodel, ""quntize_per_channel_resent18_train"", self.train_loader)<NewLine>    qmodel = torch.quantization.convert(pmodel)<NewLine><NewLine>    self.validate(qmodel, ""quntize_per_chaannel_resent18_test"", self.test_loader)<NewLine>    self.print_size_of_model(qmodel)<NewLine><NewLine>    torch.jit.save(torch.jit.script(qmodel), ""quantization_per_channel_model18.pth"")</code></pre><NewLine></div>",https://discuss.pytorch.org/u/blueskywwc,(Blueskywwc),blueskywwc,"July 1, 2020,  2:40am",,,,,
87004,INT8 quantized model is much slower than fp32 model on CPU,2020-06-26T09:09:08.950Z,6,340,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, all</p><NewLine><p>I finally success converting the fp32 model to the int8 model thanks to pytorch forum community <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.<br/><NewLine>In order to make sure that the model is quantized, I checked that the size of my quantized model is smaller than the fp32 model (500MB-&gt;130MB).<br/><NewLine>However, operating my quantized model is much slower than operating the fp32 model. (700ms -&gt; 2.4s)</p><NewLine><p>I converted pre-trained VGG16 model in torchvision.models.<br/><NewLine>I am working on Nvidia JetsonTx2, and I checked that quantized mobilenet in torchvision.models.quantization.mobilenet is much faster than fp32 mobilenet model.<br/><NewLine>So I think that my conversion work might be wrong.</p><NewLine><p>If you guys need more information, please let me know.</p><NewLine><p>This is an output of “print(quantized_model)”.</p><NewLine><pre><code class=""lang-auto"">RecursiveScriptModule(<NewLine>  original_name=VGG<NewLine>  (features): RecursiveScriptModule(<NewLine>    original_name=Sequential<NewLine>    (0): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (1): RecursiveScriptModule(original_name=ReLU)<NewLine>    (2): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (3): RecursiveScriptModule(original_name=ReLU)<NewLine>    (4): RecursiveScriptModule(original_name=MaxPool2d)<NewLine>    (5): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (6): RecursiveScriptModule(original_name=ReLU)<NewLine>    (7): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (8): RecursiveScriptModule(original_name=ReLU)<NewLine>    (9): RecursiveScriptModule(original_name=MaxPool2d)<NewLine>    (10): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (11): RecursiveScriptModule(original_name=ReLU)<NewLine>    (12): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (13): RecursiveScriptModule(original_name=ReLU)<NewLine>    (14): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (15): RecursiveScriptModule(original_name=ReLU)<NewLine>    (16): RecursiveScriptModule(original_name=MaxPool2d)<NewLine>    (17): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (18): RecursiveScriptModule(original_name=ReLU)<NewLine>    (19): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (20): RecursiveScriptModule(original_name=ReLU)<NewLine>    (21): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (22): RecursiveScriptModule(original_name=ReLU)<NewLine>    (23): RecursiveScriptModule(original_name=MaxPool2d)<NewLine>    (24): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (25): RecursiveScriptModule(original_name=ReLU)<NewLine>    (26): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (27): RecursiveScriptModule(original_name=ReLU)<NewLine>    (28): RecursiveScriptModule(original_name=Conv2d)<NewLine>    (29): RecursiveScriptModule(original_name=ReLU)<NewLine>    (30): RecursiveScriptModule(original_name=MaxPool2d)<NewLine>  )<NewLine>  (avgpool): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)<NewLine>  (classifier): RecursiveScriptModule(<NewLine>    original_name=Sequential<NewLine>    (0): RecursiveScriptModule(<NewLine>      original_name=Linear<NewLine>      (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)<NewLine>    )<NewLine>    (1): RecursiveScriptModule(original_name=ReLU)<NewLine>    (2): RecursiveScriptModule(original_name=Dropout)<NewLine>    (3): RecursiveScriptModule(<NewLine>      original_name=Linear<NewLine>      (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)<NewLine>    )<NewLine>    (4): RecursiveScriptModule(original_name=ReLU)<NewLine>    (5): RecursiveScriptModule(original_name=Dropout)<NewLine>    (6): RecursiveScriptModule(<NewLine>      original_name=Linear<NewLine>      (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)<NewLine>    )<NewLine>  )<NewLine>  (quant): RecursiveScriptModule(original_name=Quantize)<NewLine>  (dequant): RecursiveScriptModule(original_name=DeQuantize)<NewLine>)<NewLine></code></pre><NewLine><p>The following code is the conversion code that I wrote.</p><NewLine><pre><code class=""lang-auto"">from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>import numpy as np<NewLine>import torch<NewLine>import torch.nn as nn<NewLine>import torchvision<NewLine>from torch.utils.data import DataLoader<NewLine>from torchvision import datasets<NewLine>import torchvision.transforms as transforms<NewLine>import os<NewLine>import time<NewLine>import sys<NewLine>import torch.quantization<NewLine><NewLine># # Setup warnings<NewLine>import warnings<NewLine>warnings.filterwarnings(<NewLine>    action='ignore',<NewLine>    category=DeprecationWarning,<NewLine>    module=r'.*'<NewLine>)<NewLine>warnings.filterwarnings(<NewLine>    action='default',<NewLine>    module=r'torch.quantization'<NewLine>)<NewLine><NewLine># Specify random seed for repeatable results<NewLine>torch.manual_seed(191009)<NewLine><NewLine><NewLine>from torch.hub import load_state_dict_from_url<NewLine><NewLine><NewLine>__all__ = [<NewLine>    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',<NewLine>    'vgg19_bn', 'vgg19',<NewLine>]<NewLine><NewLine><NewLine>model_urls = {<NewLine>    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',<NewLine>    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',<NewLine>    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',<NewLine>    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',<NewLine>    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',<NewLine>    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',<NewLine>    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',<NewLine>    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',<NewLine>}<NewLine><NewLine><NewLine>class VGG(nn.Module):<NewLine><NewLine>    def __init__(self, features, num_classes=1000, init_weights=True):<NewLine>        super(VGG, self).__init__()<NewLine>        self.features = features<NewLine>        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))<NewLine>        self.classifier = nn.Sequential(<NewLine>            nn.Linear(512 * 7 * 7, 4096),<NewLine>            nn.ReLU(True),<NewLine>            nn.Dropout(),<NewLine>            nn.Linear(4096, 4096),<NewLine>            nn.ReLU(True),<NewLine>            nn.Dropout(),<NewLine>            nn.Linear(4096, num_classes),<NewLine>        )<NewLine>        if init_weights:<NewLine>            self._initialize_weights()<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.features(x)<NewLine>        x = self.avgpool(x)<NewLine>        x = torch.flatten(x, 1)<NewLine>        x = self.classifier(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def _initialize_weights(self):<NewLine>        for m in self.modules():<NewLine>            if isinstance(m, nn.Conv2d):<NewLine>                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')<NewLine>                if m.bias is not None:<NewLine>                    nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.BatchNorm2d):<NewLine>                nn.init.constant_(m.weight, 1)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>            elif isinstance(m, nn.Linear):<NewLine>                nn.init.normal_(m.weight, 0, 0.01)<NewLine>                nn.init.constant_(m.bias, 0)<NewLine><NewLine><NewLine>def make_layers(cfg, batch_norm=False):<NewLine>    layers = []<NewLine>    in_channels = 3<NewLine>    for v in cfg:<NewLine>        if v == 'M':<NewLine>            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]<NewLine>        else:<NewLine>            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)<NewLine>            if batch_norm:<NewLine>                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]<NewLine>            else:<NewLine>                layers += [conv2d, nn.ReLU(inplace=True)]<NewLine>            in_channels = v<NewLine>    return nn.Sequential(*layers)<NewLine><NewLine><NewLine>cfgs = {<NewLine>    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],<NewLine>    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],<NewLine>    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],<NewLine>    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],<NewLine>}<NewLine><NewLine><NewLine>def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):<NewLine>    if pretrained:<NewLine>        kwargs['init_weights'] = False<NewLine>    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)<NewLine>    if pretrained:<NewLine>        state_dict = load_state_dict_from_url(model_urls[arch],<NewLine>                                              progress=progress)<NewLine>        model.load_state_dict(state_dict)<NewLine>    return model<NewLine><NewLine><NewLine>def vgg11(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 11-layer model (configuration ""A"") from<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg11_bn(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 11-layer model (configuration ""A"") with batch normalization<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg13(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 13-layer model (configuration ""B"")<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg13_bn(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 13-layer model (configuration ""B"") with batch normalization<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg16(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 16-layer model (configuration ""D"")<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg16_bn(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 16-layer model (configuration ""D"") with batch normalization<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg19(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 19-layer model (configuration ""E"")<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)<NewLine><NewLine><NewLine>def vgg19_bn(pretrained=False, progress=True, **kwargs):<NewLine>    r""""""VGG 19-layer model (configuration 'E') with batch normalization<NewLine>    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" &lt;https://arxiv.org/pdf/1409.1556.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)<NewLine><NewLine>class AverageMeter(object):<NewLine>    """"""Computes and stores the average and current value""""""<NewLine>    def __init__(self, name, fmt=':f'):<NewLine>        self.name = name<NewLine>        self.fmt = fmt<NewLine>        self.reset()<NewLine><NewLine>    def reset(self):<NewLine>        self.val = 0<NewLine>        self.avg = 0<NewLine>        self.sum = 0<NewLine>        self.count = 0<NewLine><NewLine>    def update(self, val, n=1):<NewLine>        self.val = val<NewLine>        self.sum += val * n<NewLine>        self.count += n<NewLine>        self.avg = self.sum / self.count<NewLine><NewLine>    def __str__(self):<NewLine>        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'<NewLine>        return fmtstr.format(**self.__dict__)<NewLine><NewLine><NewLine>def accuracy(output, target, topk=(1,)):<NewLine>    """"""Computes the accuracy over the k top predictions for the specified values of k""""""<NewLine>    with torch.no_grad():<NewLine>        maxk = max(topk)<NewLine>        batch_size = target.size(0)<NewLine><NewLine>        _, pred = output.topk(maxk, 1, True, True)<NewLine>        pred = pred.t()<NewLine>        correct = pred.eq(target.view(1, -1).expand_as(pred))<NewLine><NewLine>        res = []<NewLine>        for k in topk:<NewLine>            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)<NewLine>            res.append(correct_k.mul_(100.0 / batch_size))<NewLine>        return res<NewLine><NewLine><NewLine>def evaluate(model, criterion, data_loader, neval_batches):<NewLine>    model.eval()<NewLine>    top1 = AverageMeter('Acc@1', ':6.2f')<NewLine>    top5 = AverageMeter('Acc@5', ':6.2f')<NewLine>    cnt = 0<NewLine>    with torch.no_grad():<NewLine>        for image, target in data_loader:<NewLine>            output = model(image)<NewLine>            loss = criterion(output, target)<NewLine>            cnt += 1<NewLine>            acc1, acc5 = accuracy(output, target, topk=(1, 5))<NewLine>            print('.', end = '')<NewLine>            top1.update(acc1[0], image.size(0))<NewLine>            top5.update(acc5[0], image.size(0))<NewLine>            if cnt &gt;= neval_batches:<NewLine>                 return top1, top5<NewLine><NewLine>    return top1, top5<NewLine><NewLine>def load_model(model_file):<NewLine>    if model_file is None:<NewLine>        model = vgg16(pretrained=True)<NewLine>    if not model_file is None:<NewLine>        model = vgg16()<NewLine>        state_dict = torch.load(model_file)<NewLine>        model.load_state_dict(state_dict)<NewLine>    model.to('cpu')<NewLine>    return model<NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"")/1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>'''<NewLine>import requests<NewLine><NewLine>url = 'https://s3.amazonaws.com/pytorch-tutorial-assets/imagenet_1k.zip'<NewLine>filename = '~/Downloads/imagenet_1k_data.zip'<NewLine><NewLine>r = requests.get(url)<NewLine><NewLine>with open(filename, 'wb') as f:<NewLine>    f.write(r.content)<NewLine>'''<NewLine>import torchvision<NewLine>import torchvision.transforms as transforms<NewLine>'''<NewLine>imagenet_dataset = torchvision.datasets.ImageNet(<NewLine>    'data/imagenet_1k',<NewLine>    split='train',<NewLine>    download=True,<NewLine>    transform=transforms.Compose([<NewLine>        transforms.RandomResizedCrop(224),<NewLine>        transforms.RandomHorizontalFlip(),<NewLine>        transforms.ToTensor(),<NewLine>        transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                             std=[0.229, 0.224, 0.225]),<NewLine>    ]))<NewLine>'''<NewLine><NewLine>def prepare_data_loaders(data_path):<NewLine><NewLine>    traindir = os.path.join(data_path, 'train')<NewLine>    valdir = os.path.join(data_path, 'val')<NewLine>    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                     std=[0.229, 0.224, 0.225])<NewLine><NewLine>    dataset = torchvision.datasets.ImageFolder(<NewLine>        traindir,<NewLine>        transforms.Compose([<NewLine>            transforms.RandomResizedCrop(224),<NewLine>            transforms.RandomHorizontalFlip(),<NewLine>            transforms.ToTensor(),<NewLine>            normalize,<NewLine>        ]))<NewLine><NewLine>    dataset_test = torchvision.datasets.ImageFolder(<NewLine>        valdir,<NewLine>        transforms.Compose([<NewLine>            transforms.Resize(256),<NewLine>            transforms.CenterCrop(224),<NewLine>            transforms.ToTensor(),<NewLine>            normalize,<NewLine>        ]))<NewLine><NewLine>    train_sampler = torch.utils.data.RandomSampler(dataset)<NewLine>    test_sampler = torch.utils.data.SequentialSampler(dataset_test)<NewLine><NewLine>    data_loader = torch.utils.data.DataLoader(<NewLine>        dataset, batch_size=train_batch_size,<NewLine>        sampler=train_sampler)<NewLine><NewLine>    data_loader_test = torch.utils.data.DataLoader(<NewLine>        dataset_test, batch_size=eval_batch_size,<NewLine>        sampler=test_sampler)<NewLine><NewLine>    return data_loader, data_loader_test<NewLine><NewLine>data_path = 'data/imagenet_1k'<NewLine>saved_model_dir = 'data/'<NewLine>scripted_float_model_file = 'vgg16_quantization_scripted.pth'<NewLine>scripted_quantized_model_file = 'vgg16_quantization_scripted_quantized.pth'<NewLine><NewLine>train_batch_size = 30<NewLine>eval_batch_size = 30<NewLine><NewLine>data_loader, data_loader_test = prepare_data_loaders(data_path)<NewLine>criterion = nn.CrossEntropyLoss()<NewLine>float_model = load_model(None).to('cpu')<NewLine><NewLine>float_model.eval()<NewLine><NewLine>num_eval_batches = 10<NewLine><NewLine>print(""Size of baseline model"")<NewLine>print_size_of_model(float_model)<NewLine><NewLine>top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)<NewLine>print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))<NewLine>torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)<NewLine><NewLine><NewLine>num_calibration_batches = 10<NewLine><NewLine>per_channel_quantized_model = load_model(None).to('cpu')<NewLine>per_channel_quantized_model.eval()<NewLine><NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine>per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine><NewLine>print(per_channel_quantized_model.qconfig)<NewLine><NewLine>torch.quantization.prepare(per_channel_quantized_model, inplace=True)<NewLine>evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)<NewLine>torch.quantization.convert(per_channel_quantized_model, inplace=True)<NewLine>top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)<NewLine>print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))<NewLine>torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Jungmo_Ahn,(Jungmo Ahn),Jungmo_Ahn,"June 26, 2020,  9:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I printed your quantized model def before scripting: <a href=""https://gist.github.com/vkuzo/edb2121a757d5789977935ad56820a24"" rel=""nofollow noopener"">https://gist.github.com/vkuzo/edb2121a757d5789977935ad56820a24</a></p><NewLine><p>One improvement would be to fuse subsequent Conv-ReLU modules together, so they can use the faster fused quantized kernel:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine><NewLine>model = nn.Sequential(<NewLine>    nn.Conv2d(4, 4, 1),<NewLine>    nn.ReLU(),<NewLine>)<NewLine><NewLine># Fuse each Conv and ReLU (implement this for your model)<NewLine>torch.quantization.fuse_modules(model, [['0', '1']], inplace=True)<NewLine>print(model)<NewLine><NewLine># prepare<NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine>model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine><NewLine># calibrate (toy example)<NewLine>input_data = torch.randn(4, 4, 4, 4)<NewLine>model(input_data)<NewLine><NewLine># convert<NewLine>torch.quantization.convert(model, inplace=True)<NewLine><NewLine># should see QuantizedConvReLU2d module<NewLine>print(model)<NewLine></code></pre><NewLine><p>If you still see a performance gap after this, might be good to check if QNNPACK is enabled on your target device.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>and, you can also fuse Linear + ReLU</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply.</p><NewLine><p>Even I used ‘fuse’ for conv+relu and linear+relu but there is no speed improvement.<br/><NewLine>The QNNPACK is well enabled because I checked quantized mobilenet in torchvision.models.quantization.mobilenet which uses qnnpack backend is faster than fp32 model.</p><NewLine><p>Could you suggest another feasible solution?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>maybe you should check your threads num, and use <code>torch.set_num_threads(1)</code></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>it could also be related to op support in QNNPACK.  PyTorch has a fork of QNNPACK which lives here (<a href=""https://github.com/pytorch/pytorch/tree/172f31171a3395cc299044e06a9665fec676ddd6/aten/src/ATen/native/quantized/cpu/qnnpack"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/172f31171a3395cc299044e06a9665fec676ddd6/aten/src/ATen/native/quantized/cpu/qnnpack</a>), and the readme contains the supported ops.</p><NewLine><p>Your model has a few modules which are not supported, which means they would still run but there aren’t fast ARM kernels: AdaptiveAvgPool2d, and Dropout.  Just for debugging’s sake, you could check if removing these modules or replacing them with alternatives which are optimized for ARM fixes the speed issue</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you print your model right before scripting it and verify you get this (<a href=""https://gist.github.com/vkuzo/edb2121a757d5789977935ad56820a24"" rel=""nofollow noopener"">https://gist.github.com/vkuzo/edb2121a757d5789977935ad56820a24</a>) ?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is the output of the model before scripted.</p><NewLine><pre><code class=""lang-auto"">VGG(<NewLine>  (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.015086950734257698, zero_point=2, padding=(1, 1))<NewLine>  (relu1): Identity()<NewLine>  (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005462500732392073, zero_point=0, padding=(1, 1))<NewLine>  (relu2): Identity()<NewLine>  (conv3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.002446091501042247, zero_point=0, padding=(1, 1))<NewLine>  (relu3): Identity()<NewLine>  (conv4): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0008910637116059661, zero_point=1, padding=(1, 1))<NewLine>  (relu4): Identity()<NewLine>  (conv5): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0006946324720047414, zero_point=1, padding=(1, 1))<NewLine>  (relu5): Identity()<NewLine>  (conv6): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0002671453694347292, zero_point=1, padding=(1, 1))<NewLine>  (relu6): Identity()<NewLine>  (conv7): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00013638826203532517, zero_point=3, padding=(1, 1))<NewLine>  (relu7): Identity()<NewLine>  (conv8): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.00012979305756743997, zero_point=0, padding=(1, 1))<NewLine>  (relu8): Identity()<NewLine>  (conv9): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.00012682013039011508, zero_point=1, padding=(1, 1))<NewLine>  (relu9): Identity()<NewLine>  (conv10): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=8.234349661506712e-05, zero_point=1, padding=(1, 1))<NewLine>  (relu10): Identity()<NewLine>  (conv11): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=9.820431296247989e-05, zero_point=0, padding=(1, 1))<NewLine>  (relu11): Identity()<NewLine>  (conv12): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=8.165000326698646e-05, zero_point=0, padding=(1, 1))<NewLine>  (relu12): Identity()<NewLine>  (conv13): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=8.769309351919219e-05, zero_point=0, padding=(1, 1))<NewLine>  (relu13): Identity()<NewLine>  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<NewLine>  (dropout): Dropout(p=0.5, inplace=False)<NewLine>  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))<NewLine>  (fc1): QuantizedLinearReLU(<NewLine>    in_features=25088, out_features=4096, scale=6.691644375678152e-05, zero_point=0<NewLine>    (_packed_params): LinearPackedParams()<NewLine>  )<NewLine>  (relu14): Identity()<NewLine>  (fc2): QuantizedLinearReLU(<NewLine>    in_features=4096, out_features=4096, scale=8.03592411102727e-05, zero_point=0<NewLine>    (_packed_params): LinearPackedParams()<NewLine>  )<NewLine>  (relu15): Identity()<NewLine>  (fc3): QuantizedLinear(<NewLine>    in_features=4096, out_features=1000, scale=0.0001865544618340209, zero_point=131<NewLine>    (_packed_params): LinearPackedParams()<NewLine>  )<NewLine>  (softmax): Softmax(dim=1)<NewLine>  (quant): Quantize(scale=tensor([0.0186]), zero_point=tensor([114]), dtype=torch.quint8)<NewLine>  (dequant): DeQuantize()<NewLine>)<NewLine></code></pre><NewLine><p>In my model, there is a lot of Identity() layer. I thought that these layers are generated by fuse() function. I don’t think that these affect the performance. Do they affect the latency performance of execution?</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks.  The Identity layers do not do anything and shouldn’t contribute to performance.  Your model def after quantization looks right.  Unfortunately we don’t have a JetsonX2 so we can’t check locally, and your setup looks right.  At this point might be good to try and bisect the issue - check if any particular layers are slow (in particular, ones not supported by QNNPACK).</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>FYI, in order to investigate the bottleneck of model execution, I profiled my quantized model using torch.autograd.profiler.profile().<br/><NewLine>I thought there is some problem about quantized::conv2d which QNNPACK supports.</p><NewLine><p>I will share the further progress, thank you so much <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a> ! <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><pre><code class=""lang-auto""> ---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------<NewLine>Name                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls<NewLine>---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------<NewLine>quantized::conv2d            92.64%           2.221s           92.67%           2.222s           170.929ms        13<NewLine>quantized::linear            5.69%            136.383ms        5.69%            136.452ms        45.484ms         3<NewLine>_adaptive_avg_pool2d         0.64%            15.370ms         0.64%            15.370ms         15.370ms         1<NewLine>relu_                        0.49%            11.650ms         0.49%            11.650ms         776.697us        15<NewLine>quantized_max_pool2d         0.40%            9.491ms          0.40%            9.491ms          1.898ms          5<NewLine>quantize_per_tensor          0.09%            2.261ms          0.09%            2.261ms          2.261ms          1<NewLine>contiguous                   0.02%            410.239us        0.02%            450.143us        28.134us         16<NewLine>_empty_affine_quantized      0.01%            304.640us        0.01%            304.640us        17.920us         17<NewLine>max                          0.01%            160.672us        0.01%            160.672us        160.672us        1<NewLine>q_scale                      0.00%            113.983us        0.00%            113.983us        2.478us          46<NewLine>clone                        0.00%            102.719us        0.00%            102.719us        102.719us        1<NewLine>dequantize                   0.00%            50.016us         0.00%            50.016us         50.016us         1<NewLine>q_zero_point                 0.00%            45.728us         0.00%            45.728us         1.524us          30<NewLine>view                         0.00%            44.704us         0.00%            44.704us         44.704us         1<NewLine>max_pool2d                   0.00%            36.128us         0.40%            9.527ms          1.905ms          5<NewLine>select                       0.00%            31.680us         0.00%            31.680us         31.680us         1<NewLine>reshape                      0.00%            30.208us         0.01%            195.071us        97.535us         2<NewLine>_unsafe_view                 0.00%            17.440us         0.00%            17.440us         17.440us         1<NewLine>empty_like                   0.00%            13.888us         0.00%            39.904us         39.904us         1<NewLine>_local_scalar_dense          0.00%            13.504us         0.00%            13.504us         4.501us          3<NewLine>is_floating_point            0.00%            13.440us         0.00%            13.440us         13.440us         1<NewLine>item                         0.00%            12.448us         0.00%            25.952us         8.651us          3<NewLine>flatten                      0.00%            7.456us          0.01%            138.463us        138.463us        1<NewLine>adaptive_avg_pool2d          0.00%            5.312us          0.64%            15.376ms         15.376ms         1<NewLine>dropout                      0.00%            5.216us          0.00%            5.216us          2.608us          2<NewLine>qscheme                      0.00%            4.736us          0.00%            4.736us          4.736us          1<NewLine>is_complex                   0.00%            3.360us          0.00%            3.360us          3.360us          1<NewLine>sizes                        0.00%            2.656us          0.00%            2.656us          2.656us          1<NewLine>size                         0.00%            2.240us          0.00%            2.240us          2.240us          1<NewLine>---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jungmo_ahn"">@Jungmo_Ahn</a> I met the same problem, have you solve it?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am still trying to solve it. If there is meaningful result, I will share it here.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/huoge; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/anguoyang; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  8:57pm; <NewLine> REPLY_DATE 2: June 26, 2020,  9:00pm; <NewLine> REPLY_DATE 3: June 27, 2020, 10:44am; <NewLine> REPLY_DATE 4: June 28, 2020,  7:23am; <NewLine> REPLY_DATE 5: June 29, 2020,  4:51pm; <NewLine> REPLY_DATE 6: June 29, 2020,  5:13pm; <NewLine> REPLY_DATE 7: June 29, 2020, 11:53pm; <NewLine> REPLY_DATE 8: June 30, 2020, 12:36am; <NewLine> REPLY_DATE 9: June 30, 2020,  1:07am; <NewLine> REPLY_DATE 10: July 1, 2020,  2:13am; <NewLine> REPLY_DATE 11: July 1, 2020,  2:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
87405,Resnet18 model has multiple quantized results that are inconsistent,2020-06-30T07:34:58.721Z,1,107,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Below is my quantification process：</p><NewLine><p>class quantizeModel(object):</p><NewLine><pre><code>def __init__(self):<NewLine>    super(quantizeModel, self).__init__()<NewLine>    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')<NewLine>    self.train_loader, self.test_loader = get_imagenet()<NewLine>    self.quant()<NewLine><NewLine>def quant(self):<NewLine><NewLine>    model = self.load_model()<NewLine>    model.eval()<NewLine>    self.print_size_of_model(model)<NewLine>    self.validate(model, ""original_resnet18"", self.test_loader)<NewLine>    model.fuse_model()<NewLine><NewLine>    self.print_size_of_model(model)<NewLine>    self.quantize(model)<NewLine><NewLine>def load_model(self):<NewLine>    model = resnet18()<NewLine>    state_dict = torch.load(""CIFAR10_resnet18.pth"", map_location=self.device)<NewLine>    model.load_state_dict(state_dict)<NewLine>    model.to(self.device)<NewLine>    return model<NewLine><NewLine>def print_size_of_model(self, model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"") / 1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>def validate(self, model, name, data_loader):<NewLine>    with torch.no_grad():<NewLine>        correct = 0<NewLine>        total = 0<NewLine>        acc = 0<NewLine>        for data in data_loader:<NewLine>            images, labels = data<NewLine>            images, labels = images.to(self.device), labels.to(self.device)<NewLine>            output = model(images)<NewLine><NewLine>            _, predicted = torch.max(output, dim=1)<NewLine>            total += labels.size(0)<NewLine>            correct += (predicted == labels).sum().item()<NewLine>            if total == 1024: <NewLine>                break<NewLine>        acc = round(100 * correct / total, 3)<NewLine>        print('{{""metric"": ""{}_val_accuracy"", ""value"": {}%}}'.format(name, acc))<NewLine>        return acc<NewLine><NewLine>def quantize(self, model):<NewLine>    #model.qconfig = torch.quantization.default_qconfig<NewLine>    #model.qconfig = torch.quantization.default_per_channel_qconfig<NewLine><NewLine>    model.qconfig = torch.quantization.QConfig(<NewLine>        activation=torch.quantization.observer.MinMaxObserver.with_args(reduce_range=True),<NewLine>        weight=torch.quantization.observer.PerChannelMinMaxObserver.with_args(dtype=torch.qint8,<NewLine>                                                                              qscheme=torch.per_channel_affine))<NewLine>    pmodel = torch.quantization.prepare(model)<NewLine><NewLine>    #calibration<NewLine>    self.validate(pmodel, ""quntize_per_channel_resent18_train"", self.train_loader)<NewLine>    qmodel = torch.quantization.convert(pmodel)<NewLine><NewLine>    self.validate(qmodel, ""quntize_per_chaannel_resent18_test"", self.test_loader)<NewLine>    self.print_size_of_model(qmodel)<NewLine><NewLine>    torch.jit.save(torch.jit.script(qmodel), ""quantization_per_channel_model18.pth"")<NewLine></code></pre><NewLine><p>The program is executed three times, and three different sets of quantitative results are obtained, as follows：<br/><NewLine>１．<br/><NewLine>Size (MB): 44.786115<br/><NewLine>{“metric”: “original_resnet18_val_accuracy”, “value”: <strong>75.098%</strong>}<br/><NewLine>Size (MB): 44.717413<br/><NewLine>{“metric”: “quntize_per_channel_resent18_train_val_accuracy”, “value”: 46.387%}<br/><NewLine>{“metric”: “quntize_per_chaannel_resent18_test_val_accuracy”, “value”: <strong>75.586%</strong>}<br/><NewLine>Size (MB): 11.290618<br/><NewLine>２．<br/><NewLine>Size (MB): 44.786115<br/><NewLine>{“metric”: “original_resnet18_val_accuracy”, “value”: <strong>75.098%</strong>}<br/><NewLine>Size (MB): 44.717413<br/><NewLine>{“metric”: “quntize_per_channel_resent18_train_val_accuracy”, “value”: 45.996%}<br/><NewLine>{“metric”: “quntize_per_chaannel_resent18_test_val_accuracy”, “value”: <strong>76.953%</strong>}<br/><NewLine>Size (MB): 11.290618<br/><NewLine>３．<br/><NewLine>Size (MB): 44.786115<br/><NewLine>{“metric”: “original_resnet18_val_accuracy”, “value”: <strong>75.098%</strong>}<br/><NewLine>Size (MB): 44.717413<br/><NewLine>{“metric”: “quntize_per_channel_resent18_train_val_accuracy”, “value”: 43.945%}<br/><NewLine>{“metric”: “quntize_per_chaannel_resent18_test_val_accuracy”, “value”: <strong>75.195%</strong>}<br/><NewLine>Size (MB): 11.290618</p><NewLine><p>I think the weight parameters are unchanged, the calibration data is unchanged, the quantization configuration algorithm is unchanged, and the result after quantization should also be unchanged, but the accuracy after three quantizations is different. What is the reason?</p><NewLine></div>",https://discuss.pytorch.org/u/blueskywwc,(Blueskywwc),blueskywwc,"June 30, 2020,  8:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>does it reproduce with <code>torch.manual_seed(0)</code>?  The pasted code should give the same results, perhaps the <code>get_imagenet</code> has some randomness?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for your reply，After torch.manual_seed(191009), the code gives the same result。</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/blueskywwc; <NewLine> ,"REPLY_DATE 1: June 30, 2020,  3:51pm; <NewLine> REPLY_DATE 2: July 1, 2020,  2:11am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
59966,Quantization Error During Concat &ndash; RuntimeError: Didn&rsquo;t find kernel to dispatch to for operator &lsquo;aten::_cat&rsquo;,2019-11-04T16:32:57.677Z,17,1332,"<div class=""post"" itemprop=""articleBody""><NewLine><p>During static quantization of my model, I encounter the following error -</p><NewLine><p>RuntimeError: Didn’t find kernel to dispatch to for operator ‘aten::_cat’. Tried to look up kernel for dispatch key ‘QuantizedCPUTensorId’. Registered dispatch keys are: [CPUTensorId, VariableTensorId]</p><NewLine><p>I have fused and quantized the model, as well as the input image. But it throws an error on concat raised by - y = torch.cat([sources[0], sources[1]], dim=1)</p><NewLine><p>Any suggestions would be appreciated.   <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/><br/><NewLine>Full code here -<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/experiments/blob/master/try_static_quant.py</a></h4><NewLine><pre><code class=""lang-py"">###  CRAFT model here - https://github.com/clovaai/CRAFT-pytorch/blob/master/craft.py<NewLine>import torch<NewLine>import os<NewLine>import time<NewLine>from craft import CRAFT<NewLine>import cv2<NewLine>import numpy as np<NewLine>import craft_utils<NewLine>import imgproc<NewLine>import file_utils<NewLine>from torch.autograd import Variable<NewLine>canvas_size=1280<NewLine>mag_ratio=1.5<NewLine>trained_model='craft_mlt_25k.pth'<NewLine>def uninplace(model):<NewLine>    if hasattr(model, 'inplace'):<NewLine>        model.inplace = False<NewLine>    if not model.children():<NewLine>        return<NewLine>    for child in model.children():<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/Raghav_Gurbaxani,(Raghav Gurbaxani),Raghav_Gurbaxani,"November 4, 2019,  4:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please see the usage of skip_add (+=  operation) here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture</a></p><NewLine><p>The operators listed here <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.QFunctional"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.QFunctional</a> should be replaced with their functional module counterpart in the network before post-training quantization.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>  thank you for your suggestion.  <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine><p>I replaced the ‘cat’ modules with n.quantized.FloatFunctional().cat()</p><NewLine><p>But I run into another error -<br/><NewLine><strong>TypeError: NumPy conversion for Variable[QuantizedCPUQUInt8Type] is not supported</strong></p><NewLine><p>from the line y[0,:,:,0].cpu().data.numpy()<br/><NewLine>(line 52 in the link above)</p><NewLine><p>How do I convert this quantized variable to numpy ?  Thanks again for your help.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are a couple of options depending on what you want:</p><NewLine><p>If you want quantized integer data, use int_repr (<a href=""https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor</a>)</p><NewLine><p>If you want float data, dequantize and use your existing way of converting it to numpy.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a><br/><NewLine>Thank you very much, I changed<br/><NewLine>score_link = y[0,:,:,1].cpu().data.numpy() to score_link = y[0,:,:,1].int_repr().cpu().data.numpy()  as per your suggestion. But the prediction is very bad.</p><NewLine><p>Can you point me to how to dequantize the model ?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is the final prediction. Correct? If yes, you would need to dequantize the final tensor, .e.g, using dequantized_y = y.dequantize()</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a><br/><NewLine>I tried both methods. Final prediction is really bad   <img alt="":frowning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/frowning.png?v=9"" title="":frowning:""/></p><NewLine><p>Original Prediction from FP32 model -<br/><NewLine><img alt=""image"" data-base62-sha1=""srf1d3fMvsfeNSXdiQC5HJJ28QV"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/7/c751371bae97b455047d1077fe8f926bcb825a51.jpeg"" width=""450""/></p><NewLine><p>Prediction from INT8 model -<br/><NewLine><img alt=""image"" data-base62-sha1=""c3Tm7enUCpVGGk3aDJVw7qRwKBV"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/4/548abd74691d9488395a8a4e8307a4ed987c37b7.jpeg"" width=""450""/></p><NewLine><p>Not sure where I’m going wrong <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>You may want to try some quantization accuracy improvement techniques such as</p><NewLine><p>per channel quantization for weights<br/><NewLine>Quantization aware training<br/><NewLine>Measuring torch.norm between float model and quantize model to see where it’s off the most.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>is there an example for per channel quantization and measuring the torch norm between the 2 models ?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>For per channel see <a href=""https://github.com/pytorch/tutorials/blob/master/advanced_source/static_quantization_tutorial.py"" rel=""nofollow noopener"">https://github.com/pytorch/tutorials/blob/master/advanced_source/static_quantization_tutorial.py</a></p><NewLine><p>and for norm you can use something like the following:</p><NewLine><pre><code class=""lang-auto"">SQNR = []<NewLine>for i in range(len(ref_output)):<NewLine>   <NewLine>    SQNR.append(20*torch.log10(torch.norm(ref_output[i][0])/torch.norm(ref_output[i][0]-qtz_output[i][0])).numpy())<NewLine><NewLine>print('SQNR (dB)', SQNR)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a><br/><NewLine>The performance improved slightly after per channel quantization, but it is still very bad</p><NewLine><p><img alt=""image"" data-base62-sha1=""4pBJHELHcSTBcan6N4xAPehFTDJ"" height=""250"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/e/1eedc44df1994aec7afddc82ee395e035370f443.jpeg"" width=""300""/></p><NewLine><p>Do you think I should try float 16 instead? If so, how do I change the config to change it to Float16.</p><NewLine><p>Also, in your earlier response ref_output is the output from the net ?  i.e ref_output=net(x), is that what you meant ?</p><NewLine><p>Thanks again for your help , hope I can resolve this problem</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Float16 quantized operators do not exist for static quantization. Since current cpus do not support float16 compute natively, converting to float16 for compute bound cases doesn’t provide much performance benefits.</p><NewLine><p>ref_output is from the float model. You might want to check the norm at few different places in the network to see where we are deviating too much from floating point results.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>In PyTorch there’s a way to compare the module level quantization error, which could help to debug and narrow down the issue. I’m working on an example and will post here later.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/raghav_gurbaxani"">@Raghav_Gurbaxani</a>, have you tried using histogram observer for activation? In most cases this could improve the accuracy of the quantized model. You can do:<br/><NewLine>model.qconfig = torch.quantization.QConfig(<br/><NewLine>activation=torch.quantization.default_histogram_observer,<br/><NewLine>weight=torch.quantization.default_per_channel_weight_observer)</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks <a class=""mention"" href=""/u/hx89"">@hx89</a> , if you could post that example for compare module level quantization error - It would be great <img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine><p>In the meantime, I tried the histogram observer and the result is still pretty bad  <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/><br/><NewLine><img alt=""image"" data-base62-sha1=""4pBJHELHcSTBcan6N4xAPehFTDJ"" height=""250"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/e/1eedc44df1994aec7afddc82ee395e035370f443.jpeg"" width=""300""/></p><NewLine><p>any other suggestions ?</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you checked the accuracy of fused_model? By checking the accuracy of fused_model before converting to int8 model we can know if the issue is in the preprocessing part or in the quantized model.</p><NewLine><p>If fused_model has good accuracy, the next step we can check the quantization error of the weights. Could you try the following code:</p><NewLine><pre><code class=""lang-auto"">def l2_error(ref_tensor, new_tensor):<NewLine>    """"""Compute the l2 error between two tensors.<NewLine><NewLine>    Args:<NewLine>        ref_tensor (numpy array): Reference tensor.<NewLine>        new_tensor (numpy array): New tensor to compare with.<NewLine><NewLine>    Returns:<NewLine>        abs_error: l2 error<NewLine>        relative_error: relative l2 error<NewLine>    """"""<NewLine>    assert (<NewLine>        ref_tensor.shape == new_tensor.shape<NewLine>    ), ""The shape between two tensors is different""<NewLine><NewLine>    diff = new_tensor - ref_tensor<NewLine>    abs_error = np.linalg.norm(diff)<NewLine>    ref_norm = np.linalg.norm(ref_tensor)<NewLine>    if ref_norm == 0:<NewLine>        if np.allclose(ref_tensor, new_tensor):<NewLine>            relative_error = 0<NewLine>        else:<NewLine>            relative_error = np.inf<NewLine>    else:<NewLine>        relative_error = np.linalg.norm(diff) / ref_norm<NewLine>    return abs_error, relative_error<NewLine><NewLine>float_model_dbg = fused_model<NewLine>qmodel_dbg = quantized<NewLine><NewLine>for key in float_model_dbg.state_dict().keys():<NewLine>    float_w = float_model_dbg.state_dict()[key]<NewLine>    qkey = key<NewLine>    <NewLine>    # Get rid of extra hiearchy of the fused Conv in float model<NewLine>    if key.endswith('.weight'):<NewLine>        qkey = key[:-9] + key[-7:] <NewLine><NewLine>    if qkey in qmodel_dbg.state_dict():<NewLine>        q_w = qmodel_dbg.state_dict()[qkey]<NewLine>        if q_w.dtype == torch.float:<NewLine>            abs_error, relative_error = l2_error(float_w.numpy(), q_w.detach().numpy())<NewLine>        else:<NewLine>            abs_error, relative_error = l2_error(float_w.numpy(), q_w.dequantize().numpy())<NewLine>        print(key, ', abs error = ', abs_error, "", relative error = "", relative_error)<NewLine></code></pre><NewLine><p>It should print out the quantization error for each Conv weight such as:</p><NewLine><pre><code class=""lang-auto"">features.0.0.weight , abs error =  0.21341866 , relative error =  0.01703797<NewLine>features.3.squeeze.0.weight , abs error =  0.095942035 , relative error =  0.012483358<NewLine>features.3.expand1x1.0.weight , abs error =  0.071949296 , relative error =  0.010309489<NewLine>features.3.expand3x3.0.weight , abs error =  0.18284422 , relative error =  0.025256516<NewLine>features.4.squeeze.0.weight , abs error =  0.088713735 , relative error =  0.011313644<NewLine>features.4.expand1x1.0.weight , abs error =  0.0780085 , relative error =  0.0126931975<NewLine>...<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hx89"">@hx89</a> the performance of the fused model is good</p><NewLine><p><img alt=""image"" data-base62-sha1=""srf1d3fMvsfeNSXdiQC5HJJ28QV"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/7/c751371bae97b455047d1077fe8f926bcb825a51.jpeg"" width=""450""/></p><NewLine><p>That means <strong>there’s something wrong on the quantization side</strong>, not the fusion side.  <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine><p>Here’s the log of the relative norm errors -<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/raghavgurbaxani/experiments/blob/master/quantization_error.txt"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/raghavgurbaxani/experiments/blob/master/quantization_error.txt"" rel=""nofollow noopener"" target=""_blank"">raghavgurbaxani/experiments/blob/master/quantization_error.txt</a></h4><NewLine><pre><code class=""lang-txt"">basenet.slice1.3.0.weight , abs error =  0.07433768 , relative error =  0.014456971<NewLine>basenet.slice1.7.0.weight , abs error =  0.102016546 , relative error =  0.012097403<NewLine>basenet.slice1.10.0.weight , abs error =  0.14640729 , relative error =  0.0126273325<NewLine>basenet.slice2.14.0.weight , abs error =  0.13131897 , relative error =  0.011922532<NewLine>basenet.slice2.17.0.weight , abs error =  0.17593716 , relative error =  0.011550295<NewLine>basenet.slice3.20.0.weight , abs error =  0.21453191 , relative error =  0.011749155<NewLine>basenet.slice3.24.0.weight , abs error =  0.29290414 , relative error =  0.012482245<NewLine>basenet.slice3.27.0.weight , abs error =  0.5628253 , relative error =  0.011207958<NewLine>basenet.slice4.30.0.weight , abs error =  0.19498727 , relative error =  0.010849289<NewLine>basenet.slice4.34.0.weight , abs error =  0.53952134 , relative error =  0.010825124<NewLine>basenet.slice4.37.bias , abs error =  0.0 , relative error =  0.0<NewLine>basenet.slice5.1.bias , abs error =  0.0 , relative error =  0.0<NewLine>basenet.slice5.2.bias , abs error =  0.0 , relative error =  0.0<NewLine>upconv1.conv.0.0.weight , abs error =  0.064128004 , relative error =  0.009871936<NewLine>upconv1.conv.3.0.weight , abs error =  0.13272223 , relative error =  0.011580461<NewLine>upconv2.conv.0.0.weight , abs error =  0.36004254 , relative error =  0.00813957<NewLine>upconv2.conv.3.0.weight , abs error =  0.11151659 , relative error =  0.010093152<NewLine>upconv3.conv.0.0.weight , abs error =  0.19112265 , relative error =  0.0072073564<NewLine>upconv3.conv.3.0.weight , abs error =  0.07007974 , relative error =  0.008535749<NewLine>upconv4.conv.0.0.weight , abs error =  0.0805448 , relative error =  0.0067224735<NewLine></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/raghavgurbaxani/experiments/blob/master/quantization_error.txt"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Can you suggest what to do next ? Is there any way to reduce these errors ? Apart from QAT ofcourse</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Looks like the first Conv basenet.slice1.3.0.weight has the largest error, could you try skipping the quantization of that Conv and keep it as the float module? We have previously seen some CV models’s first Conv is sensitive to quantization and skipping it would give better accuracy.</p><NewLine></div>; <NewLine> REPLY 18: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/hx89"">@hx89</a> actually it seems like all these have pretty high relative errors -<br/><NewLine>[ basenet.slice1.7.0.weight , basenet.slice1.10.0.weight , basenet.slice2.14.0.weight , basenet.slice2.17.0.weight , basenet.slice3.20.0.weight ,basenet.slice3.24.0.weight , basenet.slice3.27.0.weight , basenet.slice4.30.0.weight ,basenet.slice4.34.0.weight ]</p><NewLine><p>although that seems like a good idea, keeping a few layers as float while converting the rest to int8.</p><NewLine><p>I am not sure how to pass the partial model to torch.quantization.convert() for quantization and then combining the partially quantized model and unquantized layers together for inference on the image.</p><NewLine><p>Could you provide an example ? Thanks a ton</p><NewLine></div>; <NewLine> REPLY 19: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s actually simpler, to skip the first conv for example, there are two step:</p><NewLine><p>Step 1: Move the quant stub after the first conv in the forward function of the module.</p><NewLine><p>For example in the original quantizable module, quant stub is at the beginning before conv1:</p><NewLine><pre><code class=""lang-auto"">Class QuantizableNet(nn.Module):<NewLine>    def __init__(self):<NewLine>        ...<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.conv1(x)<NewLine>        x = self.maxpool(x)<NewLine>        x = self.fc(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>To skip the quantization of conv1 we can move self.quant() aftert conv1:</p><NewLine><pre><code class=""lang-auto"">Class QuantizableNet(nn.Module):<NewLine>    def __init__(self):<NewLine>        ...<NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = self.quant(x)<NewLine>        x = self.maxpool(x)<NewLine>        x = self.fc(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine></code></pre><NewLine><p>Step 2: Then we need to set the qconfig of conv1 to None after prepare(), this way PyTorch knows we want to keep conv1 as float module and won’t swap it with quantized module:</p><NewLine><pre><code class=""lang-auto"">model = QuantizableNet()<NewLine>...<NewLine>torch.quantization.prepare(model)<NewLine>model.conv1.qconfig = None<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 18: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 19: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: November 4, 2019,  5:31pm; <NewLine> REPLY_DATE 2: November 4, 2019,  8:45pm; <NewLine> REPLY_DATE 3: November 4, 2019,  9:40pm; <NewLine> REPLY_DATE 4: November 4, 2019,  9:52pm; <NewLine> REPLY_DATE 5: November 4, 2019,  9:54pm; <NewLine> REPLY_DATE 6: November 4, 2019, 10:10pm; <NewLine> REPLY_DATE 7: November 5, 2019, 12:09am; <NewLine> REPLY_DATE 8: November 5, 2019, 12:26am; <NewLine> REPLY_DATE 9: November 5, 2019,  2:05am; <NewLine> REPLY_DATE 10: November 5, 2019,  6:15pm; <NewLine> REPLY_DATE 11: November 5, 2019,  6:31pm; <NewLine> REPLY_DATE 12: November 5, 2019,  7:56pm; <NewLine> REPLY_DATE 13: November 5, 2019,  8:43pm; <NewLine> REPLY_DATE 14: November 5, 2019,  8:58pm; <NewLine> REPLY_DATE 15: November 5, 2019, 10:02pm; <NewLine> REPLY_DATE 16: November 5, 2019, 10:53pm; <NewLine> REPLY_DATE 17: November 5, 2019, 11:50pm; <NewLine> REPLY_DATE 18: November 5, 2019, 11:57pm; <NewLine> REPLY_DATE 19: November 6, 2019,  1:17am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: 1 Like; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> REPLY 18 LIKES: ; <NewLine> REPLY 19 LIKES: 1 Like; <NewLine> 
60008,Quantized model consists of ReLU6,2019-11-05T02:29:35.257Z,2,436,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, Can I quantize model which consists of ReLU6 activation function?</p><NewLine></div>",https://discuss.pytorch.org/u/thancaocuong,(cuongtc),thancaocuong,"November 5, 2019,  2:29am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes you can. ReLU6 was added to DEFAULT_MODULE_MAPPING. See <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/quantized-hard-sigmoid/59013"">Quantized hard sigmoid</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/pshashk"">@pshashk</a>, I did tried to fuse model with ReLU6 activation function. It throws an error. I see in pytorch source code that fuse modules currently support 4 types sequence of modules:</p><NewLine><pre><code class=""lang-auto"">  Fuses only the following sequence of modules:<NewLine>    conv, bn<NewLine>    conv, bn, relu<NewLine>    conv, relu<NewLine>    linear, relu<NewLine></code></pre><NewLine><p><a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/fuse_modules.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/fuse_modules.py</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>That is correct, we will work on adding support for fusing relu6 soon. For now, if you are doing post training quantization, you could replace relu6 with relu and proceed as a work around.<br/><NewLine>Thanks,</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you. Hope you release it soon</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> can you provide an example of how to replace relu6 with relu ? I am trying to quantize a network with relu6 activations.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/pshashk; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/thancaocuong; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/thancaocuong; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> ,"REPLY_DATE 1: November 13, 2019, 10:23am; <NewLine> REPLY_DATE 2: November 14, 2019,  7:44am; <NewLine> REPLY_DATE 3: November 18, 2019,  2:47am; <NewLine> REPLY_DATE 4: November 18, 2019,  2:47am; <NewLine> REPLY_DATE 5: June 29, 2020, 12:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
86928,Can quantizing models enable you to have bigger batch sizes during inference?,2020-06-25T17:31:47.484Z,0,66,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a model in production. I want to be able to increase my model’s throughput for speed reasons. I’ve tried quantizing the model but for some reason, if increase the batch size I still run into an OOM error. I thought that quantizing the model from fp32 to say fp16 would allow the bigger batch sizes? If that’s not the case what is the use case of quantizing models?</p><NewLine></div>",https://discuss.pytorch.org/u/miken,(Michael Nguyen),miken,"June 25, 2020,  5:32pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Quantizing model will enable to run the model at lower precision (int8) so it runs faster. Also since the tensors as quantized to 8 bit they will occupy less storage space as well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: June 26, 2020,  1:03am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86637,Slow inference on quantized MobileNetV3,2020-06-24T01:49:39.365Z,3,197,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have quantized a MobileNetV3-like Network with ‘qnnpack’ for use in an Android app. However, the quantized model is even slower than the original one.<br/><NewLine>All layers seem to be quantized correctly and the model file size decreased to 1/4 of the original size.</p><NewLine><p>The model has ~2M Parameters and input resolution is 224x224.<br/><NewLine>Here are some inference time numbers:<br/><NewLine>Model (without quantization) on Ryzen 3700x: ~50ms<br/><NewLine>Model (without quantization) on RTX 2070: ~6ms<br/><NewLine>Model (without quantization) on Huawei Mate 10 lite: ~1s<br/><NewLine>Model (with quantization) on Huawei Mate 10 lite: ~1.5s</p><NewLine><p>I did not expect that inference would take ~1s on such a model, even without quantization. Is this expected?<br/><NewLine>Why would a quantized model be slower? Are there any operations/layers/architecture conventions that should be absolutely avoided?</p><NewLine><p>Also, the output of the quantized model is extremely noisy. What could be causing this?<br/><NewLine>Here is an output example before and after model quantization:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/fd35d1fe452d9f531963a6c22d53f54288481cf1"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1.jpeg"" title=""noise""><img alt=""noise"" data-base62-sha1=""A806MK8Lh1jzKaWLIET940QXXBD"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1_2_10x10.png"" height=""235"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1_2_483x235.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1_2_483x235.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1_2_724x352.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/f/d/fd35d1fe452d9f531963a6c22d53f54288481cf1_2_966x470.jpeg 2x"" width=""483""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">noise</span><span class=""informations"">1500×732 260 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/singularity,,singularity,"June 24, 2020, 12:00pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/singularity"">@singularity</a> thanks for sharing. Is the entire network quantized or are there some layers running in float? If you can reproduce the behavior on server (using qnnpack) then you can use autograd profiler to get an op level breakdown to see which ops are causing the most slowdown.</p><NewLine><p>It might also be easier to debug accuracy issue on the server in case the quantization noise is reproducible there.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is the output from the autograd profiler before and after quantization:</p><NewLine><p><strong>Before</strong></p><NewLine><pre><code class=""lang-auto"">-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>Name                     Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes                         <NewLine>-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>conv2d                   0.67%            906.899us        0.67%            906.899us        906.899us        1                []                                   <NewLine>convolution              0.67%            902.559us        0.67%            902.559us        902.559us        1                []                                   <NewLine>_convolution             0.67%            900.649us        0.67%            900.649us        900.649us        1                []                                   <NewLine>contiguous               0.00%            1.060us          0.00%            1.060us          1.060us          1                []                                   <NewLine>contiguous               0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>contiguous               0.00%            0.120us          0.00%            0.120us          0.120us          1                []                                   <NewLine>mkldnn_convolution       0.66%            885.439us        0.66%            885.439us        885.439us        1                []                                   <NewLine>conv2d                   0.53%            711.017us        0.53%            711.017us        711.017us        1                []                                   <NewLine>convolution              0.53%            710.247us        0.53%            710.247us        710.247us        1                []                                   <NewLine>_convolution             0.52%            704.057us        0.52%            704.057us        704.057us        1                []                                   <NewLine>contiguous               0.00%            0.290us          0.00%            0.290us          0.290us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>mkldnn_convolution       0.52%            698.567us        0.52%            698.567us        698.567us        1                []                                   <NewLine>conv2d                   0.29%            389.754us        0.29%            389.754us        389.754us        1                []                                   <NewLine>convolution              0.29%            389.274us        0.29%            389.274us        389.274us        1                []                                   <NewLine>_convolution             0.29%            388.564us        0.29%            388.564us        388.564us        1                []                                   <NewLine>contiguous               0.00%            0.340us          0.00%            0.340us          0.340us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>mkldnn_convolution       0.29%            384.324us        0.29%            384.324us        384.324us        1                []                                   <NewLine>relu_                    0.02%            29.550us         0.02%            29.550us         29.550us         1                []                                   <NewLine>conv2d                   0.34%            454.195us        0.34%            454.195us        454.195us        1                []                                   <NewLine>convolution              0.34%            453.735us        0.34%            453.735us        453.735us        1                []                                   <NewLine>_convolution             0.34%            453.145us        0.34%            453.145us        453.145us        1                []                                   <NewLine>contiguous               0.00%            0.240us          0.00%            0.240us          0.240us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>mkldnn_convolution       0.33%            448.975us        0.33%            448.975us        448.975us        1                []                                   <NewLine>relu_                    0.02%            21.830us         0.02%            21.830us         21.830us         1                []                                   <NewLine>conv2d                   0.22%            291.363us        0.22%            291.363us        291.363us        1                []                                   <NewLine>convolution              0.22%            290.863us        0.22%            290.863us        290.863us        1                []                                   <NewLine>_convolution             0.22%            290.223us        0.22%            290.223us        290.223us        1                []                                   <NewLine>contiguous               0.00%            0.220us          0.00%            0.220us          0.220us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>contiguous               0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>mkldnn_convolution       0.21%            280.402us        0.21%            280.402us        280.402us        1                []                                   <NewLine>adaptive_avg_pool2d      0.04%            60.060us         0.04%            60.060us         60.060us         1                []                                   <NewLine>contiguous               0.00%            0.250us          0.00%            0.250us          0.250us          1                []                                   <NewLine>view                     0.00%            5.270us          0.00%            5.270us          5.270us          1                []                                   <NewLine>mean                     0.03%            44.300us         0.03%            44.300us         44.300us         1                []                                   <NewLine>view                     0.00%            1.870us          0.00%            1.870us          1.870us          1                []                                   <NewLine>view                     0.00%            1.690us          0.00%            1.690us          1.690us          1                []                                   <NewLine>unsigned short           0.00%            5.701us          0.00%            5.701us          5.701us          1                []                                   <NewLine>matmul                   0.03%            40.580us         0.03%            40.580us         40.580us         1                []                                   <NewLine>mm                       0.03%            34.970us         0.03%            34.970us         34.970us         1                []                                   <NewLine>relu_                    0.00%            3.950us          0.00%            3.950us          3.950us          1                []                                   <NewLine>unsigned short           0.00%            2.340us          0.00%            2.340us          2.340us          1                []                                   <NewLine>matmul                   0.00%            4.830us          0.00%            4.830us          4.830us          1                []                                   <NewLine>mm                       0.00%            4.360us          0.00%            4.360us          4.360us          1                []                                   <NewLine>sigmoid                  0.01%            13.000us         0.01%            13.000us         13.000us         1                []                                   <NewLine>view                     0.00%            2.561us          0.00%            2.561us          2.561us          1                []                                   <NewLine>expand_as                0.00%            4.660us          0.00%            4.660us          4.660us          1                []                                   <NewLine>expand                   0.00%            3.220us          0.00%            3.220us          3.220us          1                []                                   <NewLine>mul                      0.02%            21.070us         0.02%            21.070us         21.070us         1                []                                   <NewLine>relu_                    0.01%            8.960us          0.01%            8.960us          8.960us          1                []                                   <NewLine>conv2d                   0.21%            286.703us        0.21%            286.703us        286.703us        1                []                                   <NewLine>convolution              0.21%            286.053us        0.21%            286.053us        286.053us        1                []                                   <NewLine>_convolution             0.21%            285.113us        0.21%            285.113us        285.113us        1                []                                   <NewLine>contiguous               0.00%            0.230us          0.00%            0.230us          0.230us          1                []                                   <NewLine>contiguous               0.01%            17.500us         0.01%            17.500us         17.500us         1                []                                   <NewLine>contiguous               0.00%            0.200us          0.00%            0.200us          0.200us          1                []                                   <NewLine>mkldnn_convolution       0.20%            263.112us        0.20%            263.112us        263.112us        1                []                                   <NewLine>conv2d                   0.33%            443.374us        0.33%            443.374us        443.374us        1                []                                   <NewLine>convolution              0.33%            442.864us        0.33%            442.864us        442.864us        1                []                                   <NewLine>_convolution             0.33%            442.304us        0.33%            442.304us        442.304us        1                []                                   <NewLine>contiguous               0.00%            0.260us          0.00%            0.260us          0.260us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>mkldnn_convolution       0.33%            438.134us        0.33%            438.134us        438.134us        1                []                                   <NewLine>relu_                    0.02%            27.920us         0.02%            27.920us         27.920us         1                []                                   <NewLine>conv2d                   0.23%            310.863us        0.23%            310.863us        310.863us        1                []                                   <NewLine>convolution              0.23%            310.383us        0.23%            310.383us        310.383us        1                []                                   <NewLine>_convolution             0.23%            309.743us        0.23%            309.743us        309.743us        1                []                                   <NewLine>contiguous               0.00%            0.230us          0.00%            0.230us          0.230us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>contiguous               0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>mkldnn_convolution       0.23%            305.503us        0.23%            305.503us        305.503us        1                []                                   <NewLine>relu_                    0.01%            14.660us         0.01%            14.660us         14.660us         1                []                                   <NewLine>conv2d                   0.19%            261.423us        0.19%            261.423us        261.423us        1                []                                   <NewLine>convolution              0.19%            260.713us        0.19%            260.713us        260.713us        1                []                                   <NewLine>_convolution             0.19%            255.493us        0.19%            255.493us        255.493us        1                []                                   <NewLine>contiguous               0.00%            0.260us          0.00%            0.260us          0.260us          1                []                                   <NewLine>contiguous               0.00%            0.120us          0.00%            0.120us          0.120us          1                []                                   <NewLine>contiguous               0.00%            0.120us          0.00%            0.120us          0.120us          1                []                                   <NewLine>mkldnn_convolution       0.19%            250.603us        0.19%            250.603us        250.603us        1                []                                   <NewLine>conv2d                   0.20%            263.663us        0.20%            263.663us        263.663us        1                []                                   <NewLine>convolution              0.20%            263.183us        0.20%            263.183us        263.183us        1                []                                   <NewLine>_convolution             0.20%            262.683us        0.20%            262.683us        262.683us        1                []                                   <NewLine>contiguous               0.00%            0.280us          0.00%            0.280us          0.280us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>contiguous               0.00%            0.090us          0.00%            0.090us          0.090us          1                []                                   <NewLine>mkldnn_convolution       0.19%            258.533us        0.19%            258.533us        258.533us        1                []                                   <NewLine>relu_                    0.01%            13.400us         0.01%            13.400us         13.400us         1                []                                   <NewLine>conv2d                   0.15%            196.892us        0.15%            196.892us        196.892us        1                []                                   <NewLine>convolution              0.15%            196.412us        0.15%            196.412us        196.412us        1                []                                   <NewLine>_convolution             0.15%            195.812us        0.15%            195.812us        195.812us        1                []                                   <NewLine>contiguous               0.00%            0.240us          0.00%            0.240us          0.240us          1                []                                   <NewLine>contiguous               0.00%            0.110us          0.00%            0.110us          0.110us          1                []                                   <NewLine>contiguous               0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>Self CPU time total: 134.621ms<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>hit the character limit…</p><NewLine><p><strong>After</strong></p><NewLine><pre><code class=""lang-auto"">---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>Name                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes                         <NewLine>---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>item                         0.01%            7.420us          0.01%            7.420us          7.420us          1                []                                   <NewLine>_local_scalar_dense          0.01%            4.550us          0.01%            4.550us          4.550us          1                []                                   <NewLine>aten::Int                    0.00%            1.720us          0.00%            1.720us          1.720us          1                []                                   <NewLine>item                         0.00%            0.470us          0.00%            0.470us          0.470us          1                []                                   <NewLine>_local_scalar_dense          0.00%            0.240us          0.00%            0.240us          0.240us          1                []                                   <NewLine>quantize_per_tensor          0.09%            69.811us         0.09%            69.811us         69.811us         1                []                                   <NewLine>quantized::conv2d            1.53%            1.183ms          1.53%            1.183ms          1.183ms          1                []                                   <NewLine>contiguous                   0.22%            167.211us        0.22%            167.211us        167.211us        1                []                                   <NewLine>empty_like                   0.01%            9.810us          0.01%            9.810us          9.810us          1                []                                   <NewLine>qscheme                      0.00%            0.920us          0.00%            0.920us          0.920us          1                []                                   <NewLine>q_zero_point                 0.00%            0.710us          0.00%            0.710us          0.710us          1                []                                   <NewLine>q_scale                      0.00%            0.740us          0.00%            0.740us          0.740us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            2.930us          0.00%            2.930us          2.930us          1                []                                   <NewLine>q_scale                      0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>contiguous                   0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.440us          0.00%            1.440us          1.440us          1                []                                   <NewLine>quantize_per_tensor          0.01%            5.550us          0.01%            5.550us          5.550us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.320us          0.00%            1.320us          1.320us          1                []                                   <NewLine>q_zero_point                 0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>q_scale                      0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_zero_point                 0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_scale                      0.00%            0.130us          0.00%            0.130us          0.130us          1                []                                   <NewLine>quantized::conv2d            0.34%            266.002us        0.34%            266.002us        266.002us        1                []                                   <NewLine>contiguous                   0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>q_scale                      0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>contiguous                   0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.260us          0.00%            1.260us          1.260us          1                []                                   <NewLine>quantize_per_tensor          0.01%            4.290us          0.01%            4.290us          4.290us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.180us          0.00%            1.180us          1.180us          1                []                                   <NewLine>q_zero_point                 0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>q_scale                      0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_zero_point                 0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>q_scale                      0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>quantized::conv2d_relu       1.11%            856.897us        1.11%            856.897us        856.897us        1                []                                   <NewLine>contiguous                   0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>q_scale                      0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>contiguous                   0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.260us          0.00%            1.260us          1.260us          1                []                                   <NewLine>quantize_per_tensor          0.01%            4.370us          0.01%            4.370us          4.370us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.270us          0.00%            1.270us          1.270us          1                []                                   <NewLine>q_zero_point                 0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>q_scale                      0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_zero_point                 0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_scale                      0.00%            0.130us          0.00%            0.130us          0.130us          1                []                                   <NewLine>quantized::conv2d_relu       0.49%            378.753us        0.49%            378.753us        378.753us        1                []                                   <NewLine>contiguous                   0.00%            0.200us          0.00%            0.200us          0.200us          1                []                                   <NewLine>q_scale                      0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>contiguous                   0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.290us          0.00%            1.290us          1.290us          1                []                                   <NewLine>quantize_per_tensor          0.01%            4.700us          0.01%            4.700us          4.700us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.260us          0.00%            1.260us          1.260us          1                []                                   <NewLine>q_zero_point                 0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>q_scale                      0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_zero_point                 0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>q_scale                      0.00%            0.130us          0.00%            0.130us          0.130us          1                []                                   <NewLine>quantized::conv2d            0.18%            140.401us        0.18%            140.401us        140.401us        1                []                                   <NewLine>contiguous                   0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>q_scale                      0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>contiguous                   0.00%            0.100us          0.00%            0.100us          0.100us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.060us          0.00%            1.060us          1.060us          1                []                                   <NewLine>quantize_per_tensor          0.01%            3.920us          0.01%            3.920us          3.920us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.260us          0.00%            1.260us          1.260us          1                []                                   <NewLine>q_zero_point                 0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>q_scale                      0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>q_zero_point                 0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_scale                      0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>size                         0.00%            0.970us          0.00%            0.970us          0.970us          1                []                                   <NewLine>size                         0.00%            0.190us          0.00%            0.190us          0.190us          1                []                                   <NewLine>adaptive_avg_pool2d          0.02%            16.650us         0.02%            16.650us         16.650us         1                []                                   <NewLine>_adaptive_avg_pool2d         0.02%            14.410us         0.02%            14.410us         14.410us         1                []                                   <NewLine>view                         0.01%            4.641us          0.01%            4.641us          4.641us          1                []                                   <NewLine>quantized::linear            0.02%            14.480us         0.02%            14.480us         14.480us         1                []                                   <NewLine>contiguous                   0.00%            0.160us          0.00%            0.160us          0.160us          1                []                                   <NewLine>q_scale                      0.00%            0.340us          0.00%            0.340us          0.340us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.010us          0.00%            1.010us          1.010us          1                []                                   <NewLine>quantize_per_tensor          0.01%            4.510us          0.01%            4.510us          4.510us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            0.930us          0.00%            0.930us          0.930us          1                []                                   <NewLine>q_scale                      0.00%            0.220us          0.00%            0.220us          0.220us          1                []                                   <NewLine>q_zero_point                 0.00%            0.200us          0.00%            0.200us          0.200us          1                []                                   <NewLine>relu_                        0.00%            3.630us          0.00%            3.630us          3.630us          1                []                                   <NewLine>quantized::linear            0.01%            10.470us         0.01%            10.470us         10.470us         1                []                                   <NewLine>contiguous                   0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>q_scale                      0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            0.850us          0.00%            0.850us          0.850us          1                []                                   <NewLine>quantize_per_tensor          0.01%            4.310us          0.01%            4.310us          4.310us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            0.810us          0.00%            0.810us          0.810us          1                []                                   <NewLine>q_scale                      0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>q_zero_point                 0.00%            0.150us          0.00%            0.150us          0.150us          1                []                                   <NewLine>sigmoid                      0.01%            7.770us          0.01%            7.770us          7.770us          1                []                                   <NewLine>view                         0.00%            1.290us          0.00%            1.290us          1.290us          1                []                                   <NewLine>expand_as                    0.01%            5.330us          0.01%            5.330us          5.330us          1                []                                   <NewLine>expand                       0.01%            4.190us          0.01%            4.190us          4.190us          1                []                                   <NewLine>quantized::mul               0.20%            152.521us        0.20%            152.521us        152.521us        1                []                                   <NewLine>qscheme                      0.00%            0.290us          0.00%            0.290us          0.290us          1                []                                   <NewLine>qscheme                      0.00%            0.180us          0.00%            0.180us          0.180us          1                []                                   <NewLine>qscheme                      0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>_empty_affine_quantized      0.00%            1.380us          0.00%            1.380us          1.380us          1                []                                   <NewLine>q_zero_point                 0.00%            0.170us          0.00%            0.170us          0.170us          1                []                                   <NewLine>q_scale                      0.00%            0.200us          0.00%            0.200us          0.200us          1                []                                   <NewLine>q_zero_point                 0.00%            0.140us          0.00%            0.140us          0.140us          1                []                                   <NewLine>---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  <NewLine>Self CPU time total: 77.276ms<NewLine></code></pre><NewLine><p>Most of the time is spent in Conv2d/ReLU operations, but they are quantized. So it seems that quantization is indeed working as Desktop CPU time decreases from 134ms to 77ms.<br/><NewLine>However, when I run the quantized model on my mobile device (Huawei Mate 10 lite), there are no performance gains. Any ideas?</p><NewLine><p>Also, I found that the noise is likely caused by this qnnpack bug <a href=""https://github.com/pytorch/pytorch/issues/36253"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/36253</a>. When I train my model only for a few epochs, I can quantize the model without any errors. However, when I fully train the model, these errors: “output scale: convolution scale 4.636909 is greater or equal to 1.0” are thrown during quantization and the model is extremely noisy after quantization. Is there a fix for this yet?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Regarding the performance, could you set the number of threads to 1 and see if it is still slower?</p><NewLine><p>Regarding the noise - Could you try with pytorch nightly build? There was a fix for the scale issue as mentioned here - <a href=""https://github.com/pytorch/pytorch/issues/33466#issuecomment-627660191"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33466#issuecomment-627660191</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Upgrading to PyTorch Nightly fixed the errors and the output is looking much better now!</p><NewLine><p><strong>Edit:</strong> I spoke a bit too soon… there is also something wrong with the calibration. It seems that calibration is actually hurting performance. Here is the output of the quantized model after 1,10,100 and 1000 calibration images:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/6c77476758c47649315dd869b8f970da4db7ca89"" href=""https://discuss.pytorch.org/uploads/default/original/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89.png"" title=""calibration""><img alt=""calibration"" data-base62-sha1=""ftx7u2m9ASdWKGYv3SkFBQkjiyt"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89_2_10x10.png"" height=""135"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89_2_552x135.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89_2_552x135.png, https://discuss.pytorch.org/uploads/default/optimized/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89_2_828x202.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/6/c/6c77476758c47649315dd869b8f970da4db7ca89_2_1104x270.png 2x"" width=""552""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">calibration</span><span class=""informations"">1600×394 318 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div><br/><NewLine>This is what it should look like:<br/><NewLine>(output before quantization)<br/><NewLine><img alt=""Figure_1"" data-base62-sha1=""58wECf2P2OwnSTR2qxZnDzvco2J"" height=""140"" src=""https://discuss.pytorch.org/uploads/default/original/3X/2/4/24019e29c53aaec75cb6ea18401df2d374940f79.png"" width=""140""/></p><NewLine><p>I have tried setting the number of CPU threads with <em>org.pytorch.PyTorchAndroid.setNumThreads(1);</em> but it does not make a difference. I have also tried 1,2,3,4. Is this the correct way to set the thread count?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/singularity; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/singularity; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/singularity; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  6:12pm; <NewLine> REPLY_DATE 2: June 25, 2020, 12:59am; <NewLine> REPLY_DATE 3: June 25, 2020,  3:33pm; <NewLine> REPLY_DATE 4: June 25, 2020,  3:45pm; <NewLine> REPLY_DATE 5: June 25, 2020,  4:42pm; <NewLine> REPLY_DATE 6: June 25, 2020,  7:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
86768,Convert FP32 model in torchvision.models to INT8 model,2020-06-24T17:38:26.438Z,4,302,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello</p><NewLine><p>I’d like to convert fp32 model supported in torchvision.models to INT8 model to accelerate CPU inference.<br/><NewLine>As I understand, using prepare() and convert() can convert the model (<a href=""https://pytorch.org/docs/stable/quantization.html#id1"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#id1</a>).</p><NewLine><p>Any ideas to handle below error messages?<br/><NewLine>thanks a lot!</p><NewLine><p>Environment:</p><NewLine><p>Nvidia Jetson TX2<br/><NewLine>pytorch 1.4.0</p><NewLine><p>My code:</p><NewLine><pre><code class=""lang-python"">model = torchvision.models.vgg16(pretrained=True).eval()<NewLine>img = np.random.randint(255, size=(1,3,224,224), dtype=np.uint8)<NewLine>img = torch.FloatTensor(img)#.cuda()<NewLine><NewLine><NewLine>model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine>config = torch.quantization.get_default_qat_qconfig('qnnpack')<NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine><NewLine>model.qconfig = torch.quantization.default_qconfig<NewLine>model = torch.quantization.prepare(model)<NewLine>model = torch.quantization.convert(model)<NewLine><NewLine>model.eval()<NewLine>quant = QuantStub()<NewLine>img = quant(img)<NewLine>for loop in range(100):<NewLine>    start = time.time()<NewLine>    output = model.forward(img)#, layer[1])<NewLine>    _, predicted = torch.max(output, 1)<NewLine>    end = time.time()<NewLine><NewLine>    print(end-start)<NewLine></code></pre><NewLine><p>Result:</p><NewLine><pre><code class=""lang-auto"">/home/user/.local/lib/python3.6/site-packages/torch/quantization/observer.py:172: UserWarning: Must run observer before calling calculate_qparams.                           Returning default scale and zero point.<NewLine>  Returning default scale and zero point."")<NewLine>Traceback (most recent call last):<NewLine>  File ""quan2.py"", line 37, in &lt;module&gt;<NewLine>    output = model.forward(img)#, layer[1])<NewLine>  File ""/usr/local/lib/python3.6/dist-packages/torchvision-0.5.0a0+85b8fbf-py3.6-linux-aarch64.egg/torchvision/models/vgg.py"", line 43, in forward<NewLine>    x = self.features(x)<NewLine>  File ""/home/user/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/user/.local/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 100, in forward<NewLine>    input = module(input)<NewLine>  File ""/home/user/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 532, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/user/.local/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py"", line 215, in forward<NewLine>    self.dilation, self.groups, self.scale, self.zero_point)<NewLine>RuntimeError: Could not run 'quantized::conv2d' with arguments from the 'CPUTensorId' backend. 'quantized::conv2d' is only available for these backends: [QuantizedCPUTensorId]. (dispatch_ at /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:257)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0x78 (0x7f98d36258 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libc10.so)<NewLine>frame #1: &lt;unknown function&gt; + 0x145a9a8 (0x7f67d9c9a8 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libtorch.so)<NewLine>frame #2: &lt;unknown function&gt; + 0x484b200 (0x7f6b18d200 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libtorch.so)<NewLine>frame #3: &lt;unknown function&gt; + 0x6518f0 (0x7f9060e8f0 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #4: &lt;unknown function&gt; + 0x61a868 (0x7f905d7868 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>frame #5: &lt;unknown function&gt; + 0x25ee04 (0x7f9021be04 in /home/user/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<NewLine>&lt;omitting python frames&gt;<NewLine>frame #7: python3() [0x529958]<NewLine>frame #9: python3() [0x527860]<NewLine>frame #11: python3() [0x5f2bcc]<NewLine>frame #14: python3() [0x528ff0]<NewLine>frame #17: python3() [0x5f2bcc]<NewLine>frame #19: python3() [0x595e5c]<NewLine>frame #21: python3() [0x529738]<NewLine>frame #23: python3() [0x527860]<NewLine>frame #25: python3() [0x5f2bcc]<NewLine>frame #28: python3() [0x528ff0]<NewLine>frame #31: python3() [0x5f2bcc]<NewLine>frame #33: python3() [0x595e5c]<NewLine>frame #35: python3() [0x529738]<NewLine>frame #37: python3() [0x527860]<NewLine>frame #38: python3() [0x5297dc]<NewLine>frame #40: python3() [0x528ff0]<NewLine>frame #45: __libc_start_main + 0xe0 (0x7f9a22d6e0 in /lib/aarch64-linux-gnu/libc.so.6)<NewLine>frame #46: python3() [0x420e94]<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Jungmo_Ahn,(Jungmo Ahn),Jungmo_Ahn,"June 24, 2020,  5:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are a couple things might be related to the error:</p><NewLine><ol><NewLine><li>You are using original vgg16, you need some modifications so that it can be quantized. You can take a look at torchvision/models/quantization/resnet.py as well as the tutorial: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> to see how to modify the model.</li><NewLine><li>QuantStub() need to be added to the model instead of using it directly.</li><NewLine><li>Between prepare() and convert() you need to run the model to collect histogram, which will be used to determine the scale and zero point of the quantized model.</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also are you running it on GPU? PyTorch quantization currently only support CPU and mobile backend.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>yes, i am only considering cpu operation. Thanks</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot. By the way, in the case of torchvision/models/quantization/resnet.py, there is specified backend, “fbgemm”, On the other hand, in the case of torchvision/models/quantization/mobilenet.py, the specified backend is ‘qnnpack’. Because I am working on ARM processor, I have to exploit ‘qnnpack’ backend. Is there any different implementation methodology between ‘fbgemm’ and ‘qnnpack’?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The quantization workflow and methodology is the same for ‘fbgemm’ and ‘qnnpack’, they are just different backends for different platforms.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li><NewLine><p>Although I carefully read the attached link in your answer, it is hard to find ‘observer’ phase. Could you explain the observer phase? Which function in the attached link conduct the operating observer phase?</p><NewLine></li><NewLine><li><NewLine><p>Even if there is no batch norm layer, is fuse always necessary?</p><NewLine></li><NewLine></ol><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Observer phase is called calibration in the tutorial link, basically it runs several iterations and us observer to collect the statistics of the activation and weight, which will be used to do quantization in convert():</li><NewLine></ol><NewLine><pre><code class=""lang-auto""># Calibrate with the training set<NewLine>evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)<NewLine></code></pre><NewLine><ol start=""2""><NewLine><li>You need to fuse batch norm, can refer to this post: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/static-quantizing-and-batch-norm-error-could-not-run-aten-native-batch-norm-with-args-from-quantcputensorid-backend/86805"">Static quantizing and batch norm error (could not run aten::native_batch_norm with args from QuantCPUTensorid backend')</a><NewLine></li><NewLine></ol><NewLine><p>Other than that fuse is not necessary but fuse will help on the performance and accuracy.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see.</p><NewLine><p>To summary what I understood, the quantization step is done as follow.</p><NewLine><ol><NewLine><li><strong>Load pretrained fp32 model</strong></li><NewLine><li>run prepare() to prepare converting <strong>pretrained fp32</strong> model to int8 model</li><NewLine><li>run fp32model.forward() to calibrate <strong>fp32 model</strong> by operating <strong>the fp32 model</strong> for a sufficient number of times. However, this calibration phase is a kind of `blackbox’ process so I cannot notice that the calibration is actually done.</li><NewLine><li>run convert() to finally convert the calibrated model to usable int8 model.</li><NewLine></ol><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Jungmo_Ahn; <NewLine> ,"REPLY_DATE 1: June 24, 2020,  6:08pm; <NewLine> REPLY_DATE 2: June 24, 2020,  6:10pm; <NewLine> REPLY_DATE 3: June 24, 2020,  6:18pm; <NewLine> REPLY_DATE 4: June 24, 2020,  6:24pm; <NewLine> REPLY_DATE 5: June 24, 2020,  6:41pm; <NewLine> REPLY_DATE 6: June 25, 2020,  6:15pm; <NewLine> REPLY_DATE 7: June 25, 2020,  6:23pm; <NewLine> REPLY_DATE 8: June 25, 2020,  6:38pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: 1 Like; <NewLine> 
86042,Unable to quantize the model due to RuntimeError,2020-06-19T09:08:00.985Z,2,92,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to quantize a model which has upsampling layers at specific parts of network but unable to quantize it due to this Error.</p><NewLine><blockquote><NewLine><p>Expected a value of type ‘Tensor’ for argument ‘target_size’ but instead found type ‘List[int]’.<br/><NewLine>Inferred ‘target_size’ to be of type ‘Tensor’ because it was not annotated with an explicit type.</p><NewLine></blockquote><NewLine><p>Implementation of the upsampling layer</p><NewLine><pre><code class=""lang-auto"">class Upsample(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Upsample, self).__init__()<NewLine><NewLine>    def forward(self, x, target_size):<NewLine>        # assert (x.data.dim() == 4)<NewLine><NewLine>        _, _, tH, tW = target_size[0], target_size[1], target_size[2], target_size[3]<NewLine><NewLine>        B = x.data.size(0)<NewLine>        C = x.data.size(1)<NewLine>        H = x.data.size(2)<NewLine>        W = x.data.size(3)<NewLine><NewLine>        return x.view(B, C, H, 1, W, 1).expand(B, C, H, tH // H, W, tW // W).contiguous().view(B, C, tH, tW)<NewLine><NewLine></code></pre><NewLine><p>Upsampling function usage</p><NewLine><pre><code class=""lang-auto"">up = self.upsample1(x7, downsample4.size())<NewLine></code></pre><NewLine><p>Quantization code</p><NewLine><pre><code class=""lang-auto"">model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>print(model.qconfig)<NewLine><NewLine>torch.quantization.prepare(model, inplace=True)<NewLine><NewLine>print('Post Training Quantization Prepare : Inserting Observers')<NewLine>print('\n Downsampling1 Block: After observer insertion \n\n', model.down1.conv1)<NewLine><NewLine>torch.quantization.convert(model, inplace=True)<NewLine>print(""Post Training Quantization : Convert Done!"")<NewLine>print(""\n Downsampling1Block: After quantization \n\n"", model.down1.conv1)<NewLine>torch.jit.save(torch.jit.script(model), quantized_model_path)<NewLine></code></pre><NewLine><p>This is my first time trying to quantize a model in pytorch so I am totally clueless on how to solve this. Thanks in advance.</p><NewLine></div>",https://discuss.pytorch.org/u/Htut_Lynn_Aung,(Htut Lynn Aung),Htut_Lynn_Aung,"June 19, 2020,  9:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error doesn’t seem related to quantization. Since the <code>upsample1</code> is a custom module, you cannot quantize it currently so the quant, dequant stubs need to be inserted in the model correctly.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""86042"" data-username=""Htut_Lynn_Aung""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/htut_lynn_aung/40/6027_2.png"" width=""20""/> Htut_Lynn_Aung:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto""> def forward(self, x, target_size):<NewLine>        # assert (x.data.dim() == 4)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>this is an error from <code>torch.jit.script</code>, you can annotate the forward like following:</p><NewLine><pre><code class=""lang-auto""> def forward(self, x, target_size):<NewLine>        # type : (Tensor, List[int]) -&gt; Tensor<NewLine>        # assert (x.data.dim() == 4)<NewLine></code></pre><NewLine><p>to make it scriptable.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the reply. Yes, that seems to be the case. I converted the target_size to torch tensor before passing it to upsample function so that kinda solved the problem.</p><NewLine><p>From this</p><NewLine><pre><code class=""lang-auto"">x7 = self.conv7(x6)<NewLine># UPSAMPLE<NewLine> up = self.upsample1(x7, downsample4.size())<NewLine></code></pre><NewLine><p>to</p><NewLine><pre><code class=""lang-auto"">x7 = self.conv7(x6)<NewLine># UPSAMPLE<NewLine>featuremap_size = torch.tensor(downsample4.size())<NewLine>up = self.upsample1(x7, featuremap_size, self.inference)<NewLine></code></pre><NewLine><p>But the model that I am trying to optimize is YoloV4 and it has some activation functions(mish and softplus) not supported on pytorch’s quantization. Therefore, even after this solution, I was not able to quantize the model in the end.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can still surround these ops with DequantStub and QuantStub</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Htut_Lynn_Aung; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 19, 2020,  9:32pm; <NewLine> REPLY_DATE 2: June 23, 2020,  4:51pm; <NewLine> REPLY_DATE 3: June 25, 2020,  6:21am; <NewLine> REPLY_DATE 4: June 25, 2020,  4:21pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
86606,Printing Quantized Model Weights,2020-06-23T19:36:47.166Z,0,159,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How do we print quantized model weights in PyTorch?</p><NewLine><p>To print using normal PyTorch representation, I understand we use the following approach…</p><NewLine><pre><code class=""lang-auto"">print_parameters = lambda model: [print(name, param.data) for name, param in model.named_parameters() if param.requires_grad]<NewLine></code></pre><NewLine><p>Similarly, if I defined a model as follows…</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine><NewLine>class Model(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Model, self).__init__()<NewLine>        self.conv1 = torch.nn.Conv2d(2, 3, 1, bias=False)<NewLine>        self.conv2 = torch.nn.Conv2d(3, 1, 2, bias=False)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.conv1(x)<NewLine>        x = self.conv2(x)<NewLine>        return x<NewLine><NewLine>model = Model()<NewLine></code></pre><NewLine><p>Then we can print using pretty straight-forward syntax…</p><NewLine><pre><code class=""lang-auto"">print(model.conv1.weight)<NewLine>print(model.conv2.weight)<NewLine></code></pre><NewLine><p>However, both of these approaches fail when the model is converted to a quantized form. Specifically, after the following procedures…</p><NewLine><pre><code class=""lang-auto"">model.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine>torch.quantization.convert(model, inplace=True)<NewLine></code></pre><NewLine><p>Printing <code>model.conv1.weight</code> returns a method and the loop provided at the beginning does not print anything.</p><NewLine></div>",https://discuss.pytorch.org/u/Joseph_Konan,(Joseph Konan),Joseph_Konan,"June 24, 2020,  5:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We have recently prototyped PyTorch Numeric Suite that can compare the numerics between float and quantized model. It’s in nightly build and will be in 1.6.</p><NewLine><p>You can take a look at the following example to see how to compare the weights using it:<br/><NewLine></p><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/9c66c88e19e8c6cc90c59409a8074cc31cb4654c/test/quantization/test_numeric_suite.py#L81"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/9c66c88e19e8c6cc90c59409a8074cc31cb4654c/test/quantization/test_numeric_suite.py#L81"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/9c66c88e19e8c6cc90c59409a8074cc31cb4654c/test/quantization/test_numeric_suite.py#L81</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""71"" style=""counter-reset: li-counter 70 ;""><NewLine><li>        x = self.mymul.mul(x, x)</li><NewLine><li>        x = self.myadd_relu.add_relu(x, x)</li><NewLine><li>        w = self.my_scalar_add.add_scalar(x, -0.5)</li><NewLine><li>        w = self.my_scalar_mul.mul_scalar(w, 0.5)</li><NewLine><li><NewLine></li><li>        w = self.dequant(w)</li><NewLine><li>        return w</li><NewLine><li><NewLine></li><li><NewLine></li><li>class TestEagerModeNumericSuite(QuantizationTestCase):</li><NewLine><li class=""selected"">    def test_compare_weights(self):</li><NewLine><li>        r""""""Compare the weights of float and quantized conv layer</li><NewLine><li>        """"""</li><NewLine><li><NewLine></li><li>        def compare_and_validate_results(float_model, q_model):</li><NewLine><li>            weight_dict = compare_weights(</li><NewLine><li>                float_model.state_dict(), q_model.state_dict()</li><NewLine><li>            )</li><NewLine><li>            self.assertEqual(len(weight_dict), 1)</li><NewLine><li>            for k, v in weight_dict.items():</li><NewLine><li>                self.assertTrue(v[""float""].shape == v[""quantized""].shape)</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: June 24, 2020, 11:34pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
86530,Can quantized model be exported and used?,2020-06-23T09:37:13.922Z,2,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to transform the quantization ability to Caffe? Let’s say I created a quantized model using PyTorch and now I want to export the model to Caffe, can I do that by using the scale/zero_point parameters or it’s mandatory to use PyTorch for their quantization?</p><NewLine></div>",https://discuss.pytorch.org/u/Shani_Gamrian,(Shani Gamrian),Shani_Gamrian,"June 23, 2020,  9:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can take a look at ONNX, but we don’t have very good quantization support in ONNX right now, I’m not sure about the ONNX - caffe path either.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is the quantization done once and then can be used (with the scale and zero_point) or it should have special support that make it int8 during inference?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>quantization is done before inference, it transforms a floating point model to a quantized model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shani_Gamrian; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 23, 2020,  4:10pm; <NewLine> REPLY_DATE 2: June 24, 2020,  7:43am; <NewLine> REPLY_DATE 3: June 24, 2020,  7:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
82402,What do [De]QuantStub actually do?,2020-05-22T00:44:56.738Z,1,429,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have scoured all documentation I could locate and am still confused on certain accounts:</p><NewLine><ul><NewLine><li>class docstring (<a href=""https://pytorch.org/docs/stable/quantization.html#torch.quantization.QuantStub"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#torch.quantization.QuantStub</a>) says</li><NewLine></ul><NewLine><blockquote><NewLine><p>Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.</p><NewLine></blockquote><NewLine><p>which unfortunately isn’t very helpful at all (which “observer”?). That last part of that sentence seems to suggest that at inference time this module will be replaced with one that does actual float-to-int8 data conversion. But what does this module do at calibration time?</p><NewLine><ul><NewLine><li>furthermore, tutorials seem to suggest that QuantStub and DeQuantStub act as delimiters of parts of the model stack that will actually be subject to quantization; however, some other commentary (<a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/quantization-how-to-quantize-model-which-include-not-support-to-quantize-layer/76528/2"">[quantization] how to quantize model which include not support to quantize layer</a>) seems to suggest that these also “record tensor statistics” and hence unique instances of them are needed – what, one unique pair per each contiguous quantization region?</li><NewLine></ul><NewLine><p>Some more details would be very much appreciated.</p><NewLine></div>",https://discuss.pytorch.org/u/vladium,,vladium,"May 22, 2020, 12:44am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>QuantStub is just a place holder for quantize op, it needs to be unique since it has state.<br/><NewLine>DeQuantStub is a place holder for dequantize op, but it does not need to be unique since it’s stateless.</p><NewLine><p>In eager mode quantization, users need to manually place <code>QuantStub</code> and <code>DeQuantStub</code> in the model whenever the activation in the code crosses the quantized and non-quantized boundary.</p><NewLine><p>One thing to remember is for a quantized module, we always quantize the output of the module, but we don’t quantize the input of the module, so the quantization of the input Tensor should be taken care of by the previous module, that’s why we have QuantStub here, basically to quantize the input for the next quantized module in the sequence.</p><NewLine><p>So in <code>prepare</code>, we’ll attach observer for the output of <code>QuantStub</code> to record the Tensor statistics of the output Tensor, just like for other modules like <code>nn.Conv2d</code>. observer is specified by the qconfig.</p><NewLine><p>And in <code>convert</code> <code>QuantStub</code> will be swapped as <code>nnq.Quantize</code> module, and output of <code>nnq.Quantize</code>(input of <code>nnq.Conv2d</code>) will be quantized.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you, Jerry, this helps.</p><NewLine><p>To clarify</p><NewLine><blockquote><NewLine><p>In <em>eager</em> mode quantization</p><NewLine></blockquote><NewLine><p>– that is the only mode available at the moment, correct? The JIT does not quantize (yet?).</p><NewLine><p>This might sounds like a nit, but I think the following actually reflects a fundamental difficulty in quantizing in eager mode:</p><NewLine><blockquote><NewLine><p>so the quantization of the input Tensor should be taken care of by the <em>previous</em> module,</p><NewLine></blockquote><NewLine><p>How does pytorch decide what is “previous”? The true sequence of layer invocations is determined by the procedural code in <code>forward()</code> and it can involve branching at runtime or just data flow merging like with skip connections.</p><NewLine><p>(I am <a href=""https://discuss.pytorch.org/t/am-i-correct-in-concluding-that-resnet-that-comes-with-pytorch-cant-be-quantized-by-pytorch/82405"">yet to succeed in quantizing resnet</a> because of this, I suspect)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>– that is the only mode available at the moment, correct? The JIT does not quantize (yet?).</p><NewLine></blockquote><NewLine><p>yeah, eager mode is the only mode that’s supported in public release, but graph mode is coming up in 1.6 as well.</p><NewLine><blockquote><NewLine><p>How does pytorch decide what is “previous”?</p><NewLine></blockquote><NewLine><p>PyTorch don’t do this in eager mode. that’s why in eager mode users need to manually place QuantStub and DeQuantStub themselves. this is done automatically in graph mode quantization.<br/><NewLine>eager mode will just swap all modules that has a qconfig, so user need to make sure the swap makes sense and set qconfig and place QuantStub/DeQuantStub correctly.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vladium; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: May 22, 2020,  1:14am; <NewLine> REPLY_DATE 2: May 22, 2020,  2:58pm; <NewLine> REPLY_DATE 3: June 23, 2020,  4:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
86347,Types of layers for Quantization,2020-06-22T05:21:28.014Z,0,82,"<div class=""post"" itemprop=""articleBody""><NewLine><p>What types of layers are supported in PyTorch’s quantization framework? (especially ones that are more related to convnets).<br/><NewLine>I found Conv2D, Conv3D, Relu, but I couldn’t find any types of BatchNorm)</p><NewLine></div>",https://discuss.pytorch.org/u/Shani_Gamrian,(Shani Gamrian),Shani_Gamrian,"June 22, 2020,  8:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll move the category to Quantization to get a bit more visibility for this topic. <img alt="":wink:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/wink.png?v=9"" title="":wink:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can checkout <a href=""https://pytorch.org/docs/stable/quantization.html#operation-coverage"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#operation-coverage</a>, this might be a little bit out of date, to find the most up to date supported ops, you can take a look at: <a href=""https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 22, 2020,  8:20am; <NewLine> REPLY_DATE 2: June 23, 2020,  4:32pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85986,Is there an alternative to do batched matrix multiplication on Quantized Tensors?,2020-06-18T22:50:13.121Z,2,122,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I am trying to do post training static quantization, however, I am running into issues where certain operations are not defined for <code>QuantizedCPUTensorId</code>.</p><NewLine><p>Minimal reproducible example:</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; <NewLine>&gt;&gt;&gt; A = torch.Tensor([[2,2], [3,3]]).unsqueeze(0)<NewLine>&gt;&gt;&gt; B = torch.Tensor([[2,3], [2,3]]).unsqueeze(0)<NewLine>&gt;&gt;&gt; scale, zero_point, dtype = 1.0, 2, torch.qint8<NewLine>&gt;&gt;&gt; qA = torch.quantize_per_tensor(A, scale, zero_point, dtype)<NewLine>&gt;&gt;&gt; qB = torch.quantize_per_tensor(B, scale, zero_point, dtype)<NewLine>&gt;&gt;&gt; torch.matmul(A,B)<NewLine>tensor([[[ 8., 12.],<NewLine>         [12., 18.]]])<NewLine>&gt;&gt;&gt; torch.matmul(qA,qB)<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>RuntimeError: Could not run 'aten::bmm' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::bmm' is only available for these backends: [CPUTensorId, VariableTensorId].<NewLine></code></pre><NewLine><p>Are there alternatives to accomplishing the same?<br/><NewLine>I know there are certain operations that are defined here: <a href=""https://pytorch.org/docs/stable/quantization.html#floatfunctional"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#floatfunctional</a> but what would be the optimal way?</p><NewLine></div>",https://discuss.pytorch.org/u/harshsaini,(Harsh Saini),harshsaini,"June 18, 2020, 10:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If possible try using <code>nn.Linear</code> instead of <code>aten::bmm</code>.</p><NewLine><p>Currently the only way is to implement the quantized operator for <code>aten::bmm</code>.<br/><NewLine>One easy way could be by implementing the <code>quantized::linear</code> operator by looping over the batch dimension. We will be looking into implementing this operator in the future.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>, thanks for the response.</p><NewLine><p>Yes, I had thought about that but wouldn’t that operation be suboptimal? However, if there is no alternative, I guess it would have to be so for now.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems like <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.functional.linear"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.functional.linear</a> is not a viable option. It requires the input tensor to be unsigned, however, the operation explicitly is between two tensors that are <code>qint8</code>.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; torch.nn.quantized.functional.linear(qA[0,], qB[0,])<NewLine>RuntimeError: expected scalar type QUInt8 but found QInt8<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>do you need both of the inputs to be <code>qint8</code>? If you change <code>qA</code> to be <code>quint8</code> it would work</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/harshsaini; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/harshsaini; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 19, 2020,  9:38pm; <NewLine> REPLY_DATE 2: June 20, 2020,  1:35am; <NewLine> REPLY_DATE 3: June 22, 2020,  6:32pm; <NewLine> REPLY_DATE 4: June 23, 2020,  4:13pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
85477,What is the need of fusing layers in MobileNetv2?,2020-06-15T05:39:13.040Z,2,112,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have seen the static quantization tutorial, where the layers are fused before itself. I got good result with fused layers but if I don’t fuse the layers, My accuracy is very poor.</p><NewLine><p>What is the effect of layer fusion?</p><NewLine><p>Please do help me with this.</p><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"June 15, 2020,  5:39am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>layer fusion is going to fuse Conv+BN into a Conv module or Conv + BN + ReLU into a ConvRelu module. this does not change numerics itself. Without fusion conv, bn and relu will be quantized independently, that might be the reason why the accuracy drops.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>But, what is the drawback of quantizing convolution, batchnorm, relu operations independently?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>quantizing them independently will have worse performance, and also may suffer from bigger accuracy loss.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Midhilesh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 16, 2020, 12:36am; <NewLine> REPLY_DATE 2: June 18, 2020, 11:42am; <NewLine> REPLY_DATE 3: June 23, 2020,  4:11pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
86358,Bilinear is slower than nearest after QAT,2020-06-22T08:00:24.902Z,1,74,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In my origin model,the upsample part is</p><NewLine><pre><code class=""lang-auto"">F.interpolate(l7, scale_factor=2.0, mode='bilinear', align_corners=True)<NewLine></code></pre><NewLine><p>，when i get QAT model.pt and tried it on android ,the inference time of the model.pt is slow, just similar to the float.pt<br/><NewLine>So，i changed the upsample part just like</p><NewLine><pre><code class=""lang-auto"">F.interpolate(l7, scale_factor=2.0, mode='nearest')<NewLine></code></pre><NewLine><p>the inference time is speed up.<br/><NewLine>But the result of segmentation model is too bad.<br/><NewLine>Why bilinear is slower than nearest after QAT?<br/><NewLine>Is there anyone can explain and give some suggestions.<br/><NewLine>Thx</p><NewLine></div>",https://discuss.pytorch.org/u/KevinZ1992,(XiaoJian Zhang),KevinZ1992,"June 22, 2020,  8:00am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Actually, I do not know about the QAT, but <code>nearest</code> is always faster than <code>bilinear</code>. In <code>bilinear</code> a transformation need to be computed meanwhile nearest is just copy/paste without any computation (almost).</p><NewLine><p>Although in large tensors, <code>linear</code> is possibly preferred even in term of speed.</p><NewLine><p>Bests</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy.<br/><NewLine>You are right, <code>nearest</code>  is always faster than  <code>bilinear</code>.<br/><NewLine>I test it on android, the difference less than 5ms ,but it’s more than about 100ms for quanted model.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Nikronic; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/KevinZ1992; <NewLine> ,"REPLY_DATE 1: June 22, 2020,  9:04am; <NewLine> REPLY_DATE 2: June 22, 2020, 11:21am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
74056,MobileNetV2 + SSDLite quantization results in different model definition,2020-03-22T18:16:27.184Z,6,352,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to quantize a mobilenetv2  + SSDLite model from <code>https://github.com/qfgaohao/pytorch-ssd</code></p><NewLine><p>I followed the tutorial here <code>https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</code> doing  <strong>Post-training static quantization</strong></p><NewLine><p>Before quantizing the model definition looks like this</p><NewLine><pre><code class=""lang-auto"">SSD(<NewLine>  (base_net): Sequential(<NewLine>    (0): Sequential(<NewLine>      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU6(inplace=True)<NewLine>    )<NewLine>    (1): InvertedResidual(<NewLine>      (conv): Sequential(<NewLine>        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)<NewLine>        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (2): ReLU6(inplace=True)<NewLine>        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      )<NewLine>    )<NewLine>    (2): InvertedResidual(<NewLine>      (conv): Sequential(<NewLine>        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (2): ReLU6(inplace=True)<NewLine>        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)<NewLine>        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (5): ReLU6(inplace=True)<NewLine>        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)<NewLine>        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      )<NewLine>    )<NewLine>    **#Removed some stuff to stay under 32K characters**<NewLine>    (5): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))<NewLine>  )<NewLine>  (source_layer_add_ons): ModuleList()<NewLine>)<NewLine></code></pre><NewLine><p>Quantization is done using :</p><NewLine><pre><code class=""lang-auto"">model.eval().to('cpu')<NewLine>model.fuse_model()<NewLine>model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine>torch.quantization.convert(model, inplace=True)<NewLine></code></pre><NewLine><p>After quantization the model definition looks like this :</p><NewLine><pre><code class=""lang-auto"">SSD(<NewLine>  (base_net): Sequential(<NewLine>    (0): Sequential(<NewLine>      (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): QuantizedReLU6(inplace=True)<NewLine>    )<NewLine>    (1): InvertedResidual(<NewLine>      (conv): Sequential(<NewLine>        (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), groups=32, bias=False)<NewLine>        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (2): QuantizedReLU6(inplace=True)<NewLine>        (3): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)<NewLine>        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      )<NewLine>    )<NewLine>    (2): InvertedResidual(<NewLine>      (conv): Sequential(<NewLine>        (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)<NewLine>        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (2): QuantizedReLU6(inplace=True)<NewLine>        (3): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), groups=96, bias=False)<NewLine>        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>        (5): QuantizedReLU6(inplace=True)<NewLine>        (6): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0, bias=False)<NewLine>        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      )<NewLine>    )<NewLine> <NewLine>**#Removed some stuff to stay under 32K characters**<NewLine><NewLine>    (5): QuantizedConv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)<NewLine>  )<NewLine>  (source_layer_add_ons): ModuleList()<NewLine>)<NewLine><NewLine></code></pre><NewLine><p>Model size decreased from 14MB to 4MB.<br/><NewLine>but with this new definition how can i load the quantized model ?</p><NewLine><p>I’m trying the following &amp; getting the below error</p><NewLine><pre><code class=""lang-auto"">#Saving<NewLine>torch.save(q_model.state_dict(), project.quantized_trained_model_dir / file_name)<NewLine>#Loading the saved quatized model<NewLine>lq_model = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)<NewLine>lq_model.load(project.quantized_trained_model_dir / file_name)<NewLine>#Error<NewLine>RuntimeError: Error(s) in loading state_dict for SSD:<NewLine>Unexpected key(s) in state_dict: ""base_net.0.0.scale"", ""base_net.0.0.zero_point"", ""base_net.0.0.bias"", ""base_net.1.conv.0.scale"", ""base_net.1.conv.0.zero_point"", ""base_net.1.conv.0.bias"", ""base_net.1.conv.3.scale"", ""base_net.1.conv.3.zero_point"", ""base_net.1.conv.3.bias"", ""base_net.2.conv.0.scale""...<NewLine></code></pre><NewLine><p>I do understand that after quantization some layers are changed <code>Conv2d -&gt; QuantizedConv2d </code> but does that mean that I have to have 2 model definitions for original &amp; quantized versions?</p><NewLine><p>This a diff of the definitions</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502"" href=""https://discuss.pytorch.org/uploads/default/original/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502.jpeg"" title=""Screen Shot 2020-03-22 at 7.19.05 PM""><img alt=""Screen Shot 2020-03-22 at 7.19.05 PM"" data-base62-sha1=""1QEb2ZWMsuMiSUM2WLNSpVTDOg2"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502_2_10x10.png"" height=""438"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502_2_690x438.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502_2_690x438.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502_2_1035x657.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/0/c/0cf5b7d3f5e8543782a0a7373cd25084b4d8c502_2_1380x876.jpeg 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screen Shot 2020-03-22 at 7.19.05 PM</span><span class=""informations"">3584×2278 1.57 MB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/wassimseif,(Wassim Seifeddine),wassimseif,"March 22, 2020,  6:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Have you tried this?: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/how-do-i-save-and-load-quantization-model/64188/6"">How do I save and load quantization model</a></p><NewLine><p>i.e., prepare and convert steps before loading the state_dict</p><NewLine><p>Also, I would expect conv+batchnorm+relu to be fused into QuantizedConvReLU2d but I think you are using relu6 and fusion of conv+batchnorm+relu6 isn’t currently supported.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, you’ll need to quantize <code>lq_model</code> after <code>lq_model = create_mobilenetv2_ssd_lite(len(class_names), is_test=True) </code> before you load from the quantized model</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see your point. Will try it &amp; see what happens</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>This worked even though there around 1.6 seconds of overhead of quantize the vanilla model before loading the model. Thank you</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/wassimseif"">@wassimseif</a>: How satisfied were you with the results of the quantization? Where did you add the  <code>QuantStub</code> and <code>DeQuantStub</code> functions in the forward pass? In particular, I was wondering is you dequantize the localization and confidence predictions at the very end of the model or whether you dequantize beforehand.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Quantization alone resulted in a huge drop in the mAP. but doing calibration while quantizing resulted in the same mAP compared to the original model. so make sure to explore calibration also. Quant &amp; DeQuant stubs where added just in the model ( before &amp; after the forward pass )</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks a lot for your kind and informative reply, <a class=""mention"" href=""/u/wassimseif"">@wassimseif</a> !</p><NewLine><p>When I tried exclusively quantization the mAP also drops enormously and I wondered whether I was doing it correctly. Will try calibration too - thanks for the suggestion!</p><NewLine><p>Just to make sure though: At the end of the forward pass, there are two dequant stubs added then, one for the locations and one for the confidences? Initially I thought that I should dequantize after the base net  because I suspected that such find grained localization cannot be done well with 8bits but requires the more expressive 32bit representation.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need 2 DeQuant Stubs. Just 1 &amp; you can reuse it for. Yes dequantizing after the base net might work, but this would result in the SSD layers not being quantized. My case was that fp16 or 32 instruction were not available so i had to quantize the whole model.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks again for the comment, <a class=""mention"" href=""/u/wassimseif"">@wassimseif</a> !</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/wassimseif; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/wassimseif; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/wassimseif; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/wassimseif; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/FabianSchuetze; <NewLine> ,"REPLY_DATE 1: March 23, 2020, 10:39pm; <NewLine> REPLY_DATE 2: March 29, 2020,  9:20pm; <NewLine> REPLY_DATE 3: March 28, 2020,  6:37pm; <NewLine> REPLY_DATE 4: March 29, 2020,  9:21pm; <NewLine> REPLY_DATE 5: June 18, 2020, 11:39am; <NewLine> REPLY_DATE 6: June 18, 2020, 12:02pm; <NewLine> REPLY_DATE 7: June 18, 2020,  2:32pm; <NewLine> REPLY_DATE 8: June 20, 2020,  5:03am; <NewLine> REPLY_DATE 9: June 20, 2020,  7:42am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
85720,Dose static quantization support CUDA?,2020-06-17T03:05:34.316Z,1,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to know whether the quantized model obtained by Post Training Static Quantization can be run on CUDA?</p><NewLine></div>",https://discuss.pytorch.org/u/uni1,,uni1,"June 17, 2020,  3:05am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>No, it only works on CPU right now, we will consider adding CUDA support in the second half of the year</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see. Thank you for your reply!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/uni1; <NewLine> ,"REPLY_DATE 1: June 20, 2020,  1:44am; <NewLine> REPLY_DATE 2: June 20, 2020,  1:45am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85928,Did pytorch support int16 quantization?,2020-06-18T13:08:08.398Z,0,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As per the documentation, PyTorch will support int8 quantization. Will PyTorch support int16 quantization currently?</p><NewLine></div>",https://discuss.pytorch.org/u/kunasiramesh,(Kunasi Ramesh),kunasiramesh,"June 18, 2020,  2:58pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently do not support int16 quantization. There is support for fp16 dynamic quantization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: June 22, 2020,  4:39am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
63964,"Pytorch1.3 Quantization for resnet50, accuracy is zero after fused_modules",2019-12-13T11:31:08.169Z,6,330,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi</p><NewLine><p>I am experimenting pytorch 1.3 Quantization for resent50. I had taken the pre-trained model from model zoo.</p><NewLine><p>Please find below for accuracy (for 100 images) and size at different stages of my experiment</p><NewLine><pre><code class=""lang-auto"">Size (MB): 102.491395<NewLine>{""metric"": ""original_resnet50_val_accuracy"", ""value"": 93.75}<NewLine>Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<NewLine>Size (MB): 102.145116<NewLine>{""metric"": ""fused_resnet50_val_accuracy"", ""value"": 0.0}<NewLine>ConvReLU2d(<NewLine>  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))<NewLine>  (1): ReLU()<NewLine>)<NewLine><NewLine>{""metric"": ""quntize_per_tensor_resent50_val_accuracy"", ""value"": 0.0}<NewLine>{""metric"": ""quntize_per_channel_resent50_val_accuracy"", ""value"": 0.0}<NewLine>Size (MB): 25.65341<NewLine>QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=1.0, zero_point=0, padding=(3, 3))<NewLine>Size (MB): 25.957137<NewLine>QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=1.0, zero_point=0, padding=(3, 3))<NewLine></code></pre><NewLine><p>Not sure Where it went wrong</p><NewLine><p>PFB code for fusing layers</p><NewLine><pre><code class=""lang-auto"">def fuse_model(m):<NewLine>    <NewLine>    modules_to_fuse = [['conv1','bn1',""relu""]]<NewLine>    torch.quantization.fuse_modules(m, modules_to_fuse,inplace=True)<NewLine>   <NewLine>    for mod in m.layer1:<NewLine>        torch.quantization.fuse_modules(mod, [[""conv1"",""bn1""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv2"",""bn2""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv3"",""bn3"",""relu""]],inplace=True)<NewLine>        if mod.downsample:<NewLine>            torch.quantization.fuse_modules(mod.downsample, [[""0"",""1""]],inplace=True)<NewLine><NewLine>    for mod in m.layer2:<NewLine>        torch.quantization.fuse_modules(mod, [[""conv1"",""bn1""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv2"",""bn2""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv3"",""bn3"",""relu""]],inplace=True)<NewLine>        if mod.downsample:<NewLine>            torch.quantization.fuse_modules(mod.downsample, [[""0"",""1""]],inplace=True)<NewLine><NewLine>    for mod in m.layer3:<NewLine>        torch.quantization.fuse_modules(mod, [[""conv1"",""bn1""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv2"",""bn2""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv3"",""bn3"",""relu""]],inplace=True)<NewLine>        if mod.downsample:<NewLine>            torch.quantization.fuse_modules(mod.downsample, [[""0"",""1""]],inplace=True)<NewLine><NewLine>    for mod in m.layer4:<NewLine>        torch.quantization.fuse_modules(mod, [[""conv1"",""bn1""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv2"",""bn2""]],inplace=True)<NewLine>        torch.quantization.fuse_modules(mod, [[""conv3"",""bn3"",""relu""]],inplace=True)<NewLine>        if mod.downsample:<NewLine>            torch.quantization.fuse_modules(mod.downsample, [[""0"",""1""]],inplace=True)<NewLine><NewLine>    return m <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Tiru_B,(Tiru B),Tiru_B,"December 13, 2019, 11:36am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>modified My fused_model function now I am seeing same accuracy as non fused model but, Quantiation accuracy is zero.</p><NewLine><pre><code class=""lang-auto"">Size (MB): 102.491395<NewLine>{""metric"": ""original_resnet50_val_accuracy"", ""value"": 93.75}<NewLine>Size (MB): 102.143772<NewLine>{""metric"": ""fused_resnet50_val_accuracy"", ""value"": 93.75}<NewLine>{""metric"": ""quntize_per_tensor_resent50_val_accuracy"", ""value"": 0.0}<NewLine>{""metric"": ""quntize_per_channel_resent50_val_accuracy"", ""value"": 0.0}<NewLine>Size (MB): 25.653416<NewLine>Size (MB): 25.957149<NewLine><NewLine></code></pre><NewLine><p>Change in fused function is , I have added relu in layer 1 to 4</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Its great to see that the accuracy post fusion is high. What are you using for calibration before you quantize the model?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>issue with calibration data set, I have selected randomly 1024 training samples from imagenet dataset. Now I can I see some good results.</p><NewLine><pre><code class=""lang-auto"">Size (MB): 102.491395<NewLine>{""metric"": ""original_resnet50_val_accuracy"", ""value"": 90.234375}<NewLine>Size (MB): 102.143772<NewLine>{""metric"": ""fused_resnet50_val_accuracy"", ""value"": 90.234375}<NewLine>calibration<NewLine>{""metric"": ""quntize_per_tensor_resent50_val_accuracy"", ""value"": 78.22265625}<NewLine>after quantization<NewLine>{""metric"": ""quntize_per_tensor_resent50_val_accuracy"", ""value"": 88.28125}<NewLine>calibration<NewLine>{""metric"": ""quntize_per_tensor_resent50_val_accuracy"", ""value"": 76.26953125}<NewLine>after quantization<NewLine>{""metric"": ""quntize_per_channel_resent50_val_accuracy"", ""value"": 89.84375}<NewLine>size of quantization per tensor model<NewLine>Size (MB): 25.653446<NewLine>size of quantization per channel model<NewLine>Size (MB): 25.957137<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey Tiru, can you please share your full code because I am facing same issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/tiru_b"">@Tiru_B</a>, can you share your code as I am having the same issue inspite of adding a relu layer.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>U can check in tiru1930  github  user Id</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""32"" src=""https://github.githubassets.com/favicons/favicon.svg"" width=""32""/><NewLine><a href=""https://github.com/tiru1930/resnet_quantization"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""400"" src=""https://avatars3.githubusercontent.com/u/12211287?s=400&amp;v=4"" width=""400""/><NewLine><h3><a href=""https://github.com/tiru1930/resnet_quantization"" rel=""nofollow noopener"" target=""_blank"">tiru1930/resnet_quantization</a></h3><NewLine><p>Contribute to tiru1930/resnet_quantization development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Tiru_B; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Tiru_B; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/studML; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/khizar-anjum; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Tiru_B; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/Tiru_B; <NewLine> ,"REPLY_DATE 1: December 13, 2019, 11:48am; <NewLine> REPLY_DATE 2: December 13, 2019, 11:03pm; <NewLine> REPLY_DATE 3: December 16, 2019,  5:37am; <NewLine> REPLY_DATE 4: May 15, 2020,  5:33am; <NewLine> REPLY_DATE 5: June 18, 2020,  2:55pm; <NewLine> REPLY_DATE 6: June 18, 2020,  4:41pm; <NewLine> REPLY_DATE 7: June 18, 2020,  4:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 2 Likes; <NewLine> 
85735,&lsquo;aten::slow_conv_transpose2d&rsquo; not support in &lsquo;QuantizedCPUTensorID&rsquo;,2020-06-17T04:54:18.226Z,0,157,"<div class=""post"" itemprop=""articleBody""><NewLine><p>OS: Win7 64bit<br/><NewLine>Pytorch 1.5.0_CPU<br/><NewLine>when I try to quantize an unet model, meet the error below:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'aten::slow_conv_transpose2d' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::slow_conv_transpose2d' is only available for these backends: [CPUTensorId, VariableTensorId].<NewLine></code></pre><NewLine><p>Is there any way to workaround this?</p><NewLine></div>",https://discuss.pytorch.org/u/zhijian_li,(Zhijian Li),zhijian_li,"June 17, 2020,  5:02am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Had the exact same problem. I worked around this by inserting <code>torch.quantization.DeQuantStub</code> and <code>torch.quantization.QuantStub</code> before and after the <code>ConvTranspose2d</code> layer. I don’t know if this affects performance or anything.</p><NewLine><p>Just take a look at the following class I converted:</p><NewLine><pre><code class=""lang-auto"">from torch.quantization import DeQuantStub, QuantStub<NewLine>class UpsamplerBlock (nn.Module):<NewLine>    def __init__(self, ninput, noutput):<NewLine>        super(UpsamplerBlock, self).__init__()<NewLine>        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)<NewLine>        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine><NewLine>    def forward(self, input):<NewLine>        output = self.conv(self.dequant(input))<NewLine>        output = self.bn(self.quant(output))<NewLine>        return F.relu(output)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, quantized conv transpose 2d is not supported yet, but <a class=""mention"" href=""/u/zafar"">@Zafar</a> is working on it right now</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/khizar-anjum; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 17, 2020,  8:35pm; <NewLine> REPLY_DATE 2: June 18, 2020,  1:24am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
85133,Pytorch1.5.0/win7 64bit/ Didn&rsquo;t find engine for operation quantized::conv2d_prepack NoQEngine,2020-06-12T02:25:13.160Z,0,124,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Pytorch 1.5.0+cu92<br/><NewLine>Windows7 64 bit<br/><NewLine><strong>model.qconfig = torch.quantization.default_qconfig<br/><NewLine>torch.quantization.prepare(model, inplace=True)<br/><NewLine>torch.quantization.convert(model, inplace=True)</strong><br/><NewLine>&lt;---- Here<br/><NewLine><strong>Exception has occurred: RuntimeError</strong><br/><NewLine><strong>Didn’t find engine for operation quantized::conv2d_prepack NoQEngine</strong></p><NewLine></div>",https://discuss.pytorch.org/u/zhijian_li,(Zhijian Li),zhijian_li,"June 12, 2020,  2:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you check if the <code>USE_FBGEMM</code> macro is turned on in your build environment? I believe it should be supported.</p><NewLine><p>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> who might be able to add more info.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We use VS 14.11 to build binaries for CUDA 9.2, so there is no FBGEMM support. If you need FBGEMM, then please use the binaries with other CUDA versions instead.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/peterjc123; <NewLine> ,"REPLY_DATE 1: June 16, 2020, 11:42pm; <NewLine> REPLY_DATE 2: June 17, 2020,  4:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67457,Issue with Quantization,2020-01-23T10:53:39.689Z,0,485,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have input image of shape <code>32*1*32*128</code> where 32 is my batch size(index 0).<br/><NewLine>I want to quantize my model. When I call my evaluate function then it shows this error</p><NewLine><pre><code class=""lang-auto""> File ""/media/ai/ashish/OCR/Text_Recognition/modules/transformation.py"", line 158, in build_P_prime<NewLine>    batch_size, 3, 2).float().to(device)), dim=1)  # batch_size x F+3 x 2<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'aten::_cat' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::_cat' is only available for these backends: [CUDATensorId, CPUTensorId, VariableTensorId].<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Ashish_Gupta1,(Ashish Gupta),Ashish_Gupta1,"January 23, 2020, 11:41am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I meet same question, have you solve the question?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you use FloatFunctional(<a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/functional_modules.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/functional_modules.py</a>) to replace the call to <code>torch.cat</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 11, 2020,  8:05am; <NewLine> REPLY_DATE 2: June 16, 2020,  4:22pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85494,Bug in QAT for classification?,2020-06-15T09:37:18.904Z,2,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When i use a pretrained QAT model, my task is face detection, the face landmark and the bbox is almost same like float training, but the classification score from the result of softmax will be wrong, because it will be greater than 1.It think it is a bug in scale and point from softmax?</p><NewLine></div>",https://discuss.pytorch.org/u/xieydd,(xieydd),xieydd,"June 16, 2020,  8:20am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Anybody help me? <img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My code fault, sorry.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/xieydd; <NewLine> ,"REPLY_DATE 1: June 16, 2020,  1:36am; <NewLine> REPLY_DATE 2: June 16, 2020,  9:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
85469,Dynamic + Post-training static quantization at the same time,2020-06-15T01:02:11.556Z,0,73,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,<br/><NewLine>I have a CNN+linear model that I want to quantize. To make the most of PyTorch’s quantization, is there any way to perform static quantization on the CNN and dynamic on linear?</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/felicitywang,(Felicitywang),felicitywang,"June 15, 2020,  1:02am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""85469"" data-username=""felicitywang""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/f/919ad9/40.png"" width=""20""/> felicitywang:</div><NewLine><blockquote><NewLine><p>Thank you.</p><NewLine></blockquote><NewLine></aside><NewLine><p>if you are doing static quantization, why don’t do static quantization on both conv and linear?</p><NewLine><p>If you really need to do this, you can first apply static quantization, and then dynamic quantization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 16, 2020, 12:37am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84901,Error in QAT evaluate,2020-06-10T12:21:35.850Z,9,204,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When i run QAT, training is normal, but when i want to evaluate the qat model, an error <code>length of scales must equal to channel</code> confuse me.<br/><NewLine>I use pytorch 1.4.0, and my code is</p><NewLine><pre><code class=""lang-auto""># Traing is Normal<NewLine>net = MyQATNet()<NewLine>net.fuse_model()<NewLine>net.qconfig = torch.quantization.get_default_qat_config(""fbgemm"")<NewLine>net_eval = net<NewLine>model.apply(torch.quantization.enable_observer)<NewLine>model.apply(torch.quantization.enable_fake_quant)<NewLine><NewLine># Evaluate<NewLine>qat_model = copy.deepcopy(net_eval)<NewLine>qat_model.to(torch.device('cpu'))<NewLine>torch.quantization.convert(qat_model, inplace=True) # Error is there<NewLine>qat_model.eval()<NewLine></code></pre><NewLine><p><a class=""mention"" href=""/u/ptrblck"">@ptrblck</a> Can you have a look?</p><NewLine></div>",https://discuss.pytorch.org/u/xieydd,(xieydd),xieydd,"June 10, 2020, 12:21pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Anybody help me, thanks a lot?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>can you paste your network definition and the input you use to run the model? it might be a problem with your input</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think i find where is the error, When i use DataParallel, it will be such an error, but when i use single gpu, will no error.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>How can i save the qat trained model, when i save <code>torch.save(qat_model.state_dict(),'qat_model.pth')</code> or i directly save training model <code>torch.save(net, 'net.pth')</code>, when i want to load the pretrained qat model, for qat_model, the key is like  <code>conv1.0.activation_post_process.scale</code>; and when net, the key have no <code>conv1.0.activation_post_process.scale</code>, but expected key is  <code>conv1.0.0.activation_post_process.scale</code>, so KeyError happened. When i see the model definition, expected key is right.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think somebody have same question like me. <a href=""https://github.com/pytorch/pytorch/issues/32691"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/32691</a></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""4"" data-topic=""84901"" data-username=""xieydd""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/x/90ced4/40.png"" width=""20""/> xieydd:</div><NewLine><blockquote><NewLine><p>I think i find where is the error, When i use DataParallel, it will be such an error, but when i use single gpu, will no error.</p><NewLine></blockquote><NewLine></aside><NewLine><p>We recently fixed a bug with QAT and DataParallel, please try with pytorch nightly to see if the issue still persists. cc <a class=""mention"" href=""/u/vasiliy_kuznetsov"">@Vasiliy_Kuznetsov</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>you mean the load_state_dict KeyError is also solved in newest version of pytorch? tks</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think somebody have the same error: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/issue-with-quantization/67457"">Issue with Quantization</a></p><NewLine><p>I think i know the answew:</p><NewLine><pre><code class=""lang-auto""># pytorch 1.4<NewLine>#save<NewLine>torch.save(net.state_dict(),'xx') # fp32 model<NewLine>#load<NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig(""fbgemm"")<NewLine>model.fuse_model()<NewLine>torch.quantization.prepare_qat(net, inplace=True)<NewLine>state_dict = torch.load('xx', map_location=torch.device('cpu'))<NewLine># remove module.  module<NewLine>#torch.quantization.convert(net,inplace=True) # convert it to int8 model<NewLine>x = torch.randn((2,3,300,300))<NewLine>y = net(x)<NewLine>print(y)<NewLine>print('Success')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, <a href=""https://github.com/pytorch/pytorch/pull/37032"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/37032</a> fixes an error for DataParallel.  <a class=""mention"" href=""/u/xieydd"">@xieydd</a> you can try the nightly to verify if this fixes your problem.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>nightly can fix my problem, tks for all.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/xieydd; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/Vasiliy_Kuznetsov; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/xieydd; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  3:06pm; <NewLine> REPLY_DATE 2: June 11, 2020,  2:29am; <NewLine> REPLY_DATE 3: June 11, 2020,  2:33am; <NewLine> REPLY_DATE 4: June 11, 2020,  2:41am; <NewLine> REPLY_DATE 5: June 11, 2020,  3:33am; <NewLine> REPLY_DATE 6: June 11, 2020,  5:24am; <NewLine> REPLY_DATE 7: June 11, 2020,  6:10am; <NewLine> REPLY_DATE 8: June 12, 2020,  3:49am; <NewLine> REPLY_DATE 9: June 11, 2020,  3:59pm; <NewLine> REPLY_DATE 10: June 15, 2020,  2:08am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> 
85329,Post-Training Static Quantization for Last FC layer gets rubbish results,2020-06-13T12:17:36.680Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have this model that I have statically quantized in two manners: 1) feature extractor only and 2) feature extractor + classifier. I get adequate results with the former, but I get rubbish with the later.<br/><NewLine>I do not know where to start looking, does anyone have any idea what could be going wrong?<br/><NewLine>It seems when I quantize the classifier, there is very little variation in the classification output.<br/><NewLine>Validation error before quantization for both cases is good, but it goes down to almost 0 when I quantize the classifier.</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/juanmanpr,(Juanma),juanmanpr,"June 13, 2020, 12:17pm",,,,,
85078,How to quantize torch.zeros_like(x)?,2020-06-11T16:05:51.556Z,0,128,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am getting all kind of errors like:</p><NewLine><p>RuntimeError: Could not run ‘aten::empty.memory_format’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::empty.memory_format’ is only available for these backends: [CUDATensorId, SparseCPUTensorId, VariableTensorId, CPUTensorId, MkldnnCPUTensorId, SparseCUDATensorId].</p><NewLine><p>when using hacky solutions like nn.quantized.FloatFunctional().mul(x,0)</p><NewLine><p>What is the correct way to quantize this operation?</p><NewLine></div>",https://discuss.pytorch.org/u/juanmanpr,(Juanma),juanmanpr,"June 11, 2020,  4:05pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>sorry, it seems that nn.quantized.FloatFunctional().mul_scalar(x,0) hacky solutions work in pytorch 1.5 (I was using 1.4).</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/juanmanpr; <NewLine> ,"REPLY_DATE 1: June 11, 2020,  4:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84653,How to deploy quantized model to C++ frontend?,2020-06-08T14:07:06.171Z,0,81,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there any tutorial or instructions about how to deploy our quantized model to C++ frontend? I can’t find any on the Internet.</p><NewLine></div>",https://discuss.pytorch.org/u/seeker,(LIU Qingyuan),seeker,"June 8, 2020,  2:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>maybe <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/cpp_export.html</a>? there is nothing specific to quantized models, it should work the same way as other models</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  4:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84840,Backpropagation gets slower in mixprecision,2020-06-10T05:21:57.756Z,0,64,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>After converting my program into mixed-precision using amp, the forward time gets shorter while the backward time gets longer when I record running time by “import time”.</p><NewLine><p>Then I use “torch.profiler”  to record the running time.</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a4eee5c5737235a1add66e5c006949d76b789071"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071.png"" title=""image""><img alt=""image"" data-base62-sha1=""nx48Ja8sxbAiQdaq78KkNHpGdgZ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071_2_10x10.png"" height=""462"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071_2_690x462.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071_2_690x462.png, https://discuss.pytorch.org/uploads/default/optimized/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071_2_1035x693.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/a/4/a4eee5c5737235a1add66e5c006949d76b789071_2_1380x924.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1704×1143 56 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>However, it seems that the result of torch.profiler meets my expectation: cuda.time becomes around 1/4 of the fp32.</p><NewLine><p>Here my question is: what is the cuda.time means and why the backpropagation time becomes longer in mix precision?</p><NewLine><p>mixed-precision:<br/><NewLine>import time :<br/><NewLine>Entire Epoch: [1]	Train: [0]	SUM: 1828.379	DT: 13.270	FW: 71.987	BK: 1711.248	CM: 31.873	<br/><NewLine>torch profiler:<br/><NewLine>Self CPU time total: 56.883s<br/><NewLine>CUDA time total: 893.112s</p><NewLine><p>FP32:<br/><NewLine>import time :<br/><NewLine>Entire Epoch: [3]	Train: [0]	SUM: 1584.260	DT: 15.714	FW: 368.550	BK: 1119.231	CM: 80.766<br/><NewLine>torch profiler:<br/><NewLine>Self CPU time total: 105.049s<br/><NewLine>CUDA time total: 3021.355s</p><NewLine><p>Detailed Logs of my program are  listed below:<br/><NewLine><a href=""http://49.234.107.127:81/index.php/s/qa3Yjo8WJwNZjCS"" rel=""nofollow noopener"">http://49.234.107.127:81/index.php/s/qa3Yjo8WJwNZjCS</a> (mixed precision)<br/><NewLine><a href=""http://49.234.107.127:81/index.php/s/y8SpyfiM3d5SZp7"" rel=""nofollow noopener"">http://49.234.107.127:81/index.php/s/y8SpyfiM3d5SZp7</a></p><NewLine><p>My model uses conv3d and I run my code on Tesla V100.</p><NewLine><p>Many thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/dujiangsu,(DouJS),dujiangsu,"June 10, 2020,  5:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you post the results of your manual profiling here, please?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> ,"REPLY_DATE 1: June 10, 2020,  9:04am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80954,"Post Quantizing conv1d, PReLU &amp; layerNorm layers can be done?",2020-05-13T03:15:31.034Z,14,466,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I build a pytorch model based on conv1d. I gone through quantization and implemented some cases as well but all those are working on conv2d, bn,relu but In my case, my model is built on conv1d and PReLU. Does this quatization valid for these network layers? Because when I did quantization only the layers which are included in mapping is only quantized. Let me show you those layers for which quantization is valid(i.e which are included in mapping)<br/><NewLine>Please find the list of modules it supports:(according to source code in went through)<br/><NewLine>(Actual layer : quantized layer)<br/><NewLine>nn.Linear: nnq.Linear,<br/><NewLine>nn.ReLU: nnq.ReLU,<br/><NewLine>nn.ReLU6: nnq.ReLU6,<br/><NewLine>nn.Conv2d: nnq.Conv2d,<br/><NewLine>nn.Conv3d: nnq.Conv3d,<br/><NewLine>nn.BatchNorm2d: nnq.BatchNorm2d,<br/><NewLine>nn.BatchNorm3d: nnq.BatchNorm3d,<br/><NewLine>QuantStub: nnq.Quantize,<br/><NewLine>DeQuantStub: nnq.DeQuantize,</p><NewLine><h1>Wrapper Modules:</h1><NewLine><p>nnq.FloatFunctional: nnq.QFunctional,</p><NewLine><h1>Intrinsic modules:</h1><NewLine><p>nni.ConvReLU2d: nniq.ConvReLU2d,<br/><NewLine>nni.ConvReLU3d: nniq.ConvReLU3d,<br/><NewLine>nni.LinearReLU: nniq.LinearReLU,<br/><NewLine>nniqat.ConvReLU2d: nniq.ConvReLU2d,<br/><NewLine>nniqat.LinearReLU: nniq.LinearReLU,<br/><NewLine>nniqat.ConvBn2d: nnq.Conv2d,<br/><NewLine>nniqat.ConvBnReLU2d: nniq.ConvReLU2d,</p><NewLine><h1>QAT modules:</h1><NewLine><p>nnqat.Linear: nnq.Linear,<br/><NewLine>nnqat.Conv2d: nnq.Conv2d,</p><NewLine><p>Is it, it means that quantization can’t be done on conv1d and PReLU?</p><NewLine></div>",https://discuss.pytorch.org/u/BOLLOJU_ARAVIND,(BOLLOJU ARAVIND),BOLLOJU_ARAVIND,"May 13, 2020,  3:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>We are in the process of implementing the Conv1d module and ConvReLU1d fused module. The PR list is here <a href=""https://github.com/pytorch/pytorch/pull/38438"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/38438</a>. Feel free to try out your model with the changes in this PR. The quantization flow should convert it.<br/><NewLine>We don’t currently support fusion with PReLU and LayerNorm, so they will have to be executed separately.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>Fusing is optional one in quantization if I’m not wrong. We need our modules to be quantized i.e., each layer we implemented, in order to get our quantized parameters to pass through it. Only the quantized model will work if all the layers were quantized is it right? Or else we need to dequantize the parameters again before it passes through not quantized layer is it so?</p><NewLine><p>Thank you <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>Hey, you gave me that PR link above in the last comment for support of quantized conv1d. I decided to give a try with that but the torch module is not importing and it showing the error ‘torch.version’ is not there. So i copied ‘version.py’ from my earlier version after this too its not importing torch throwing another error ‘torch._c import default_generators’ failed to import.</p><NewLine><p>May I know that the changes in that PR is applicable for torch CPU version or not?</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a> ,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Only the quantized model will work if all the layers were quantized is it right? Or else we need to dequantize the parameters again before it passes through not quantized layer is it so?</p><NewLine></blockquote><NewLine><p>You can insert <code>QuantStub</code>, <code>DequantStub</code> blocks around the code that can be quantized. Please see <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> for an example of this.</p><NewLine><blockquote><NewLine><p>May I know that the changes in that PR is applicable for torch CPU version or not?</p><NewLine></blockquote><NewLine><p>The change is applicable for CPU, to get these changes you can either build pytorch from source or install from pytorch nightly.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><p>Thank you, Now I’m able to work with the changes in new PR as you suggested I tried with nightly.</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>I have done Quantizing my model, now I tried to save it with ‘jit.save(jit.script(model))’ but it pops the error as ‘‘aten::slice.t(t[] l, int start, int end=9223372036854775807, int step=1) -&gt; (t[]): could not match type tensor to list[t] in argument ‘l’: cannot match list[t] to tensor.’’ very similar to this 2 more pops also arised. I googled about this error and in some discussions I found that it is the error regarding slicing(: , : , : ) operator says that ‘jit.script will not support scripting for directly used slicing operator’. Is it the error actually pointed to that? Because I too used slicing operator in the middle of my model and error throws at the same line.</p><NewLine><p>This is one thing and now I go with another alternative to save quantized model i.e., with state_dict(). I’m able to save model with this but when I want to perform inference I have to initialize the model with the params which I have saved earlier with state_dict(). Now the params are quantized one but our model is defined for float. So error popped up as ‘exception occured : (‘copying from quantized tensor to non-quantized tensor is not allowed, please use dequantize to get a float tensor from a quantized tensor’’. This is the issue that I can’t save my model with jit and if I do so with state_dict() here I can’t initialize my model to go with inference.</p><NewLine><p>Can you suggest any alternative?</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a><br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you give a small repro for the error with <code>aten::slice</code> not working with jit?</p><NewLine><p>Regarding loading the quantized state_dict - you will first have to convert the model to quantized model before loading the state dict as well. You can call prepare and convert APIs to do this (no need to calibrate). This way the model state dict will match the saved quantized state dict and you should be able to load it.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>Thanks for the suggestion.</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""80954"" data-username=""supriyar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/> supriyar:</div><NewLine><blockquote><NewLine><p>call prepare and convert APIs</p><NewLine></blockquote><NewLine></aside><NewLine><p>I tried with this one and its done.</p><NewLine><p>Hey regarding</p><NewLine><aside class=""quote no-group"" data-post=""8"" data-topic=""80954"" data-username=""supriyar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/> supriyar:</div><NewLine><blockquote><NewLine><p><code>aten::slice</code> not working with jit</p><NewLine></blockquote><NewLine></aside><NewLine><p>This has done successfully because JIT might accepts only direct representation of Int for slicing as “<strong>w[:,:,:-16]</strong>” where as I initially represented as  “w = w[:,:,:-2*2**3]”. So tried a chance and it worked.</p><NewLine><p>Something I would like to say is JIT is not able to find attribute “.new_tensor” where as “.clone().detach()” is identified</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><p>I have some doubt about operations on quantized variables. Here I’m quoting it</p><NewLine><blockquote><NewLine><pre><code>    z = self.dequant_tcnet(z)<NewLine>    w = self.dequant_tcnet(w)<NewLine>    v = self.dequant_tcnet(v)<NewLine>    x = self.dequant_tcnet(x)<NewLine>    z = z + w<NewLine>    x = x + v<NewLine>    x = self.quant_tcnet2(x)<NewLine></code></pre><NewLine></blockquote><NewLine><p>Here inorder to perform these 2 operations  z = z + w; x = x + v I need to dequantize the variables involved in that operations and then perform those operations. Can we able to perform those operations without dequantizing i.e., as quantized variables because if I do run this without dequantizing variables “<em>RuntimeError: Could not run ‘aten::add.Tensor’ with arguments from the ‘QuantizedCPU’ backend. ‘aten::add.Tensor’ is only available for these backends: [CPU, MkldnnCPU, SparseCPU, Autograd, Profile]</em>.” error is interrupting.</p><NewLine><p>Can we perform “addition” on 2 quantized tensor variables? without dequantizing them in any other alternative.</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><blockquote><NewLine><pre><code>z = self.dequant_tcnet(z)<NewLine>w = self.dequant_tcnet(w)<NewLine>v = self.dequant_tcnet(v)<NewLine>x = self.dequant_tcnet(x)<NewLine>z = z + w<NewLine>x = x + v<NewLine>x = self.quant_tcnet2(x)<NewLine></code></pre><NewLine></blockquote><NewLine><blockquote><NewLine><p>Can we perform “addition” on 2 quantized tensor variables? without dequantizing them in any other alternative.</p><NewLine></blockquote><NewLine><p>For this i tried QFunctional &amp; FloatFunctional but output is not up to the mark by using this. where as placing quantstubs works good.</p><NewLine><p>I have some concern here. My float model which takes 0.2 - 0.3 second (~300ms) to process single input whereas after i quantizing my model with Int8 Quantization the time taken is increased from 0.2-0.3(float precision) to 0.4 to 0.5(Int8).</p><NewLine><p>Here i show you the exact float model block and quantized model block</p><NewLine><blockquote><NewLine><pre><code>  ***** float block ******<NewLine>   *****Round 1********<NewLine>    y = self.conv1x12(x)<NewLine>    y = self.prelu2(y)<NewLine>    y = self.norm2(y)<NewLine>    w = self.depthwise_conv12(y)<NewLine>    #w = w[:,:,:-2*2**2]<NewLine>    w = w[:,:,:-8]<NewLine>    y = self.depthwise_conv2(y)<NewLine>    y = y[:,:,:-8]<NewLine>    y = self.prelu22(y)<NewLine>    y = self.norm22(y)<NewLine>    v = self.pointwise_conv2(y)<NewLine>    z = z + w<NewLine>    x = x + v<NewLine></code></pre><NewLine></blockquote><NewLine><p>This is float model block. This block/computation will be repeated 13 more times (total 14 blocks). This is taking 0.2 - 0.3 seconds</p><NewLine><blockquote><NewLine><p><em><strong><strong>Quantized block</strong></strong></em><br/><NewLine><strong><strong>round 1</strong></strong>**<br/><NewLine>y = self.conv1x12(x)<br/><NewLine>y = self.prelu2(y)<br/><NewLine>y = self.norm2(y)<br/><NewLine>w = self.depthwise_conv12(y)<br/><NewLine>w = w[:,:,:-8]<br/><NewLine>y = self.depthwise_conv2(y)<br/><NewLine>y = y[:,:,:-8]<br/><NewLine>y = self.prelu22(y)<br/><NewLine>y = self.norm22(y)<br/><NewLine>v = self.pointwise_conv2(y)<br/><NewLine>w = self.dequant_tcnet(w)<br/><NewLine>z = self.dequant_tcnet(z)<br/><NewLine>v = self.dequant_tcnet(v)<br/><NewLine>x = self.dequant_tcnet(x)<br/><NewLine>z = z + w<br/><NewLine>x = x + v<br/><NewLine>x = self.quant_tcnet3(x)</p><NewLine></blockquote><NewLine><p>This is quantized block model where is placed quantstubs for those arthematic operations &amp; remaining all layers are quantized. This quantized model is taking 0.4 - 0.5 seconds</p><NewLine><p>So after quantizing my model, the size of model is optimized but computation time is not optimized. Could you tell me is there any flaw? I crosschecked and the output is also good but computation is not reduced</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>,</p><NewLine><p>Referring to above mentioned issue I want to make it clear about for which layers i have done quantization.</p><NewLine><ol><NewLine><li>Conv1d (from nightly)</li><NewLine><li>LayerNorm (from nightly)</li><NewLine><li>ReLU</li><NewLine><li>Linear</li><NewLine></ol><NewLine><p>additional layers:</p><NewLine><ol><NewLine><li>quantstub / dequantstubs</li><NewLine><li>QFunctional / FloatFunctional</li><NewLine></ol><NewLine><p>All these layers are quantized and I fused Relu and Conv1d as well ( since beginning Im referring to this documentation <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">Static Quantization with eager mode in pytorch</a></p><NewLine><p>If I use FloatFunctional, I’m not using Quant/Dequantstubs in my model where arithmetic operations are triggered between quantized layers.</p><NewLine><p>After successfully done quantizing, still my model CPU computation is not reduced instead computation has increased after quantization!</p><NewLine><p>Could you tell me in which cases this might happen?</p><NewLine><p>Thanks,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>For add you could use <code>torch.nn.FloatFunctional</code>, the extra dequant and quant ops in the network could be slowing things down.</p><NewLine><p>Regarding performance you can try running the torch.autograd.profiler on your model for some iterations to see which ops take up most time. It will give you an op level breakdown with runtime so you can compare float vs quantized model.</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><p>Yah, I did that replacement of quantstubs/dequantstubs with floatfunctional. I’ll drop it here</p><NewLine><pre><code>    &gt; y = self.conv1x11(x)<NewLine></code></pre><NewLine><blockquote><NewLine><pre><code>    y = self.prelu1(y)<NewLine>    y = self.norm1(y)<NewLine>    w = self.depthwise_conv11(y)<NewLine>    #w = w[:,:,:-2*2**1]<NewLine>    w = w[:,:,:-4]<NewLine>    y = self.depthwise_conv1(y)<NewLine>    #y = y[:,:,:-2*2**1]<NewLine>    y = y[:,:,:-4]<NewLine>    y = self.prelu11(y)<NewLine>    y = self.norm11(y)<NewLine>    v = self.pointwise_conv1(y)<NewLine>    #w = self.pointwise_conv_skp1(y)<NewLine>    #z = self.dequant_tcnet(z)<NewLine>    #w = self.dequant_tcnet(w)<NewLine>    #v = self.dequant_tcnet(v)<NewLine>    #x = self.dequant_tcnet(x)<NewLine>    #z = z + w<NewLine>    #x = x + v<NewLine>    z = self.Qf_s.add(z,w)<NewLine>    x = self.Qf_s.add(x,v)<NewLine>    #x = self.quant_tcnet2(x)<NewLine>    #z = self.quant(z)<NewLine></code></pre><NewLine></blockquote><NewLine><p>as you can see now I removed stubs and using FloatFunctional but yet usage is not reduced</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 14: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>,</p><NewLine><p>Regarding CPU usage i debug the usage with</p><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""80954"" data-username=""supriyar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/db5fbb/40.png"" width=""20""/> supriyar:</div><NewLine><blockquote><NewLine><p>torch.autograd.profiler</p><NewLine></blockquote><NewLine></aside><NewLine><p>Here I’m listing the output:</p><NewLine><blockquote><NewLine><p>Usage of Float model:</p><NewLine><hr/><NewLine><p>Name                      Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls</p><NewLine><hr/><NewLine><p><strong>slow_conv_dilated2d</strong>       25.34%           <strong>127.183ms</strong>        98.72%           495.549ms        42.081us         11776<br/><NewLine>size                      15.79%           79.242ms         15.79%           79.242ms         0.463us          171097<br/><NewLine>_cat                      9.76%            49.007ms         11.61%           58.285ms         2.534ms          23<br/><NewLine>mkldnn_convolution        9.67%            48.534ms         19.48%           97.764ms         1.397ms          70<br/><NewLine>threshold                 6.45%            32.364ms         6.52%            32.726ms         1.091ms          30<br/><NewLine>slice                     5.26%            26.391ms         9.76%            49.012ms         2.065us          23737<br/><NewLine>native_layer_norm         3.79%            19.017ms         7.74%            38.849ms         669.817us        58<br/><NewLine><em>convolution              3.71%            18.632ms         86.18%           432.610ms        7.459ms          58<br/><NewLine>empty                     2.81%            14.117ms         2.82%            14.143ms         1.179us          11991<br/><NewLine>select                    2.65%            13.278ms         9.45%            47.425ms         4.027us          11778<br/><NewLine>as_strided                2.52%            12.638ms         2.52%            12.638ms         0.530us          23837<br/><NewLine>fill</em>                     2.37%            11.903ms         2.38%            11.933ms         2.026us          5889<br/><NewLine>add                       2.23%            11.208ms         4.53%            22.760ms         421.489us        54</p><NewLine><p>total time : 502 ms</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>Cpu usage of Quantized model without fusing:</p><NewLine><hr/><NewLine><p>Name                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls</p><NewLine><hr/><NewLine><p><strong>quantized::conv1d</strong>            81.39%           <strong>454.615ms</strong>        81.39%           454.615ms        7.838ms          58<br/><NewLine>quantized::add               9.25%            51.657ms         9.25%            51.657ms         1.987ms          26<br/><NewLine>quantized::layer_norm        7.62%            42.582ms         7.62%            42.582ms         1.468ms          29<br/><NewLine>relu                         0.63%            3.513ms          1.31%            7.296ms          121.595us        60<br/><NewLine>quantized::mul               0.59%            3.322ms          0.59%            3.322ms          3.322ms          1<br/><NewLine>quantized::linear            0.18%            1.019ms          0.18%            1.019ms          1.019ms          1</p><NewLine><p>total time : 558ms</p><NewLine></blockquote><NewLine><blockquote><NewLine><p>CPU usage of quantized model after fusing:</p><NewLine><hr/><NewLine><p>Name                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls</p><NewLine><hr/><NewLine><p><strong>quantized::conv1d</strong>            41.77%           <strong>239.086ms</strong>        41.77%           239.086ms        7.970ms          30<br/><NewLine><strong>quantized::conv1d_relu</strong>       39.94%           <strong>228.633ms</strong>        39.94%           228.633ms        8.165ms          28<br/><NewLine>quantized::add               9.35%            53.523ms         9.35%            53.523ms         2.059ms          26<br/><NewLine>quantized::layer_norm        7.67%            43.932ms         7.67%            43.932ms         1.515ms          29<br/><NewLine>quantized::mul               0.62%            3.564ms          0.62%            3.564ms          3.564ms          1<br/><NewLine>index_add_                   0.27%            1.542ms          0.54%            3.100ms          1.550ms          2<br/><NewLine>quantized::linear            0.18%            1.017ms          0.18%            1.017ms          1.017ms          1<br/><NewLine>relu                         0.06%            370.631us        0.13%            762.027us        190.507us        4</p><NewLine><p>total time : 572 ms</p><NewLine></blockquote><NewLine><p>If you see these three usages conv1d is taking high usage after quantization</p><NewLine><blockquote><NewLine><p>quantized::conv1d            <strong>239.086ms</strong><br/><NewLine>quantized::conv1d_relu         <strong>228.633ms</strong><br/><NewLine>slow_conv_dilated2d         <strong>127.183ms</strong></p><NewLine></blockquote><NewLine><p>Could anyone have any view why this was happened i.e., quantized conv1d increased CPU usage?</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>,<br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> REPLY 15: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><aside class=""quote no-group"" data-post=""15"" data-topic=""80954"" data-username=""BOLLOJU_ARAVIND""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/bolloju_aravind/40/24107_2.png"" width=""20""/> BOLLOJU_ARAVIND:</div><NewLine><blockquote><NewLine><p>quantized::conv1d <strong>239.086ms</strong><br/><NewLine>quantized::conv1d_relu <strong>228.633ms</strong><br/><NewLine>slow_conv_dilated2d <strong>127.183ms</strong></p><NewLine></blockquote><NewLine></aside><NewLine><p>Do you have any idea why this quantized Conv1d takes more time?</p><NewLine><p>Thanks <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 16: <div class=""post"" itemprop=""articleBody""><NewLine><p>We are currently implementing the operator using quantized::conv2d under the hood after unsqueezing the activation and weight tensors. That might be proving sub-optimal for certain input shapes.</p><NewLine><p>Could you give us details about the input and weight dimensions and the parameters (kernel, stride, pad, dilation) to conv1d in your case?</p><NewLine></div>; <NewLine> REPLY 17: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,</p><NewLine><p>Thanks for the reply.</p><NewLine><p>Here I’m attaching some layers in my model</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/e816d030ae0c4dcd6893f993ed0169f27b337f1d"" href=""https://discuss.pytorch.org/uploads/default/original/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d.png"" title=""Screenshot_2020-06-09-07-25-38-14""><img alt=""Screenshot_2020-06-09-07-25-38-14"" data-base62-sha1=""x79EjIEZb1jmzZ4dMTsxSmvuK4J"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d_2_10x10.png"" height=""279"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d_2_690x279.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d_2_690x279.png, https://discuss.pytorch.org/uploads/default/optimized/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d_2_1035x418.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/e/8/e816d030ae0c4dcd6893f993ed0169f27b337f1d.png 2x"" width=""690""/><div class=""meta""><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Screenshot_2020-06-09-07-25-38-14</span><span class=""informations"">1261×511 214 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg></div></a></div></p><NewLine><p>These are the  layers in float model</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 14: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 15: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 16: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 17: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> ,"REPLY_DATE 1: May 13, 2020, 10:50pm; <NewLine> REPLY_DATE 2: May 14, 2020,  3:06am; <NewLine> REPLY_DATE 3: May 20, 2020,  3:28am; <NewLine> REPLY_DATE 4: May 28, 2020,  3:57am; <NewLine> REPLY_DATE 5: May 20, 2020,  7:39am; <NewLine> REPLY_DATE 6: May 27, 2020,  1:18pm; <NewLine> REPLY_DATE 7: May 28, 2020,  4:01am; <NewLine> REPLY_DATE 8: May 28, 2020,  2:58pm; <NewLine> REPLY_DATE 9: May 29, 2020,  9:52am; <NewLine> REPLY_DATE 10: May 31, 2020,  2:56am; <NewLine> REPLY_DATE 11: June 1, 2020, 11:57am; <NewLine> REPLY_DATE 12: June 2, 2020,  3:28am; <NewLine> REPLY_DATE 13: June 2, 2020,  3:33am; <NewLine> REPLY_DATE 14: June 2, 2020,  8:03am; <NewLine> REPLY_DATE 15: June 8, 2020,  5:06am; <NewLine> REPLY_DATE 16: June 8, 2020, 10:27pm; <NewLine> REPLY_DATE 17: June 9, 2020,  2:00am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> REPLY 14 LIKES: ; <NewLine> REPLY 15 LIKES: ; <NewLine> REPLY 16 LIKES: ; <NewLine> REPLY 17 LIKES: ; <NewLine> 
84515,What is the use of fused modules in quantization?,2020-06-07T11:48:09.455Z,0,103,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am working on static quantization and have found that quantizing model with fuse module gives more accuracy than the models without applying fuse model.</p><NewLine><p>What is the meaning of ConvRelu2d? whether the batchnormalization part is fused with the convolution weights or bachnormalization is removed?</p><NewLine><p>Please do help me understand this concept.</p><NewLine></div>",https://discuss.pytorch.org/u/Midhilesh,(Midhilesh),Midhilesh,"June 7, 2020, 11:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/midhilesh"">@Midhilesh</a>,<br/><NewLine>What I understand from <code>ConvReLU2d()</code> is that it just converts from<br/><NewLine><code>Conv2d()</code>-&gt; <code>ReLU()</code>  to something like <code>ReLU(Conv2d())</code> .<br/><NewLine>However, there is no <code>BatchNorm2d</code> here, if you want to fuse even the BatchNorm layer, you could use this <code>ConvBnReLU2d()</code> .</p><NewLine><p>You will find more info from the quantization doc <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.intrinsic.ConvReLU2d"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#torch.nn.intrinsic.ConvReLU2d</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/chetan_patil; <NewLine> ,"REPLY_DATE 1: June 7, 2020,  3:48pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
80652,Question about quantization tutorial and fusing model,2020-05-11T07:36:15.266Z,3,298,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am new to Deep Learning and Pytorch. I am interested in quantization and have gone through the <a href=""https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html"" rel=""nofollow noopener"">Transfer learning</a> and <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">Post Training Static Quantization</a> tutorial. However, there are a few questions that i hope to get some idea from the community.</p><NewLine><p>For <a href=""https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html"" rel=""nofollow noopener"">Transfer learning</a>:</p><NewLine><ul><NewLine><li>I noticed that the quantized model implements a custom head, for fine tuning purpose. However, since the model is restructured due to the</li><NewLine></ul><NewLine><blockquote><NewLine><p>from torch import nn</p><NewLine><p>def create_combined_model(model_fe):</p><NewLine><h3>Step 1. Isolate the feature extractor.</h3><NewLine><p>model_fe_features = nn.Sequential(<br/><NewLine>model_fe.quant,  # Quantize the input<br/><NewLine>model_fe.conv1,<br/><NewLine>model_fe.bn1,<br/><NewLine>model_fe.relu,<br/><NewLine>model_fe.maxpool,<br/><NewLine>model_fe.layer1,<br/><NewLine>model_fe.layer2,<br/><NewLine>model_fe.layer3,<br/><NewLine>model_fe.layer4,<br/><NewLine>model_fe.avgpool,<br/><NewLine>model_fe.dequant,  # Dequantize the output<br/><NewLine>)</p><NewLine><h1>Step 2. Create a new “head”</h1><NewLine><p>new_head = nn.Sequential(<br/><NewLine>nn.Dropout(p=0.5),<br/><NewLine>nn.Linear(num_ftrs, 2),<br/><NewLine>)</p><NewLine><h1>Step 3. Combine, and don’t forget the quant stubs.</h1><NewLine><p>new_model = nn.Sequential(<br/><NewLine>model_fe_features,<br/><NewLine>nn.Flatten(1),<br/><NewLine>new_head,<br/><NewLine>)<br/><NewLine>return new_model</p><NewLine></blockquote><NewLine><ul><NewLine><li>Why is there no new <code>forward()</code> defined for it?  Can the <code>forward()</code> recognize the new network layout for the model automatically?</li><NewLine></ul><NewLine><p>For <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">Post Training Static Quantization</a>:</p><NewLine><ul><NewLine><li>I noticed that the tutorial transforms the pretrained model to quantized model by merging the intermediate operations such as <a href=""https://pytorch.org/docs/stable/nn.html#conv2d"" rel=""nofollow noopener""><code>nn.Conv2d()</code></a>, <a href=""https://pytorch.org/docs/stable/nn.html#relu"" rel=""nofollow noopener""><code>nn.ReLU()</code></a> and <a href=""https://pytorch.org/docs/stable/nn.html#batchnorm2d"" rel=""nofollow noopener""><code>nn.BatchNorm2d()</code></a>, into <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.intrinsic.ConvBnReLU2d"" rel=""nofollow noopener""> <code>ConvBnReLU2d()</code> </a>. I know that the guideline in <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">Quantization</a>, suggests to perform operation fusing whenever quantizing a model. But is it possible to implement each quantized modules independently without fusing all of them as one?  As i believe i have seen the quantization implementation of <a href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.Conv2d"" rel=""nofollow noopener""><code>nn.quantized.Conv2d()</code></a>, <a href=""https://pytorch.org/docs/stable/quantization.html#relu"" rel=""nofollow noopener""><code>nn.quantized.ReLU()</code></a>. (<em>Although there is no <code>nn.quantized.BatchNorm2d()</code> yet</em>).</li><NewLine><li>The reason i am asking is because i am interested in extracting the intermediate outputs. I would like to inspect the intermediate outputs such as output from <code>nn.quantized.Conv2d()</code> and <code>nn.quantized.ReLU()</code> independently. I believe if i fuse the module using <code>ConvBnReLU2d</code>, it would only yield me the final output that has gone through <code>BatchNorm2d()</code> and <code>ReLU()</code> instead of the intermediate outputs for each intermediate operations, right?</li><NewLine></ul><NewLine><p>I am new to this community and this is my first post. If this post does not follow the community guideline, please let me know. Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/JC_DL,,JC_DL,"May 11, 2020,  7:37am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For Post Training Static Quantization<br/><NewLine>I think you can leave Conv2d and ReLU separated, but it will impact the performance. It could work for debugging purpose. For batchnorm you have to fuse it with Conv since there’s no quantized batchnorm.</p><NewLine><p>cc <a class=""mention"" href=""/u/zafar"">@Zafar</a> for question on transfer learning.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jc_dl"">@JC_DL</a> we do support quantized conv, quantized relu and quantized batchnorm operators. So it should be possible to execute these operators standalone as well.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, may i know where i can find out more about quantized batchnorm? I did not see <code>BatchNorm2d()</code> listed under <code>torch.nn.quantized</code> in the <a href=""https://pytorch.org/docs/stable/quantization.html#id11"" rel=""nofollow noopener"">Quantization</a> page. Thanks.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe the docs haven’t been updated. Will do so.<br/><NewLine>Here is the code for quantized batchorm - <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/batchnorm.py#L10"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/batchnorm.py#L10</a></p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><ol><NewLine><li>Can you please explain why there is a performance impact if we don’t fuse the layers?</li><NewLine><li>what exactly the meaning of fusing layers?</li><NewLine><li>what does ConvRelu2d means? What has happened to the batch normalization layer?</li><NewLine></ol><NewLine><p>Please help me to understand!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/JC_DL; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Midhilesh; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  6:59pm; <NewLine> REPLY_DATE 2: May 12, 2020,  3:44am; <NewLine> REPLY_DATE 3: May 12, 2020,  5:09am; <NewLine> REPLY_DATE 4: May 13, 2020, 10:55pm; <NewLine> REPLY_DATE 5: June 7, 2020, 12:04pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
84431,Dynamic quantization error: Mixed serialization of script and non-script modules is not supported,2020-06-06T14:56:52.549Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have an issue in quantizing yolov3 model. I am working on this implementation: <a href=""https://github.com/eriklindernoren/PyTorch-YOLOv3"" rel=""nofollow noopener"">https://github.com/eriklindernoren/PyTorch-YOLOv3</a><br/><NewLine>I followed the dynamic quantization tutorial and the quantization code as following: loading the model, loading pretrained weights from Darknet and using the dynamic quantization.</p><NewLine><pre><code class=""lang-auto"">model = Darknet(opt.model_def, img_size=opt.img_size).to(device)<NewLine><NewLine>if opt.weights_path.endswith("".weights""):<NewLine>    # Load darknet weights<NewLine>    model.load_darknet_weights(opt.weights_path)<NewLine>else:<NewLine>    # Load checkpoint weights<NewLine>    model.load_state_dict(torch.load(opt.weights_path))<NewLine><NewLine>qmodel = torch.quantization.quantize_dynamic(model, dtype=torch.qint8)<NewLine></code></pre><NewLine><p>But I got this error message:</p><NewLine><p>“For purely script modules use my_script_module.save() instead.”)<br/><NewLine>_pickle.PickleError: ScriptModules cannot be deepcopied using copy.deepcopy or saved using torch.save. Mixed serialization of script and non-script modules is not supported. For purely script modules use my_script_module.save() instead.</p><NewLine><p>Do you have any suggestions guys?</p><NewLine></div>",https://discuss.pytorch.org/u/baheytharwat,(Bahey Tharwat),baheytharwat,"June 6, 2020,  2:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It looks like you are trying to quantize the scripted net.<br/><NewLine>The correct order seems like first quantize your net then script it!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/huoge; <NewLine> ,"REPLY_DATE 1: June 16, 2020,  2:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
84144,Tied &ldquo;conv1d&rdquo; and &ldquo;conv_transpose1d&rdquo; not geting the same result as the input,2020-06-04T07:43:10.470Z,0,68,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Here is a snippet of my test code：</p><NewLine><pre><code class=""lang-python"">    data = torch.tensor([[[1.], [2.], [3.], [4.]]])  # N=1 * L=4 * C_in=1<NewLine>    data = data.permute(0, 2, 1)<NewLine>    conv = nn.Conv1d(1, 1, 2, bias=None)<NewLine>    print(conv.weight)<NewLine>    output = conv(data)<NewLine>    print(""output : {}"".format(output))<NewLine>    dweight = conv.weight.transpose(0, 1).flip(2, )   # dweight = conv.weight.transpose(0, 1)<NewLine>    print(""dweight : {}"".format(dweight))<NewLine><NewLine>    dconv = F.conv_transpose1d(input=output, weight=dweight, bias=None)<NewLine>    print(dconv)   # != data<NewLine></code></pre><NewLine><p>The result I suppose it would be is: input -&gt; conv1d(input) -&gt; **conv_transpose1d(conv1d(input)) should be <strong>equal</strong> to <strong>input</strong>.</p><NewLine><p>But they are <strong>not equal</strong>, whether I flip the temporal axis or not, but they are supposed to be the same, right?</p><NewLine><p>I’m really confused and frustrated here, could anyone figure it out?</p><NewLine></div>",https://discuss.pytorch.org/u/111324,(啸寒 孙),111324,"June 4, 2020,  8:03am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please don’t tag specific people, as this might discourage others to answer.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think I misunderstand the “tied weight” concept.</p><NewLine><p>I wrote the conv_transposed1d in <strong>doubly block circulant matrix</strong> form and I find that one don’t need to flip the temporal axis actually.</p><NewLine><p>Suppose the conv1d’s matrix is <img height=""17"" src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5CLarge%20W"" width=""22""/> and the corresponding conv_transpose1d’s matrix is  <img height=""22"" src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5CLarge%20W%5E%7B%5Cmathr%7BT%7D%7D"" width=""37""/>.</p><NewLine><p>The square matrix <img height=""22"" src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5CLarge%20W%5Ctimes%20W%5E%7B%5Cmathr%7BT%7D%7D"" width=""80""/> apprently is not always identity matrix. So the result need not to be identical to the input.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/111324; <NewLine> ,"REPLY_DATE 1: June 4, 2020,  7:59am; <NewLine> REPLY_DATE 2: June 4, 2020,  9:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
83916,No performance improvement using quantization model in pytorch,2020-06-02T17:38:59.173Z,0,139,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have trained a model in pytorch with float data type. I want to improve my inference time by converting this model to quantized model. I have used  <strong>torch.quantization.convert</strong> and <strong>torch.quantization.quantize_dynamic</strong>  api to convert my model’s weight to uint8 data type. However, when I use this model for inference, I do not get any performance improvement. Am I doing something wrong here ?</p><NewLine><p>The Unet Model code:</p><NewLine><pre><code class=""lang-auto"">def gen_initialization(m):<NewLine>    if type(m) == nn.Conv2d:<NewLine>        sh = m.weight.shape<NewLine>        nn.init.normal_(m.weight, std=math.sqrt(2.0 / (sh[0]*sh[2]*sh[3])))<NewLine>        nn.init.constant_(m.bias, 0)<NewLine>    elif type(m) == nn.BatchNorm2d:<NewLine>        nn.init.constant_(m.weight, 1)<NewLine>        nn.init.constant_(m.bias, 0)<NewLine><NewLine>class TripleConv(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super(TripleConv, self).__init__()<NewLine>        mid_ch = (in_ch + out_ch) // 2<NewLine>        self.conv = nn.Sequential(<NewLine>            nn.Conv2d(in_ch, mid_ch, kernel_size=3, stride=1, padding=1, bias=True),<NewLine>            nn.BatchNorm2d(num_features=mid_ch),<NewLine>            nn.LeakyReLU(negative_slope=0.1),<NewLine>            nn.Conv2d(mid_ch, mid_ch, kernel_size=3, stride=1, padding=1, bias=True),<NewLine>            nn.BatchNorm2d(num_features=mid_ch),<NewLine>            nn.LeakyReLU(negative_slope=0.1),<NewLine>            nn.Conv2d(mid_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),<NewLine>            nn.BatchNorm2d(num_features=out_ch),<NewLine>            nn.LeakyReLU(negative_slope=0.1)<NewLine>        )<NewLine>        self.conv.apply(gen_initialization)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.conv(x)<NewLine><NewLine><NewLine>class Down(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super(Down, self).__init__()<NewLine>        self.triple_conv = TripleConv(in_ch, out_ch)<NewLine>        self.avg_pool_conv = nn.AvgPool2d(2, 2)<NewLine>        self.in_ch = in_ch<NewLine>        self.out_ch = out_ch<NewLine><NewLine>    def forward(self, x):<NewLine>        self.cache = self.triple_conv(x)<NewLine>        pad = torch.zeros(x.shape[0], self.out_ch - self.in_ch, x.shape[2], x.shape[3], device=x.device)<NewLine>        x = torch.cat((x, pad), dim=1)<NewLine>        self.cache += x<NewLine>        return self.avg_pool_conv(self.cache)<NewLine><NewLine><NewLine>class Center(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super(Center, self).__init__()<NewLine><NewLine>        self.conv = nn.Sequential(<NewLine>            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),<NewLine>            nn.BatchNorm2d(num_features=out_ch),<NewLine>            nn.LeakyReLU(negative_slope=0.1, inplace=True)<NewLine>        )<NewLine>        self.conv.apply(gen_initialization)<NewLine><NewLine>    def forward(self, x):<NewLine>        return self.conv(x)<NewLine><NewLine><NewLine>class Up(nn.Module):<NewLine>    def __init__(self, in_ch, out_ch):<NewLine>        super(Up, self).__init__()<NewLine>        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear',<NewLine>                                    align_corners=True)<NewLine>        self.triple_conv = TripleConv(in_ch, out_ch)<NewLine><NewLine>    def forward(self, x, cache):<NewLine>        x = self.upsample(x)<NewLine>        x = torch.cat((x, cache), dim=1)<NewLine>        x = self.triple_conv(x)<NewLine>        return x<NewLine><NewLine><NewLine>class UNet(nn.Module):<NewLine>    def __init__(self, in_ch, first_ch=None):<NewLine>        super(UNet, self).__init__()<NewLine><NewLine>        if not first_ch:<NewLine>            first_ch = 32<NewLine><NewLine>        self.down1 = Down(in_ch, first_ch)<NewLine>        self.down2 = Down(first_ch, first_ch*2)<NewLine>        self.down3 = Down(first_ch*2, first_ch*4)<NewLine>        self.down4 = Down(first_ch*4, first_ch*8)<NewLine>        self.center = Center(first_ch*8, first_ch*8)<NewLine>        self.up4 = Up(first_ch*8*2, first_ch*4)<NewLine>        self.up3 = Up(first_ch*4*2, first_ch*2)<NewLine>        self.up2 = Up(first_ch*2*2, first_ch)<NewLine>        self.up1 = Up(first_ch*2, first_ch)<NewLine>        self.output = nn.Conv2d(first_ch, in_ch, kernel_size=3, stride=1,<NewLine>                                padding=1, bias=True)<NewLine>        self.output.apply(gen_initialization)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.down1(x)<NewLine>        x = self.down2(x)<NewLine>        x = self.down3(x)<NewLine>        x = self.down4(x)<NewLine>        x = self.center(x)<NewLine>        x = self.up4(x, self.down4.cache)<NewLine>        x = self.up3(x, self.down3.cache)<NewLine>        x = self.up2(x, self.down2.cache)<NewLine>        x = self.up1(x, self.down1.cache)<NewLine>        return self.output(x)<NewLine></code></pre><NewLine><p>The inference code:</p><NewLine><pre><code class=""lang-auto"">from tqdm import tqdm<NewLine>import os<NewLine>import numpy as np<NewLine>import torch<NewLine>import gan_network<NewLine>import torch.nn.parallel<NewLine>from torch.utils.data import DataLoader<NewLine>import torch.utils.data as data<NewLine>import random<NewLine>import glob<NewLine>import scipy.io<NewLine>import time<NewLine>os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   # see issue #152<NewLine>os.environ[""CUDA_VISIBLE_DEVICES""]=""0""<NewLine><NewLine><NewLine>class DataFolder(data.Dataset):<NewLine>    def __init__(self, file):<NewLine>        super(DataFolder, self).__init__()<NewLine>        self.image_names = []<NewLine>        fid = file<NewLine>        for line in fid:<NewLine>            # line = line[:-1]<NewLine>            if line == '':<NewLine>                continue<NewLine>            # print(line)<NewLine>            self.image_names.append(line)<NewLine>        random.shuffle(self.image_names)<NewLine>        self.image_names = self.image_names[0:]<NewLine><NewLine>    def __len__(self):<NewLine>        return len(self.image_names)<NewLine><NewLine>    def __getitem__(self, index):<NewLine>        path = self.image_names[index]<NewLine>        img = np.load(path)<NewLine>        img = np.rollaxis(img, 2, 0)<NewLine>        img = torch.from_numpy(img[:, :, :])<NewLine>        return img, path<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    batch_size = 1<NewLine>    image_size = 2048<NewLine>    channels = 6<NewLine>    model_path = 'D:/WorkProjects/Network_Training_Aqusens/FullFovReconst/network/network_epoch9.pth'<NewLine>    test_data = glob.glob('D:/save/temp/*.npy')<NewLine>    dest_dir = 'D:/save/temp/results/'<NewLine><NewLine>    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')<NewLine>    net = gan_network.UNet(6, 32)<NewLine>    if torch.cuda.device_count() &gt; 1:<NewLine>        net = torch.nn.DataParallel(net)<NewLine>    net.to(device)<NewLine>    net.load_state_dict(torch.load(model_path))<NewLine>    quantized_model = torch.quantization.quantize_dynamic(net, {torch.nn.Conv2d, torch.nn.BatchNorm2d}, inplace=False)<NewLine><NewLine>    dataset = DataFolder(file=test_data)<NewLine>    print(f'{len(dataset)}')<NewLine>    data_loader = DataLoader(dataset=dataset, num_workers=4,<NewLine>                             batch_size=batch_size, shuffle=False,<NewLine>                             drop_last=False, pin_memory=True)<NewLine>    input = torch.Tensor(batch_size, channels, image_size, image_size).to(device)<NewLine><NewLine>    t0 = time.time()<NewLine>    with torch.no_grad():<NewLine>        for i, batch in enumerate(tqdm(data_loader)):<NewLine>            input.copy_(batch[0])<NewLine>            output = net(input).cpu().clone().numpy()<NewLine>            np.array(output)<NewLine>            output = np.rollaxis(output, 1, 4)<NewLine>            for num in range(batch_size):<NewLine>                arr = output[num, :, :, :]<NewLine>                file_name = os.path.basename(batch[1][num])<NewLine>                save_name = os.path.join(dest_dir, file_name)<NewLine>                save_name = save_name.replace("".npy"", """")<NewLine>                scipy.io.savemat(save_name+'.mat', {'output': arr})<NewLine>    t1 = time.time()<NewLine>    print(f'Elapsed time = {t1-t0}')<NewLine></code></pre><NewLine><p>For models <strong>net</strong> and <strong>quantized_model</strong>, i get the elapsed time around 30 seconds for 12 images passed through them.</p><NewLine></div>",https://discuss.pytorch.org/u/dnaik,(Dhruvin),dnaik,"June 2, 2020,  5:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I can no longer find the requisite doc link, but Pytorch dynamic quantization is currently (v1.5) only provided for Linear and LSTM layers. A model that doesn’t have a high proportion of those will not benefit from dynamic quantization.</p><NewLine><p>(To confirm, print() a quantized model to see which layers have been replaced.)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dnaik"">@dnaik</a></p><NewLine><p>In the quoted message I didn’t find that you did quantized the model with convert() API. If it is, then quantize the model with convert() API.<br/><NewLine>Refer to this Doc to quantize model with prepare &amp; convert() APIs : <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">Static quantization</a></p><NewLine><p>Quantize_dynamic() is not applicable to your model because as of now it only supports to quantize lSTM and Linear layers.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/vladium; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> ,"REPLY_DATE 1: June 2, 2020,  7:09pm; <NewLine> REPLY_DATE 2: June 3, 2020,  3:13am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
83732,Accessing accumulated quantized result,2020-06-01T15:51:50.988Z,0,60,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>The PyTorch quantized operations are great but they return the result after it has been quantized back to 8-bit integer. Is there a simple way of accessing the accumulated result at int32? Are there other libraries, such as Caffe or a different quantization backend, that have this API?</p><NewLine></div>",https://discuss.pytorch.org/u/samgd,(Sam),samgd,"June 1, 2020,  3:51pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Re-quantization (conversion from int32 to int8) is performed in the quantized operator libraries - FBGEMM and QNNPACK.<br/><NewLine>Currently there is no easy way to access this value from pytorch API.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: June 2, 2020,  3:30am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
83314,Quantized MaxPool2d and AdaptiveAvgPool2d,2020-05-29T04:10:38.268Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi all,</p><NewLine><p>I have been experimenting with the post static quantization feature on VGG-16.<br/><NewLine>I know that <code>torch.quantization.convert()</code> will automatically remap every layer in the model to its quantized implementation.</p><NewLine><p>However, i noticed that, a few types of layer is not converted, which is:</p><NewLine><blockquote><NewLine><p><code>nn.MaxPool2d()</code> , <code>nn.AdaptiveAvgPool2d()</code> and <code>nn.Dropout()</code></p><NewLine></blockquote><NewLine><p>I believe <code>nn.Dropout()</code> should not be an issue, whether its quantized or not.<br/><NewLine>However, i am not sure if <code>nn.MaxPool2d()</code> and  <code>nn.AdaptiveAvgPool2d()</code> would do any difference if it is not quantized.</p><NewLine><p>I have seen <code>nn.quantized.MaxPool2d()</code> being mentioned <a href=""https://pytorch.org/docs/stable/quantization.html#torch-nn-quantized"" rel=""nofollow noopener"">here</a> and tried to remap my layer to this module. But, it seems like it is still referring to <code>nn.modules.pooling.MaxPool2d()</code> when i check the layer type after reassigning.</p><NewLine><p>I have also seen <code>nn.quantized.functional.MaxPool2d()</code> and <code>nn.quantized.functional.AdaptiveAvgPool2d()</code> being mentioned in the Quantization <a href=""https://pytorch.org/docs/stable/quantization.html#torch-nn-quantized-functional"" rel=""nofollow noopener"">documentation</a>. But i have read from the forum, and found that, it is not conventional to directly call <code>functional</code>, instead, its <code>module</code> or its wrapper class should be called.</p><NewLine><p>So, i would like to ask, is there any effect to my quantized model performance if i don’t change the <code>nn.MaxPool2d()</code> and <code>nn.AdaptiveAvgPool2d()</code> to their quantized version?</p><NewLine><p>Should i just leave <code>nn.MaxPool2d()</code> and <code>nn.AdaptiveAvgPool2d()</code> as it is?</p><NewLine><p>Or, if i should change to their quantized implementation, how should i do it?</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/JC_DL,,JC_DL,"May 29, 2020,  4:10am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You do not need to change MaxPool2d() and adaptiveAvgPool2d() from nn to nn.quantized. These operations do not require calibration and are automatically converted to quantized operations when convert is called. Under the hood, these modules call the appropriate function when quantized values are passed as input.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: May 29, 2020, 10:07pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
82950,Problem about training with int8,2020-05-26T10:44:28.757Z,0,182,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,</p><NewLine><p>Recently, we are focusing on training with int8, not inference on int8. Considering the numerical limitation of int8, at first we keep all parameters in fp32 and only quantize convolution layer (conduct int8 operation) as it is the most compute-intensive part of a model. During the past months, we have achieved some progress (such accuracy comparable to fp32 training and faster than fp32 training), and our paper is accepted by CVPR2020 (<a href=""https://arxiv.org/pdf/1912.12607.pdf"" rel=""nofollow noopener"">https://arxiv.org/pdf/1912.12607.pdf</a>).</p><NewLine><p>Now, we want to further explore quantization of other layers like ReLU, Pooling and BatchNorm, and keep dataflow in int8 in order to save memory.<br/><NewLine>(1) However, we could not use int8 tensor as input &amp; output of a layer in PyTorch due to the autograd mechanism. Would you consider supporting int8 tensor in the future?<br/><NewLine>(2) Moreover, we want to pass quantization parameters like scale from layer to layer, but there are some problems at the shortcut connection. During backward, the autograd mechanism will add the gradient from main path and the gradient from the shortcut connection automatically. As we map quantization parameters to tensor, the quantization parameters are lost after the add operation. Could you provide some suggestions?</p><NewLine><p>Many thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/zfonemore,(zhu feng),zfonemore,"May 26, 2020, 10:44am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think there are some work planned for int8 training, cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> for more details.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/hx89"">@hx89</a> , I am also one of the authors of INT8 training paper on CVPR2020.<br/><NewLine>Firstly, I want to clarify that we are focus on <strong>using INT8 computation to speed up training process</strong> , not for quantization aware training. It means that we need to quantize gradient on convolution backward not only for forward.</p><NewLine><p>we are now achieve nearly no accuracy (&lt; 1% TOP1) decent on ImageNet of ResNet / MobileNet/ Inception/ ShuffleNet, Even on detection task on Pascal VOC and COCO with RetinaNet and FasterRCNN we got only ~1% mAP drop. We can check accuracy table on paper:</p><NewLine><p>(see table 7,8 and 9)</p><NewLine><p>And we also considering about INT8 computation implement and overhead reducing (see Section 3.6. General Purpose Training Framework)</p><NewLine><p>We implement with DP4A on GTX1080TI and finally get 1.6x(forward) and 1.9x(backward) speed on convolution over cuDNN:</p><NewLine><p>(see Figure 8.)</p><NewLine><p>We are now plan INT8 Training V2 about quantization all CNN layer include ReLU, Pooling, Add. But We found pytorch not yet support INT8 gradient backward, So we need Pytorch team to give some help.</p><NewLine><p>Feel great to see your response!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi.<br/><NewLine>We are interested in your research, could you share your code?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Forwil_Yu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/111179; <NewLine> ,"REPLY_DATE 1: May 26, 2020,  8:55pm; <NewLine> REPLY_DATE 2: May 28, 2020, 12:58pm; <NewLine> REPLY_DATE 3: September 28, 2020,  2:50am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> 
82566,The expanded size of the tensor must match the existing size at non-singleton dimension,2020-05-23T05:31:29.020Z,1,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Trying to quantize the object detection model of <a href=""https://github.com/sacmehta/EdgeNets"" rel=""nofollow noopener"">EdgeNet2</a> but fail, using the pretrained model from <a href=""https://github.com/sacmehta/EdgeNets/blob/master/model/detection/model_zoo/README.md"" rel=""nofollow noopener"">here</a>(ms coco 300x300).</p><NewLine><pre><code>    qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine>    print(qconfig)		<NewLine>    model.eval()<NewLine>    model.qconfig = qconfig<NewLine>    torch.quantization.prepare(model, inplace=True)		        <NewLine>	<NewLine>    predictor = BoxPredictor(cfg=cfg, device=device)<NewLine>    #works fine if I do not run the main_images<NewLine>    main_images(predictor=predictor, model=model, object_names=object_names,<NewLine>                in_dir=args.im_dir, out_dir=args.save_dir, device=device)<NewLine>	<NewLine>    # Convert to quantized model<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    torch.save(model.state_dict(), ""espnet2v_300.pt"")        		<NewLine>    print(""save as quantize model"")		       <NewLine><NewLine><NewLine>def main_images(predictor, model, object_names, in_dir, out_dir, device='cuda'):<NewLine>    png_file_names = glob.glob(in_dir + os.sep + '*.png')<NewLine>    jpg_file_names = glob.glob(in_dir + os.sep + '*.jpg')<NewLine>    file_names = png_file_names + jpg_file_names<NewLine><NewLine>    if len(file_names) == 0:<NewLine>        print_error_message('No image files in the folder')<NewLine><NewLine>    # model in eval mode<NewLine>    model.eval()<NewLine>    with torch.no_grad():<NewLine>        for img_name in file_names:<NewLine>            image = cv2.imread(img_name)<NewLine>            predictor.predict(model, image, is_scaling=False)                <NewLine></code></pre><NewLine><p>If I do not run the main_images function, I can quantize the model, If I  run the main_images function, I received error message.</p><NewLine><p><strong>The expanded size of the tensor (243076) must match the existing size (262144) at non-singleton dimension 0.  Target sizes: [243076].  Tensor sizes: [262144]</strong></p><NewLine><p>The main_images function works fine if I do not quantize it, any idea how should I fix this?Or do anyone know an object detection project of pytorch suit for quantization? Thanks</p><NewLine><p>ps : Tried with the model of tflite, but accuracy is not that good.</p><NewLine></div>",https://discuss.pytorch.org/u/ngap_wei_Tham,(Ngap Wei Tham),ngap_wei_Tham,"May 23, 2020,  5:31am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Based on the error message it seems you might be passing the wrong input shape.<br/><NewLine>Is this error specific to the quantization module, i.e. is it running fine without quantization?<br/><NewLine>Could you try to resize the <code>image</code> array to the expected shape?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""82566"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>is it running fine without quantization?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, totally fine.</p><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""82566"" data-username=""ptrblck""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/ptrblck/40/1823_2.png"" width=""20""/> ptrblck:</div><NewLine><blockquote><NewLine><p>Could you try to resize the <code>image</code> array to the expected shape?</p><NewLine></blockquote><NewLine></aside><NewLine><p>Already do that, it works before quantize</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ngap_wei_Tham; <NewLine> ,"REPLY_DATE 1: May 24, 2020,  8:20am; <NewLine> REPLY_DATE 2: May 25, 2020,  4:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80343,Expending PyTorch with lower than 8-bit Quantization,2020-05-08T15:11:40.327Z,8,277,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am interested in using PyTorch for 1-bit neural network training. It seems to me that pytorch now only supports <code>dtype=qint8</code>. I am wondering if there is an good guide for PyTorch dtype system and how to expanding it.</p><NewLine><p>Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/lijunzh,(Lijun Zhu),lijunzh,"May 8, 2020,  3:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> has a diff out, but it’s not landed yet: <a href=""https://github.com/pytorch/pytorch/pull/33743"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33743</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>, that’s good info to keep an eye on.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/33743"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33743 </a> give a nice touch on the sub-8-bit quantization. However, if I want to do some 1-bit quantization which quantizes the feature map and weight matrices into {-1, 1}.  This may requires further changes in the <code>qscheme</code>. I am guessing that will require me add some PyTorch intrinsics in ATen? Or there is a better way to accomendate that need?</p><NewLine><p>In any case, I am looking forward to see <a href=""https://github.com/pytorch/pytorch/pull/33743"" rel=""nofollow noopener"">33743</a> land soon.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""80343"" data-username=""lijunzh""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/ac91a4/40.png"" width=""20""/> lijunzh:</div><NewLine><blockquote><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/33743"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33743 </a> give a nice touch on the sub-8-bit quantization. However, if I want to do some 1-bit quantization which quantizes the feature map and weight matrices into {-1, 1}. This may requires further changes in the <code>qscheme</code> . I am guessing that will require me add some PyTorch intrinsics in ATen? Or there is a better way to accomendate that need?</p><NewLine></blockquote><NewLine></aside><NewLine><p>right, if it is {-1, 1} it is not probably not affine quantization, what would you quantize 0 into?<br/><NewLine>I think you’ll probably need to extend qscheme and implement a new quantizer(<a href=""https://codebrowser.bddppq.com/pytorch/pytorch/aten/src/ATen/quantized/Quantizer.h.html"" rel=""nofollow noopener"">https://codebrowser.bddppq.com/pytorch/pytorch/aten/src/ATen/quantized/Quantizer.h.html</a>) to support this.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>if it is {-1, 1} it is not probably not affine quantization, what would you quantize 0 into?</p><NewLine></blockquote><NewLine><p>I am trying to follow the <a href=""https://arxiv.org/abs/1603.05279"" rel=""nofollow noopener"">XNOR-Net</a> paper and its variant which quantizes the filter weights <strong>W</strong> into <strong>B</strong> such that <strong>W</strong> approximates a<strong>B</strong> where a is a real valued scalar. Thus, by the equation (4) in that paper, <strong>B</strong>i = +1 if <strong>W</strong>i &gt;= 0 and <strong>B</strong>i = -1 if <strong>W</strong>i &lt; 0.</p><NewLine><p>I am looking at the ATen code, it seems to be that if we want to add the support such binary quantization scheme, we will have to recompile PyTorch locally in order to have the new ATen library takes effect. Is there any way I can do it using the official PyTorch distribution without re-compilation?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, that is correct, you’ll need to implement a new quantization scheme, and probably adding a new quantize function <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3809"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3809</a> etc. we don’t have a good way to simulate this in current codebase I think. since this level of support is targeted at production.</p><NewLine><p>However, if you just want to simulate the accuracy of the model in training, you might get away with less complete support in core, for example, just add a new quantization scheme and add support for it in the fake quantize module: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/fake_quantize.py#L92"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/fake_quantize.py#L92</a>.</p><NewLine><p>But anyways you’ll probably need to add a new quantization scheme <a href=""https://github.com/pytorch/pytorch/blob/master/c10/core/QScheme.h"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/c10/core/QScheme.h</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the detailed explanation. This is very helpful.</p><NewLine><p>I mainly target at evaluating models in 1-bit precision with QAT, so I guess the second approach is good enough (for now). It seems that I still will need touch the Aten/c10 code and recompile it locally. If so, is this something the PyTorch team interested to merge upstream in the future (assuming my code meets the quality requirement)?</p><NewLine><p>On a related note, Nvidia’s new <a href=""https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/"" rel=""nofollow noopener"">A100 architecture</a> will support binary (1-bit) precision.</p><NewLine><blockquote><NewLine><p>Acceleration for all data types, including FP16, BF16, TF32, FP64, INT8, INT4, and Binary</p><NewLine></blockquote><NewLine><p>This is not too far away from the production. Details can also be found in their <a href=""https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf"" rel=""nofollow noopener"">white paper</a>. It feel like an interesting direction for PyTorch community to explore and will be meaningful if we can support in the long-run.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah low precision quantization is definitely something we want to pursue, but it may not need extra quantization schemes to support them, although we’ll need new data type support. for example, we can have per tensor affine quantization(existing quantization scheme) with Int4(new data type).</p><NewLine><p>In the case of 1-bit precision to {1, -1}, we also need a new quantization scheme since it is not affine quantization. if the integer values are consecutive, e.g. {-1, 0, 1}, {0, 1}, I think we should be able to represent it with per tensor affine quantization and a new INT1/INT2 data type.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I agree with the comment of sub-8-bit quantization. We should be able to support 2-7 bit using the existing infrastructure with some new data types INT2-7.</p><NewLine><p>In the case of 1-bit (binary), you can represent {-1, 1} in {0, 1} by assigning -1 to 0. In fact, that’s what will be implemented in hardware. However, that means you will replace multiplication by XNOR. This change results in a separate set of operators/functionals/modules need to be overload for binary network. From math point of view, I would like to see BNN implemented in this way (which exactly match the hardware). However, it is a lot of work and hard to be maintained (separately from all other NN modules). Frankly, an engineer will argue that there is no significant benefit of doing so. I feel like a new data type BINT1 for {-1, 1} (to be different from INT1 for {0, 1}) is a better choice.</p><NewLine><p>I will try to experiment with this idea and submit issue/PR in the coming months.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lijunzh; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lijunzh; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/lijunzh; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/lijunzh; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/lijunzh; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  6:04pm; <NewLine> REPLY_DATE 2: May 8, 2020,  6:25pm; <NewLine> REPLY_DATE 3: May 8, 2020,  6:32pm; <NewLine> REPLY_DATE 4: May 8, 2020,  6:55pm; <NewLine> REPLY_DATE 5: May 8, 2020,  8:01pm; <NewLine> REPLY_DATE 6: May 20, 2020,  9:50pm; <NewLine> REPLY_DATE 7: May 21, 2020,  1:32pm; <NewLine> REPLY_DATE 8: May 21, 2020,  4:23pm; <NewLine> REPLY_DATE 9: May 21, 2020,  5:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: 1 Like; <NewLine> 
82043,Loading a dynamically quantized Transformers model,2020-05-19T19:54:49.015Z,2,143,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’ve trained a custom transformer model and followed <a href=""https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html#serialize-the-quantized-model"" rel=""nofollow noopener"">this</a>  to save a quantized model.</p><NewLine><p>However when I try to load the model using</p><NewLine><pre><code class=""lang-auto"">model.load_state_dict(torch.load('path'))<NewLine></code></pre><NewLine><p>I receive the following error:</p><NewLine><p>Missing key(s) in state_dict: “xxxxx.weight”,<br/><NewLine>Unexpected key(s) in state_dict: “xxxx.scale, xxxx.zero_point, …”</p><NewLine><p>It looks like the names of the original parameters of the model have been changed. Can anyone help with how I can resolve this error?</p><NewLine></div>",https://discuss.pytorch.org/u/Pramodith,(Pramodith Bprum),Pramodith,"May 19, 2020,  7:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>While loading the model is the <code>model</code> now a quantized model? If you convert the model to quantized model and then load the quantized state_dict it should work.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m not sure that I understand, assuming class A inherits from nn.Module and corresponds to the architecture of my dnn.</p><NewLine><p><code>model = A()</code><br/><NewLine>is essentially all I do. Do I need to do anything to quantize it?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok I think I get it now, I have to do something like this after</p><NewLine><p>model = A</p><NewLine><pre><code class=""lang-auto"">quantized_model = torch.quantization.quantize_dynamic(<NewLine>    model, {torch.nn.Linear}, dtype=torch.qint8<NewLine>)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Pramodith; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Pramodith; <NewLine> ,"REPLY_DATE 1: May 21, 2020,  3:47am; <NewLine> REPLY_DATE 2: May 21, 2020,  3:44am; <NewLine> REPLY_DATE 3: May 21, 2020,  3:46am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
80707,"RuntimeError: Could not run &lsquo;aten::thnn_conv2d_forward&rsquo; with arguments from the &lsquo;QuantizedCPUTensorId&rsquo; backend. &lsquo;aten::thnn_conv2d_forward&rsquo; is only available for these backends: [CPUTensorId, VariableTensorId]",2020-05-11T13:56:31.029Z,6,699,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to do post quantization.<br/><NewLine>added QuantStub() and DeQuantStub() in the model</p><NewLine><pre><code>   super(ConvTasNet, self).__init__()<NewLine>    # Hyper-parameter<NewLine>    self.N, self.L, self.B, self.Sk, self.H, self.P, self.X, self.R, self.C = N, L, B, Sk, H, P, X, R, C<NewLine>    self.norm_type = norm_type<NewLine>    self.causal = causal<NewLine>    self.mask_nonlinear = mask_nonlinear<NewLine>    self.skip_conn = []<NewLine>    #quantization<NewLine>    self.quant = QuantStub()<NewLine>    self.dequant = DeQuantStub()<NewLine>    # Components<NewLine>    self.encoder = Encoder(L, N)<NewLine>    self.separator = TemporalConvNet(N, B, Sk, H, P, X, R, C, norm_type, causal, mask_nonlinear)<NewLine>    self.decoder = Decoder(N, L)<NewLine>    # init<NewLine>    for p in self.parameters():<NewLine>        if p.dim() &gt; 1:<NewLine>            nn.init.xavier_normal_(p)<NewLine><NewLine>def forward(self, mixture):<NewLine>    """"""<NewLine>    Args:<NewLine>        mixture: [M, T], M is batch size, T is #samples<NewLine>    Returns:<NewLine>        est_source: [M, C, T]<NewLine>    """"""<NewLine>    mixture = self.quant(mixture)<NewLine>    mixture_w = self.encoder(mixture)<NewLine>    est_mask = self.separator(mixture_w)<NewLine>    est_source = self.decoder(mixture_w, est_mask)<NewLine>    est_source = self.dequant(est_source)<NewLine><NewLine>    return est_source<NewLine></code></pre><NewLine><p>Now,  I want to evaluate model without and with Quantize converted model</p><NewLine><pre><code> if __name__ == '__main__':<NewLine>args = parser.parse_args()<NewLine>print(args)<NewLine>#evaluate(args)<NewLine><NewLine><NewLine>num_calibration_batches = 10<NewLine>model = ConvTasNet.load_model('E:\\Project\\MVNS_EC\\quantization\\Simple-Neural-Nets-with-PyTorch-master\\Eager_Post_quant\\MVNS_model\\Quantization\\model\\final.pth.tar')<NewLine>model.eval()<NewLine>model.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine>evaluate(model,args)<NewLine>torch.quantization.convert(model, inplace=True)<NewLine>evaluate(model,args)<NewLine></code></pre><NewLine><p>Evaluation without quantization is done, and also the model is converted to Quantized model.<br/><NewLine>I am recieving error in the last line, when I am trying to evaluate with the quantized model.<br/><NewLine>Please guide me, what is that I am doing wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Sasank_Kottapalli,(Sasank Kottapalli),Sasank_Kottapalli,"May 11, 2020,  1:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you add model.to(‘cpu’) before model.eval()?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I added model.to(‘cpu’) before model.eval(), still facing the same issue.<br/><NewLine>is it because I didn’t fuse the model? or is it the placement of QuantStub()?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using conv1d, prelu and layernorm layers.<br/><NewLine>Does pytorch support quantization to these layers?<br/><NewLine>or is it possible only for the following layers?</p><NewLine><p>(Actual layer : quantized layer)<br/><NewLine>nn.Linear: nnq.Linear,<br/><NewLine>nn.ReLU: nnq.ReLU,<br/><NewLine>nn.ReLU6: nnq.ReLU6,<br/><NewLine>nn.Conv2d: nnq.Conv2d,<br/><NewLine>nn.Conv3d: nnq.Conv3d,<br/><NewLine>nn.BatchNorm2d: nnq.BatchNorm2d,<br/><NewLine>nn.BatchNorm3d: nnq.BatchNorm3d,<br/><NewLine>QuantStub: nnq.Quantize,<br/><NewLine>DeQuantStub: nnq.DeQuantize</p><NewLine><pre><code># Wrapper Modules:<NewLine>nnq.FloatFunctional: nnq.QFunctional<NewLine><NewLine># Intrinsic modules:<NewLine>nni.ConvReLU2d: nniq.ConvReLU2d,<NewLine>nni.ConvReLU3d: nniq.ConvReLU3d,<NewLine>nni.LinearReLU: nniq.LinearReLU,<NewLine>nniqat.ConvReLU2d: nniq.ConvReLU2d,<NewLine>nniqat.LinearReLU: nniq.LinearReLU,<NewLine>nniqat.ConvBn2d: nnq.Conv2d,<NewLine>nniqat.ConvBnReLU2d: nniq.ConvReLU2d,<NewLine><NewLine># QAT modules:<NewLine>nnqat.Linear: nnq.Linear,<NewLine>nnqat.Conv2d: nnq.Conv2d,</code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I build a pytorch model based on conv1d. I gone through quantization and implemented some cases as well but all those are working on conv2d, bn,relu but In my case, my model is built on conv1d and PReLU. Does this quatization valid for these network layers? Because when I did quantization only the layers which are included in mapping is only quantized. Let me show you those layers for which quantization is valid(i.e which are included in mapping)<br/><NewLine>Please find the list of modules it supports:(according to source code in went through)<br/><NewLine>(Actual layer : quantized layer)<br/><NewLine>nn.Linear: nnq.Linear,<br/><NewLine>nn.ReLU: nnq.ReLU,<br/><NewLine>nn.ReLU6: nnq.ReLU6,<br/><NewLine>nn.Conv2d: nnq.Conv2d,<br/><NewLine>nn.Conv3d: nnq.Conv3d,<br/><NewLine>nn.BatchNorm2d: nnq.BatchNorm2d,<br/><NewLine>nn.BatchNorm3d: nnq.BatchNorm3d,<br/><NewLine>QuantStub: nnq.Quantize,<br/><NewLine>DeQuantStub: nnq.DeQuantize,</p><NewLine><h1>Wrapper Modules:</h1><NewLine><p>nnq.FloatFunctional: nnq.QFunctional,</p><NewLine><h1>Intrinsic modules:</h1><NewLine><p>nni.ConvReLU2d: nniq.ConvReLU2d,<br/><NewLine>nni.ConvReLU3d: nniq.ConvReLU3d,<br/><NewLine>nni.LinearReLU: nniq.LinearReLU,<br/><NewLine>nniqat.ConvReLU2d: nniq.ConvReLU2d,<br/><NewLine>nniqat.LinearReLU: nniq.LinearReLU,<br/><NewLine>nniqat.ConvBn2d: nnq.Conv2d,<br/><NewLine>nniqat.ConvBnReLU2d: nniq.ConvReLU2d,</p><NewLine><h1>QAT modules:</h1><NewLine><p>nnqat.Linear: nnq.Linear,<br/><NewLine>nnqat.Conv2d: nnq.Conv2d,</p><NewLine><p>Is it, it means that quantization can’t be done on conv1d and PReLU?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Conv1d support is being added, <a href=""https://github.com/pytorch/pytorch/pull/38438"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/38438</a>. We dont support fusion with pReLU and LayerNorm currently, so those ops will have to execute separately.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>Apart from fusing with PReLU and layerNorm, does these two are able to get quantized? Since these two layers are not mapped in default mapping config as you can see it in last comment I kept.</p><NewLine><p>Thankyou <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>We currently do not have support to quantize pReLU, but it should be very similar to leakyReLU which we do support.<br/><NewLine>Quantization of LayerNorm is supported as seen in <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/normalization.py#L9"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/normalization.py#L9</a></p><NewLine><p>We will add it to our documentation. Thanks for brining it up!</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/supriyar"">@supriyar</a></p><NewLine><p>Thanks for that. I’m able to see quantized layer mappings for LayerNorm, ReLU &amp; ReLU6 but LeakyReLU is not included here. Is LeakyReLU going to be added?</p><NewLine><p>Thanks <a class=""mention"" href=""/u/supriyar"">@supriyar</a>,<br/><NewLine>Aravind</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You should be able to use the functional one <code>torch.nn.functional.leaky_relu</code><br/><NewLine>cc <a class=""mention"" href=""/u/zafar"">@Zafar</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sasank_Kottapalli; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Sasank_Kottapalli; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/BOLLOJU_ARAVIND; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  6:52pm; <NewLine> REPLY_DATE 2: May 12, 2020,  4:46am; <NewLine> REPLY_DATE 3: May 12, 2020, 12:45pm; <NewLine> REPLY_DATE 4: May 12, 2020,  5:40pm; <NewLine> REPLY_DATE 5: May 13, 2020, 10:51pm; <NewLine> REPLY_DATE 6: May 14, 2020,  3:15am; <NewLine> REPLY_DATE 7: May 20, 2020,  4:57am; <NewLine> REPLY_DATE 8: May 20, 2020,  7:40am; <NewLine> REPLY_DATE 9: May 20, 2020,  6:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> 
81227,Purpose of scale and zero point for layer,2020-05-14T15:47:27.631Z,2,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I have this</p><NewLine><pre><code class=""lang-auto"">{'model_state_dict': OrderedDict([(u'conv_3x3_32a.weight', tensor([[<NewLine>          [[ 0.0367,  0.0294, -0.1065],<NewLine>          [ 0.0918,  0.1065, -0.0331],<NewLine>          [-0.0147,  0.0184, -0.1028]]],<NewLine>.......<NewLine>          [[[ 0.1249,  0.0661, -0.0257],<NewLine>          [ 0.0735, -0.0257, -0.1028],<NewLine>          [ 0.0441, -0.0698, -0.0771]]]], size=(40, 1, 3, 3),<NewLine>       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,<NewLine>       scale=0.00367316859774, zero_point=0)), (u'conv_3x3_32a.scale', tensor(0.0031)), (u'conv_3x3_32a.zero_point', tensor(160))<NewLine></code></pre><NewLine><p>I understand that the weights tensor has its own scale which is 0.00367316859774, but I have 2 questions:</p><NewLine><ol><NewLine><li>Which is the purpose of the layer scale and zero point? Where do I use them?</li><NewLine><li>How can I find which is the re-quantization scale used after the weight and input multiplication and accumulation? I don’t know how to access it.</li><NewLine></ol><NewLine></div>",https://discuss.pytorch.org/u/dassima,(Dassima),dassima,"May 14, 2020,  3:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>scale and zero point are the quantization parameters for the layer. They are used to quantize the weight from fp32 to int8 domain.</p><NewLine><p>re-quantization scale is defined based on input, weight and output scale. It is defined as<br/><NewLine>requant_scale =  input_scale_fp32 * weight_scale_fp32 / output_scale_fp32</p><NewLine><p>The conversion from accumulated int32 values to fp32 happens in the quantization backends, either FBGEMM or QNNPACK and the requantization scale can be found there.<br/><NewLine>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>thank you so much for your answer and please excuse me, but there are 2 scales that seems connected to the weights. The one inside the tensor parameters and the one of the layer. In my example:</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""81227"" data-username=""dassima""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dassima/40/20474_2.png"" width=""20""/> dassima:</div><NewLine><blockquote><NewLine><pre><code class=""lang-auto"">(u'conv_3x3_32a.weight', tensor([[<NewLine>          [[ 0.0367,  0.0294, -0.1065],<NewLine>          [ 0.0918,  0.1065, -0.0331],<NewLine>          [-0.0147,  0.0184, -0.1028]]],<NewLine>.......<NewLine>          [[[ 0.1249,  0.0661, -0.0257],<NewLine>          [ 0.0735, -0.0257, -0.1028],<NewLine>          [ 0.0441, -0.0698, -0.0771]]]], size=(40, 1, 3, 3),<NewLine>       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,<NewLine>       scale=0.00367316859774, zero_point=0)<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>and</p><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""81227"" data-username=""dassima""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dassima/40/20474_2.png"" width=""20""/> dassima:</div><NewLine><blockquote><NewLine><p><code>(u'conv_3x3_32a.scale', tensor(0.0031))</code></p><NewLine></blockquote><NewLine></aside><NewLine><p>so number 0.00367316859774  and 0.0031 which generate different results. If the 0.0031 does the actual conversion from 32FP to 8INT, what does 0.00367316859774 ?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>(u’conv_3x3_32a.scale’, tensor(0.0031))</p><NewLine></blockquote><NewLine><p>This scale value here refers to the output scale of the layer. Please see the docs for more details<br/><NewLine><a class=""onebox"" href=""https://pytorch.org/docs/stable/quantization.html#id12"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/quantization.html#id12</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  4:49am; <NewLine> REPLY_DATE 2: May 20, 2020,  9:55am; <NewLine> REPLY_DATE 3: May 20, 2020,  4:29pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
82188,Quantization with custom weights in nn.Conv2d(),2020-05-20T14:12:36.899Z,1,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I want to quantize a model that I have created that uses a custom <code>Parameter</code> to hold the weights of several <code>Conv2d()</code> layers. I pass this <code>Parameter</code> to the forward function, which then assigns the different parts of the <code>Parameter</code> to the weights of the <code>Conv2d()</code> layers, which requires a cast to <code>Parameter</code> on every forward function call. This works fine for normal use, if inefficient, but when I want to use the Quantization package, the assignment throws this error:</p><NewLine><blockquote><NewLine><p>KeyError: “attribute ‘weight’ already exists”</p><NewLine></blockquote><NewLine><p>When I don’t use quantization, I can use the functional <code>conv2d()</code>, but I don’t think that is supported yet with nn.quantize. What’s the difference between a quantized model and a regular model, and how can I fix this error?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/louis,,louis,"May 20, 2020,  2:12pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here is a minimal example:</p><NewLine><pre><code class=""lang-auto""><NewLine>import torch<NewLine>import torch.nn as nn<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>import copy<NewLine><NewLine>class Block(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Block, self).__init__()<NewLine>        self.conv1 = nn.Conv2d(1,1,3, padding=1, bias=False)<NewLine>    def forward(self, x, conv_parameter):<NewLine>        self.conv1.weight = torch.nn.parameter.Parameter(conv_parameter) <NewLine>        return self.conv1(x)<NewLine>    <NewLine>class BigBlock(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(BigBlock, self).__init__()<NewLine>        self.conv_parameter = torch.nn.parameter.Parameter(torch.rand((1,1,3,3)))<NewLine>        self.block = Block()<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.block(x, self.conv_parameter)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>x = torch.rand((1,1,5,5))<NewLine>net = BigBlock()<NewLine>print(net(x)) # works fine<NewLine><NewLine>qnet = copy.deepcopy(net)<NewLine>qnet = qnet.eval()<NewLine>qnet.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(qnet, inplace=True)<NewLine>net(torch.rand((1,1,5,5)))<NewLine>torch.quantization.convert(qnet, inplace=True)<NewLine>print(qnet)<NewLine>print(qnet(x)) # throws an error<NewLine><NewLine></code></pre><NewLine><p>I am using pytorch-1.6.0.dev202, Python 3.8.1.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>So the issue is that I was passing <code>floatTensor</code> to a now <code>torch.qint8</code>, and attempting to cast it to a <code>Parameter</code>. The solution is to put an if statement like so:</p><NewLine><pre><code class=""lang-auto"">class Block(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Block, self).__init__()<NewLine>        self.conv1 = torch.nn.Conv2d(1,1,3, padding=1, bias=False)<NewLine>    def forward(self, x, conv_parameter):<NewLine>        if(isinstance(self.conv1, torch.nn.quantized.Conv2d)):<NewLine>            return self.conv1(x)<NewLine>        else:<NewLine>            self.conv1.weight = torch.nn.parameter.Parameter(conv_parameter) <NewLine>        return self.conv1(x)<NewLine></code></pre><NewLine><p>which works since my passed in <code>conv_parameter</code> should be equal to the weight stored in <code>self.conv1</code>, so we can just ignore the conversion to Parameter when using the quantized network.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/louis; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/louis; <NewLine> ,"REPLY_DATE 1: May 20, 2020,  2:35pm; <NewLine> REPLY_DATE 2: May 20, 2020,  3:09pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
80274,Quantized Conv2d bug,2020-05-08T03:11:04.195Z,10,358,"<div class=""post"" itemprop=""articleBody""><NewLine><p>As my test, if input’s (dtype quint8) zero point is large, for example 128, the torch.nn.quantized.Conv2d will give a wrong result on Ubuntu 18.04 or windows 10. Some output feature map points match correct result, some output feature map points mismatch correct result, and the difference is much more than 1 or 2, is about 10 or 20).</p><NewLine><p>If I set input’s zero point smaller as 75, the quantized conv2d become correct. However, different images have different range, for example, a image after quantized range [-54, 81]: maximum zero point is 75, [-66, 84]: maximum zero point is 69. No regular pattern.</p><NewLine><p>However, on Centos7, the quantized Conv2d give a correct result. The scripts, python and pytorch and torchvision version are completely same.</p><NewLine><p>So I have no idea why. Very appropriately, if anyone could help me.<br/><NewLine>My test CNN is ResNet50 and data set is Image Net ILSVRC2012.</p><NewLine></div>",https://discuss.pytorch.org/u/elvindp,(Elvindp),elvindp,"May 8, 2020,  3:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>could you provide some test case so that we can reproduce the problem?<br/><NewLine>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> <a class=""mention"" href=""/u/zafar"">@Zafar</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>First, I make use of the per-trained quantize ResNet50 model on torchvision.models.quantization.resnet</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torchvision.models.resnet import Bottleneck, BasicBlock, ResNet, model_urls<NewLine>import torch.nn as nn<NewLine>from torchvision.models.utils import load_state_dict_from_url<NewLine>from torch.quantization import QuantStub, DeQuantStub, fuse_modules<NewLine>from torch._jit_internal import Optional<NewLine><NewLine>__all__ = ['QuantizableResNet', 'resnet50']<NewLine><NewLine>quant_model_urls = {<NewLine>    'resnet50_fbgemm':<NewLine>        'https://download.pytorch.org/models/quantized/resnet50_fbgemm_bf931d71.pth',<NewLine>}<NewLine><NewLine><NewLine>def _replace_relu(module):<NewLine>    reassign = {}<NewLine>    for name, mod in module.named_children():<NewLine>        _replace_relu(mod)<NewLine>        # Checking for explicit type instead of instance<NewLine>        # as we only want to replace modules of the exact type<NewLine>        # not inherited classes<NewLine>        if type(mod) == nn.ReLU or type(mod) == nn.ReLU6:<NewLine>            reassign[name] = nn.ReLU(inplace=False)<NewLine><NewLine>    for key, value in reassign.items():<NewLine>        module._modules[key] = value<NewLine><NewLine><NewLine>def quantize_model(model, backend):<NewLine>    _dummy_input_data = torch.rand(1, 3, 299, 299)<NewLine>    if backend not in torch.backends.quantized.supported_engines:<NewLine>        raise RuntimeError(""Quantized backend not supported "")<NewLine>    torch.backends.quantized.engine = backend<NewLine>    model.eval()<NewLine>    # Make sure that weight qconfig matches that of the serialized models<NewLine>    if backend == 'fbgemm':<NewLine>        model.qconfig = torch.quantization.QConfig(<NewLine>            activation=torch.quantization.default_observer,<NewLine>            weight=torch.quantization.default_per_channel_weight_observer)<NewLine>    elif backend == 'qnnpack':<NewLine>        model.qconfig = torch.quantization.QConfig(<NewLine>            activation=torch.quantization.default_observer,<NewLine>            weight=torch.quantization.default_weight_observer)<NewLine><NewLine>    model.fuse_model()<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    model(_dummy_input_data)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine><NewLine>    return<NewLine><NewLine><NewLine>class QuantizableBottleneck(Bottleneck):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super(QuantizableBottleneck, self).__init__(*args, **kwargs)<NewLine>        self.skip_add_relu = nn.quantized.FloatFunctional()<NewLine>        self.relu1 = nn.ReLU(inplace=False)<NewLine>        self.relu2 = nn.ReLU(inplace=False)<NewLine><NewLine>    def forward(self, x):<NewLine>        identity = x<NewLine>        out = self.conv1(x)<NewLine>        out = self.bn1(out)<NewLine>        out = self.relu1(out)<NewLine>        out = self.conv2(out)<NewLine>        out = self.bn2(out)<NewLine>        out = self.relu2(out)<NewLine><NewLine>        out = self.conv3(out)<NewLine>        out = self.bn3(out)<NewLine><NewLine>        if self.downsample is not None:<NewLine>            identity = self.downsample(x)<NewLine>        out = self.skip_add_relu.add_relu(out, identity)<NewLine><NewLine>        return out<NewLine><NewLine>    def fuse_model(self):<NewLine>        fuse_modules(self, [['conv1', 'bn1', 'relu1'],<NewLine>                            ['conv2', 'bn2', 'relu2'],<NewLine>                            ['conv3', 'bn3']], inplace=True)<NewLine>        if self.downsample:<NewLine>            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)<NewLine><NewLine><NewLine>class QuantizableResNet(ResNet):<NewLine><NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super(QuantizableResNet, self).__init__(*args, **kwargs)<NewLine><NewLine>        self.quant = torch.quantization.QuantStub()<NewLine>        self.dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        # Ensure scriptability<NewLine>        # super(QuantizableResNet,self).forward(x)<NewLine>        # is not scriptable<NewLine>        x = self.conv1(x)<NewLine>        x = self.bn1(x)<NewLine>        x = self.relu(x)<NewLine>        x = self.maxpool(x)<NewLine><NewLine>        x = self.layer1(x)<NewLine>        x = self.layer2(x)<NewLine>        x = self.layer3(x)<NewLine>        x = self.layer4(x)<NewLine><NewLine>        x = self.avgpool(x)<NewLine>        x = torch.flatten(x, 1)<NewLine>        x = self.fc(x)<NewLine><NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine><NewLine>    def fuse_model(self):<NewLine>        r""""""Fuse conv/bn/relu modules in resnet models<NewLine>        Fuse conv+bn+relu/ Conv+relu/conv+Bn modules to prepare for quantization.<NewLine>        Model is modified in place.  Note that this operation does not change numerics<NewLine>        and the model after modification is in floating point<NewLine>        """"""<NewLine><NewLine>        fuse_modules(self, ['conv1', 'bn1', 'relu'], inplace=True)<NewLine>        for m in self.modules():<NewLine>            if type(m) == QuantizableBottleneck:<NewLine>                m.fuse_model()<NewLine><NewLine><NewLine>def _resnet(arch, block, layers, pretrained, progress, quantize, **kwargs):<NewLine>    model = QuantizableResNet(block, layers, **kwargs)<NewLine>    _replace_relu(model)<NewLine>    if quantize:<NewLine>        # TODO use pretrained as a string to specify the backend<NewLine>        backend = 'fbgemm'<NewLine>        quantize_model(model, backend)<NewLine>    else:<NewLine>        assert pretrained in [True, False]<NewLine><NewLine>    if pretrained:<NewLine>        if quantize:<NewLine>            model_url = quant_model_urls[arch + '_' + backend]<NewLine>        else:<NewLine>            model_url = model_urls[arch]<NewLine><NewLine>        state_dict = load_state_dict_from_url(model_url,<NewLine>                                              progress=progress)<NewLine><NewLine>        model.load_state_dict(state_dict)<NewLine>    return model<NewLine><NewLine><NewLine>def resnet50(pretrained=False, progress=True, quantize=False, **kwargs):<NewLine>    r""""""ResNet-50 model from<NewLine>    `""Deep Residual Learning for Image Recognition"" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_<NewLine>    Args:<NewLine>        pretrained (bool): If True, returns a model pre-trained on ImageNet<NewLine>        progress (bool): If True, displays a progress bar of the download to stderr<NewLine>    """"""<NewLine>    return _resnet('resnet50', QuantizableBottleneck, [3, 4, 6, 3], pretrained, progress,<NewLine>                   quantize, **kwargs)<NewLine><NewLine></code></pre><NewLine><p>And then do inference for a image from Image Net</p><NewLine><pre><code class=""lang-auto"">from PIL import Image<NewLine>import torch.backends.quantized<NewLine>import torchvision.transforms as transforms<NewLine>from qresnet50_original import resnet50<NewLine>import numpy as np<NewLine><NewLine>model = resnet50(pretrained=True, quantize=True)<NewLine>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<NewLine>                                 std=[0.229, 0.224, 0.225])<NewLine><NewLine>transform = transforms.Compose([<NewLine>    transforms.Resize(256),<NewLine>    transforms.CenterCrop(224),<NewLine>    transforms.ToTensor(),<NewLine>    normalize,<NewLine>])<NewLine><NewLine>img = Image.open(""n01440764-0-tench.JPEG"")<NewLine><NewLine>img_t = transform(img)<NewLine>batch_t = torch.unsqueeze(img_t, 0)<NewLine># quantized input, and to test zero point effect, set zp 128<NewLine>pre_quant = model.quant<NewLine>pre_quant.zero_point = torch.tensor([128])<NewLine>q_input = pre_quant(batch_t)<NewLine><NewLine>conv1_fp = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)<NewLine>conv1_fp.weight = torch.nn.Parameter(model.conv1.weight().dequantize())<NewLine>conv1_fp.bias = torch.nn.Parameter(model.conv1.bias())<NewLine>relu = torch.nn.ReLU(inplace=True)<NewLine><NewLine>scale = model.conv1.scale<NewLine>correct_output = np.round(relu(conv1_fp(q_input.dequantize()) / scale).detach().numpy())<NewLine><NewLine>output = np.round((model.conv1(q_input).detach().dequantize() / scale).numpy())<NewLine>diff = np.abs(correct_output-output)<NewLine>print(np.sum(diff))<NewLine><NewLine></code></pre><NewLine><p>If I change the pre_quant.zero_point to 128 (in fact larger than 75, will cause difference), the difference between correct output and quantization output will be large.<br/><NewLine>The test image is<br/><NewLine><img alt=""n01440764-0-tench"" data-base62-sha1=""voDAUFyGc3fdlKC2JvleMhngRSj"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/c/dc0c649beb49ec61073956680c1c58f84cbf33cb.jpeg"" width=""500""/></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>My PC info: OS: Ubuntu 18.04, CPU: i7-9700K, Wrong ouput<br/><NewLine>Laptop info: OS: Windows 10, CPU: i7-8565U, Wrong ouput<br/><NewLine>My GPU Server: OS: CentOS 7.7, CPU: Intel® Xeon® Gold 6230, Correct output</p><NewLine><p>PyTorch tested: 1.3.1, 1.4, 1.5. Python tested: 3.6, 3.7</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry that too late to reply. Wait for your help!</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>The scale and zero point for input are calculated after a calibration step across a representative set of images. By default, for PyTorch quantization, these are chosen such that quantized values are in the range [0, 127] for activations and zero_point is somewhere in the same range (e.g., 57 for the model here). We restrict quantized activation values to [0, 127] instead of [0, 255] for uint8 to avoid the saturation explained here: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv.cpp#L633-L661"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv.cpp#L633-L661</a></p><NewLine><p>When you force the zero_point to 128, the quantized activations become in the range [0, 255] and it is possible that the saturation happens. BTW, any reason for forcing zero_point to a large value? Let the calibration step pick scale and zero_points for the model you are quantizing.</p><NewLine><p>It is strange that you see different behavior on the cpus you mentioned. I would expect the same behavior on all the CPUs you mentioned.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Well, I try to use symmetric quantization scheme, which will force the zero point to 128, because asymmetric quantization mode will cause some other effects to our project. In this case, as I suppose, the range is [-128, 127], and after ReLU, the range become [0, 127]. This seems to be wrong. In fact, I prefer to using qint8 as feature map’s data type, but qint8 is not supported.<br/><NewLine>So can I say that in fbgemm, the qconv calculate the unsigned integer summation first and then minus summation with zero points? And the summation may cause saturation.<br/><NewLine>Also, the different behaviors confuse me…</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For activations (uint8), the range is [0, 127] and after ReLU it stays the same. Which summation are you talking about? accumulations during convolution? accumulations (activations times weight matrix accumulations) are done in int32.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Okay, so the saturation happens during multiplication, but not the accumulations during convolution. But I suppose that before convolution, the uint8 range [0, 255] will minus zero point 128, which means the range will be  [-128, 127]. However, it seems to be wrong.<br/><NewLine>Besides, when using nn.quantized.Conv2d but not nn.intrinsic.quantized.Conv2dReLU, the feature map output range should be [-128, 127] (symmetric quantization mode).<br/><NewLine>Actually, I am confused that why using uint8 not int8.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it true that doing multiplication before minus zero point?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. It’s doing multiplication before subtracting zero_point.</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>Okay, so the saturation happens during multiplication, but not the accumulations during convolution.</p><NewLine></blockquote><NewLine><p>The saturation I mentioned earlier, happens during multiplication. However, for large number of channels even the accumulations into an int32 might overflow. This should be very rare and should only happen for a very large number of channels. For such a case, there is no saturation but the accumulator overflows and wraps around.</p><NewLine><blockquote><NewLine><p>Actually, I am confused that why using uint8 not int8.</p><NewLine></blockquote><NewLine><p>We are using uint8 for activations because the x64 vector instruction to multiply two 8-bit integers (vpmaddubsw) accepts uint8 for one input and int8 for another. We chose uint8 for activations and int8 for weights. int8 for activation can also be supported by a preprocessing step, i.e., by converting int8 to uint8 and adjusting zero_point for activations (add 128 to all int8 elements and new_zero_point = old_zero_point + 128 in the preprocessing step).</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you very much for your explanation!</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/elvindp; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/elvindp; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  6:09pm; <NewLine> REPLY_DATE 2: May 11, 2020,  7:36am; <NewLine> REPLY_DATE 3: May 11, 2020,  7:39am; <NewLine> REPLY_DATE 4: May 11, 2020,  7:38am; <NewLine> REPLY_DATE 5: May 12, 2020,  5:23am; <NewLine> REPLY_DATE 6: May 12, 2020,  8:11am; <NewLine> REPLY_DATE 7: May 12, 2020,  4:49pm; <NewLine> REPLY_DATE 8: May 13, 2020,  1:57am; <NewLine> REPLY_DATE 9: May 14, 2020,  6:56am; <NewLine> REPLY_DATE 10: May 14, 2020,  5:50pm; <NewLine> REPLY_DATE 11: May 14, 2020,  6:00pm; <NewLine> REPLY_DATE 12: May 18, 2020,  2:16am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
80576,Internal mutex in C++ quantized backend,2020-05-10T14:41:40.917Z,3,121,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I have some deep learning model which i want to transfer to C++ and make parallel threaded inference. My use-case requires all threads to have its own model replica and each thread must execute model in one core.</p><NewLine><p>Here is python script</p><NewLine><pre><code class=""lang-auto"">import os<NewLine>os.environ[""OMP_NUM_THREADS""] = ""1""<NewLine>os.environ[""MKL_NUM_THREADS""] = ""1""<NewLine>import tqdm<NewLine>import argparse<NewLine>import torch<NewLine>import torch.nn.quantized<NewLine>import torch.quantization<NewLine><NewLine><NewLine>def make_fused_linear(in_features: int, out_features: int):<NewLine>    return torch.quantization.fuse_modules(<NewLine>        torch.nn.Sequential(<NewLine>            torch.nn.Linear(in_features=in_features, out_features=out_features),<NewLine>            torch.nn.ReLU(inplace=True)<NewLine>        ),<NewLine>        modules_to_fuse=['0', '1']<NewLine>    )<NewLine><NewLine><NewLine>class FeedforwardModel(torch.nn.Module):<NewLine>    def __init__(self, features):<NewLine>        super(FeedforwardModel, self).__init__()<NewLine>        self._net = torch.nn.Sequential(<NewLine>            make_fused_linear(features, 90),<NewLine>            make_fused_linear(90, 90),<NewLine>            make_fused_linear(90, 90),<NewLine>            make_fused_linear(90, 90),<NewLine>            make_fused_linear(90, 90),<NewLine>            make_fused_linear(90, 90),<NewLine>        )<NewLine>        self._final = torch.nn.Linear(90, 50)<NewLine>        self._quant = torch.quantization.QuantStub()<NewLine>        self._dequant = torch.quantization.DeQuantStub()<NewLine><NewLine>    def forward(self, x: torch.Tensor):<NewLine>        x = self._quant(x)<NewLine>        x = self._final(self._net(x))<NewLine>        x = self._dequant(x)<NewLine>        return x<NewLine><NewLine><NewLine>def timeit_model(model, *inputs):<NewLine>    for _ in tqdm.trange(10000000000000):<NewLine>        with torch.no_grad():<NewLine>            model(*inputs)<NewLine><NewLine><NewLine>if __name__ == '__main__':<NewLine>    parser = argparse.ArgumentParser()<NewLine>    parser.add_argument(""-q"", action=""store_true"", help=""Use quantized model"")<NewLine>    parser.add_argument(""-b"", default=1, help=""Batch size"", type=int)<NewLine><NewLine>    torch.set_num_interop_threads(1)<NewLine>    torch.set_num_threads(1)<NewLine><NewLine>    args = parser.parse_args()<NewLine>    use_quantized = args.q<NewLine>    batch_size = args.b<NewLine><NewLine>    in_features = 40 * 64  # new user model with 40 queries<NewLine>    inputs = torch.rand(batch_size, in_features)<NewLine><NewLine>    with torch.no_grad():<NewLine>        if not use_quantized:<NewLine>            model = FeedforwardModel(in_features)<NewLine>            model.eval()<NewLine>            traced_script_module = torch.jit.trace(model, inputs)<NewLine>            traced_script_module.save(""model.torch"")<NewLine><NewLine>            timeit_model(traced_script_module, inputs)<NewLine>        else:<NewLine>            model = FeedforwardModel(in_features)<NewLine>            model.eval()<NewLine>            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>            torch.quantization.prepare(model, inplace=True)<NewLine><NewLine>            model(inputs)<NewLine><NewLine>            torch.quantization.convert(model, inplace=True)<NewLine><NewLine>            traced_script_module = torch.jit.trace(model, inputs)<NewLine>            traced_script_module.save(""quantized_model.torch"")<NewLine><NewLine>            timeit_model(traced_script_module, inputs)<NewLine></code></pre><NewLine><p>And corresponding C++ code</p><NewLine><pre><code class=""lang-auto"">#include &lt;iostream&gt;<NewLine>#include &lt;future&gt;<NewLine>#include &lt;torch/all.h&gt;<NewLine>#include &lt;torch/script.h&gt; // One-stop header.<NewLine><NewLine><NewLine>int main(int argc, const char* argv[]) {<NewLine>    // WARNING! this does not work for quantized model! For quantized, only setting<NewLine>    // MKL_NUM_THREADS=1 and OMP_NUM_THREADS=1 work! We are investigating this issue with torch guys<NewLine>    torch::set_num_threads(1);<NewLine>    at::set_num_interop_threads(1);<NewLine><NewLine>    auto model_path = argv[1];<NewLine>    auto num_threads = std::stoi( argv[2] );<NewLine><NewLine>    torch::Tensor inputs = torch::zeros({1, 40 * 64}, torch::kFloat32);<NewLine><NewLine>    std::vector&lt;std::future&lt;void&gt;&gt; futures;<NewLine>    for (auto i = 0; i &lt; num_threads; i++) {<NewLine>        futures.emplace_back(std::move(std::async(std::launch::async, [model_path, inputs]() {<NewLine>            torch::NoGradGuard torch_guard;<NewLine>            auto model = torch::jit::load(model_path);<NewLine>            model.eval();<NewLine>            auto thread_inputs = inputs.clone();<NewLine><NewLine>            while (true) {<NewLine>                model.forward({thread_inputs});<NewLine>            }<NewLine>        })));<NewLine>    }<NewLine><NewLine>    for (auto &amp;f : futures) { f.get(); }<NewLine><NewLine>    return 0;<NewLine>}<NewLine></code></pre><NewLine><p>The first issue is written in C++ comments: setting num threads programmatically does not work with quantized backend.</p><NewLine><p>The second, more severe issue - when i launch code in 40 threads on 40-core machine, floating-point model parallels perfectly (as it should), quantized model stucks with some mutex. This is easily seen either in htop (cpu cores spend time in kernel syscalls) and strace. strace says that quantized model calls futex very frequently. Floating point model does no syscalls after all threads started.</p><NewLine><p>Can you help me to get rid of this lock? Maybe i’m doing something wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/alxmopo3ov,(Alexey Morozov),alxmopo3ov,"May 10, 2020,  2:41pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think quantized backend uses fbgemm which uses MKL_NUM_THREADS=1 and OMP_NUM_THREADS=1 to control the threading. cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/jianyuhuang"">@jianyuhuang</a> for the second issue.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you using a recent version of PyTorch (and compiled with C++14?). We made a <a href=""https://github.com/pytorch/FBGEMM/commit/35e486b706abd4d575ae9b7aaf090002aa78551e"" rel=""nofollow noopener"">recent improvement in the quantized backend library</a> (fbgemm) to make use of read-write lock from C++14 and these are not supported on MacOS due to issues. Are you running it on MacOS?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/FBGEMM/blob/master/src/CodeCache.h#L12-L16"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/FBGEMM/blob/master/src/CodeCache.h#L12-L16"" rel=""nofollow noopener"" target=""_blank"">pytorch/FBGEMM/blob/master/src/CodeCache.h#L12-L16</a></h4><NewLine><pre class=""onebox""><code class=""lang-h""><ol class=""start lines"" start=""12"" style=""counter-reset: li-counter 11 ;""><NewLine><li>#if __cplusplus &gt;= 201402L &amp;&amp; !defined(__APPLE__)</li><NewLine><li>// For C++14, use shared_timed_mutex.</li><NewLine><li>// some macOS C++14 compilers don't support shared_timed_mutex.</li><NewLine><li>#define FBGEMM_USE_SHARED_TIMED_MUTEX</li><NewLine><li>#endif</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you!</p><NewLine><p>I am already using these environment variables.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you!</p><NewLine><p>I am running code on linux server with 46-core processor. Unfortunately, getting fresh latest libtorch did not help.</p><NewLine><p>I will try profile code and figure out which part of code makes issue.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. Profiling would be of great help.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alxmopo3ov; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/alxmopo3ov; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: May 11, 2020,  6:44pm; <NewLine> REPLY_DATE 2: May 11, 2020,  9:31pm; <NewLine> REPLY_DATE 3: May 13, 2020,  7:16am; <NewLine> REPLY_DATE 4: May 13, 2020,  7:19am; <NewLine> REPLY_DATE 5: May 14, 2020,  6:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
81173,How to fuse layers of any convolutional neural network?,2020-05-14T10:03:50.615Z,0,114,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>Is there any generalized way or code to fuse layers of any convolutional model ?<br/><NewLine>ex. AlexNet, ResNet, VGG<br/><NewLine>just one code which will work for all sort of model to fuse their conv + bn + relu layers.</p><NewLine></div>",https://discuss.pytorch.org/u/studML,(Nick john),studML,"May 14, 2020, 10:03am",,,,,
77978,Fuse ConvBnReLU model error,2020-04-23T05:25:56.169Z,1,276,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I use ResNet-18 to test in post static quantization, and the step 2 is I need to fuse module like conv,bn,relu or conv,bn and so on. The the code is wrote as follows:</p><NewLine><pre><code class=""lang-auto"">110     def fuse_model(self):<NewLine>111         modules_names = [m for m in self.named_modules()]<NewLine>112         modules_list = [m for m in self.modules()]<NewLine>113         for ind, m in enumerate(modules_list):<NewLine>114             if type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.BatchNorm2d and type(modules_list[ind+2]) == nn.ReLU:<NewLine>115                 print(""Find ConvBNReLu: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0], '--&gt;', modules_names[ind+2][0])<NewLine>116                 torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0], modules_names[ind+2][0]], inplace=True)<NewLine>117             elif type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.BatchNorm2d:<NewLine>118                 print(""Find ConvBN: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0])<NewLine>119                 torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0]], inplace=True)<NewLine>120             elif type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.ReLU:<NewLine>121                 print(""Find ConvReLU: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0])<NewLine>122                 torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0]], inplace=True)<NewLine></code></pre><NewLine><p>And I print the layer to fuse as follows:<br/><NewLine>Find ConvBN:  conv1 --&gt; bn1<br/><NewLine>Find ConvBN:  layer1.0.conv1 --&gt; layer1.0.bn1<br/><NewLine>Find ConvBNReLu:  layer1.0.conv2 --&gt; layer1.0.bn2 --&gt; layer1.0.relu<br/><NewLine>Find ConvBN:  layer1.1.conv1 --&gt; layer1.1.bn1<br/><NewLine>Find ConvBNReLu:  layer1.1.conv2 --&gt; layer1.1.bn2 --&gt; layer1.1.relu<br/><NewLine>Find ConvBN:  layer2.0.conv1 --&gt; layer2.0.bn1<br/><NewLine>Find ConvBNReLu:  layer2.0.conv2 --&gt; layer2.0.bn2 --&gt; layer2.0.relu<br/><NewLine>Find ConvBN:  layer2.0.shortcut.0 --&gt; layer2.0.shortcut.1<br/><NewLine>Find ConvBN:  layer2.1.conv1 --&gt; layer2.1.bn1<br/><NewLine>Find ConvBNReLu:  layer2.1.conv2 --&gt; layer2.1.bn2 --&gt; layer2.1.relu<br/><NewLine>Find ConvBN:  layer3.0.conv1 --&gt; layer3.0.bn1<br/><NewLine>Find ConvBNReLu:  layer3.0.conv2 --&gt; layer3.0.bn2 --&gt; layer3.0.relu<br/><NewLine>Find ConvBN:  layer3.0.shortcut.0 --&gt; layer3.0.shortcut.1<br/><NewLine>Find ConvBN:  layer3.1.conv1 --&gt; layer3.1.bn1<br/><NewLine>Find ConvBNReLu:  layer3.1.conv2 --&gt; layer3.1.bn2 --&gt; layer3.1.relu<br/><NewLine>Find ConvBN:  layer4.0.conv1 --&gt; layer4.0.bn1<br/><NewLine>Find ConvBNReLu:  layer4.0.conv2 --&gt; layer4.0.bn2 --&gt; layer4.0.relu<br/><NewLine>Find ConvBN:  layer4.0.shortcut.0 --&gt; layer4.0.shortcut.1<br/><NewLine>Find ConvBN:  layer4.1.conv1 --&gt; layer4.1.bn1<br/><NewLine>Find ConvBNReLu:  layer4.1.conv2 --&gt; layer4.1.bn2 --&gt; layer4.1.relu</p><NewLine><p>And It seems every thing is ok, but when I run eval with pretrained model, the accuracy is 1% for CIFAR100, which origin acc is about 73%. So I start to debug, I found that when I remove fuse ConvBnRelu module, the result is good.</p><NewLine><pre><code class=""lang-auto"">110     def fuse_model(self):<NewLine>111         modules_names = [m for m in self.named_modules()]<NewLine>112         modules_list = [m for m in self.modules()]<NewLine>113         for ind, m in enumerate(modules_list):<NewLine>114             #if type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.BatchNorm2d and type(modules_list[ind+2]) == nn.ReLU:<NewLine>115             #    print(""Find ConvBNReLu: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0], '--&gt;', modules_names[ind+2][0])<NewLine>116             #    torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0], modules_names[ind+2][0]], inplace=True)<NewLine>117             if type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.BatchNorm2d:<NewLine>118                 print(""Find ConvBN: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0])<NewLine>119                 torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0]], inplace=True)<NewLine>120             elif type(m) == nn.Conv2d and type(modules_list[ind+1]) == nn.ReLU:<NewLine>121                 print(""Find ConvReLU: "", modules_names[ind][0], '--&gt;', modules_names[ind+1][0])<NewLine>122                 torch.quantization.fuse_modules(self, [modules_names[ind][0], modules_names[ind+1][0]], inplace=True)<NewLine></code></pre><NewLine><p>So I am confused if ConvBnReLU fuse module has problem, and I test it on pytorch-1.5 and pytorch 1.4, I has this problem both.<br/><NewLine>So please help me, thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/dingyongchao,(DingYongchao),dingyongchao,"April 23, 2020,  5:25am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""77978"" data-username=""dingyongchao""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/d/ea666f/40.png"" width=""20""/> dingyongchao:</div><NewLine><blockquote><NewLine><p>ind</p><NewLine></blockquote><NewLine></aside><NewLine><p>Can you share more details? Are you calling fusion after the model is set to eval? Are you quantizing the model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Raghuraman,</p><NewLine><p>I bump into this question when I was searching for details about the module fusion.</p><NewLine><p>In your quantization tutorial, it was explicitly mentioned that module fusion will help make the model faster by saving on memory access while also improving numerical accuracy.</p><NewLine><p>I am curious about what exactly does PyTorch do when fusing the modules? I may understand that it may save memory access but why will the fusion improve the numerical accuracy?</p><NewLine><p>Thanks a lot,</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Yozey; <NewLine> ,"REPLY_DATE 1: April 25, 2020, 12:26am; <NewLine> REPLY_DATE 2: May 13, 2020,  3:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
72137,Prepare_qat on module removes hooks,2020-03-05T12:06:53.595Z,11,351,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I’ve just started to dive into quantization tools that were introduced in version 1.3.<br/><NewLine>For now I am trying to train a network with existing model generation code. The code has certain subtleties, one of these are <code>_forward_pre_hooks</code> in several submodules. (See code below)</p><NewLine><p>Here is the problem. After <code>prepare_qat</code> with default config changes submodules to ones with fake quantization and hooks are disappeared. Is it possible to prevent hooks from disappearing during <code>prepare_qat</code> (and <code>submodule.fuse()</code> too)</p><NewLine><p>There is an intermediate code:</p><NewLine><pre><code class=""lang-auto"">...<NewLine>print('pre qat: ', model.backbone.bottom_up.blocks[2][3].conv_pwl)<NewLine>print('pre qat: ', model.backbone.bottom_up.blocks[2][3].conv_pwl._forward_pre_hooks.values())<NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')<NewLine>torch.quantization.prepare_qat(model, inplace=True)<NewLine>print('post qat: ', model.backbone.bottom_up.blocks[2][3].conv_pwl)<NewLine>print('post qat: ', model.backbone.bottom_up.blocks[2][3].conv_pwl._forward_pre_hooks.values())<NewLine>...<NewLine></code></pre><NewLine><p>And the output is:</p><NewLine><pre><code class=""lang-auto"">pre qat:  Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                                                                                                    <NewLine>pre qat:  odict_values([functools.partial(&lt;bound method FeatureHooks._collect_output_hook of &lt;timm.models.feature_hooks.FeatureHooks object at 0x7fd3b5bcf5d0&gt;&gt;, 'blocks.2.3.conv_pwl')])<NewLine>post qat:  Conv2d(<NewLine>  120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False<NewLine>  (activation_post_process): FakeQuantize(<NewLine>    fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.]), zero_point=tensor([0])<NewLine>    (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>  )<NewLine>  (weight_fake_quant): FakeQuantize(<NewLine>    fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.]), zero_point=tensor([0])<NewLine>    (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))<NewLine>  )<NewLine>)<NewLine>post qat:  odict_values([])<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"March 5, 2020, 12:06pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>As I can to prevent hooks from disappearing it is needed to put relevant code somewhere here (during <code>prepare_qat</code>):<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L335"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L335"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/torch/quantization/quantize.py#L335</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""325"" style=""counter-reset: li-counter 324 ;""><NewLine><li>        mod: input module</li><NewLine><li>        mapping: a dictionary that maps from nn module to nnq module</li><NewLine><li><NewLine></li><NewLine><li>    Return:</li><NewLine><li>        The corresponding quantized module of `mod`</li><NewLine><li>    """"""</li><NewLine><li>    new_mod = mod</li><NewLine><li>    # Always replace dequantstub with dequantize</li><NewLine><li>    if hasattr(mod, 'qconfig') and mod.qconfig is not None or type(mod) == DeQuantStub:</li><NewLine><li>        if type(mod) in mapping:</li><NewLine><li class=""selected"">            new_mod = mapping[type(mod)].from_float(mod)</li><NewLine><li>    return new_mod</li><NewLine><li><NewLine></li><NewLine><li>def get_observer_dict(mod, target_dict, prefix=""""):</li><NewLine><li>    r""""""Traverse the modules and save all observers into dict.</li><NewLine><li>    This is mainly used for quantization accuracy debug</li><NewLine><li>    Args:</li><NewLine><li>        mod: the top module we want to save all observers</li><NewLine><li>        prefix: the prefix for the current module</li><NewLine><li>        target_dict: the dictionary used to save all the observers</li><NewLine><li>    """"""</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>But during fusion it is not that obvious because we map up to three modules to one and each of them could have hooks. I think it is possible to work around with <code>torch.quantization.fuse_modules(...fuser_func=&lt;func&gt;...)</code></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah feel free to submit a PR for preserving the pre hooks in swapping.</p><NewLine><p>For fusion I think we probably need to error out if you have a prehook for intermediate moduels like BatchNorm because when we fuse batchnorm into conv, batchnorm is gone.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’ll try!</p><NewLine><p>So Jerry could you explain please for what purpose that was done?</p><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L99"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/quantize.py#L99"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/torch/quantization/quantize.py#L99</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""89"" style=""counter-reset: li-counter 88 ;""><NewLine><li>        if type(child) == nnq.FloatFunctional:</li><NewLine><li>            if hasattr(child, 'qconfig') and child.qconfig is not None:</li><NewLine><li>                child.activation_post_process = child.qconfig.activation()</li><NewLine><li>        else:</li><NewLine><li>            add_observer_(child)</li><NewLine><li><NewLine></li><NewLine><li>    # Insert observers only for leaf nodes, note that this observer is for</li><NewLine><li>    # the output of the module, for input QuantStub will observe them</li><NewLine><li>    if hasattr(module, 'qconfig') and module.qconfig is not None and \</li><NewLine><li>       len(module._modules) == 0 and not isinstance(module, torch.nn.Sequential):</li><NewLine><li class=""selected"">        # observer and hook will be gone after we swap the module</li><NewLine><li>        module.add_module('activation_post_process', module.qconfig.activation())</li><NewLine><li>        module.register_forward_hook(_observer_forward_hook)</li><NewLine><li><NewLine></li><NewLine><li>def add_quant_dequant(module):</li><NewLine><li>    r""""""Wrap the leaf child module in QuantWrapper if it has a valid qconfig</li><NewLine><li>    Note that this function will modify the children of module inplace and it</li><NewLine><li>    can return a new module which wraps the input module as well.</li><NewLine><li><NewLine></li><NewLine><li>    Args:</li><NewLine><li>        module: input module with qconfig attributes for all the leaf modules</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>After <code>prepare()</code> is called <code>convert()</code> will remove all hooks.<br/><NewLine>I think there’s a reason to create such hook than to remove it should we preserve all pre forward / post forward hooks except this one?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yeah I think we should preserve it, but we need to consider each case carefully, since this is interfering with the quantization. That is, when do we run pre forward hook and post forward hook, do we run it before/after observation/quantization?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, again <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>,</p><NewLine><p>Have a look at this example of pre forward hook run. Here’s EfficientNet implementation that assumes it can be integrated as backbone to FPN (<a href=""https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/efficientnet.py#L471"" rel=""nofollow noopener"">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/efficientnet.py#L471</a>)<br/><NewLine>So here we can see that during forward blocks are called sequentially than we collect input features from specific layers.</p><NewLine><p>Assume that we have <code>block1</code> who outputs some data, <code>block2</code> who has pre forward hook and directly obtains data from first one and <code>block3</code> who waits for the same data but obtains it using hook of second one.</p><NewLine><p>In this particular example (EfficientNet implementation) during preparartion pre forward hook on <code>block2</code> should be called after observation and therefore after quantization (because we collected statistics for that). If <code>block2</code> happens to be the first in a row who work with quantized data it is very likely that <code>block3</code> works with it too. Anyway we can place dequant before <code>block3</code>.<br/><NewLine>As for post forward hooks we do not modify input of the module so run them after observation and quantization</p><NewLine><p>Please leave your thoughts when you have time</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""72137"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>Assume that we have <code>block1</code> who outputs some data, <code>block2</code> who has pre forward hook and directly obtains data from first one and <code>block3</code> who waits for the same data but obtains it using hook of second one.</p><NewLine></blockquote><NewLine></aside><NewLine><p>I think this requires that the hooks will do some meaningful computation for both quantized and unquantized data. But as long as we define the semantics clearly it should be OK. So after quantization, the for pre_forward hooks will work with quantized input from previous later, and the forward hooks will work with quantized output from current later, right?</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""72137"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zetyquickly/40/22489_2.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>Assume that we have <code>block1</code> who outputs some data, <code>block2</code> who has pre forward hook and directly obtains data from first one and <code>block3</code> who waits for the same data but obtains it using hook of second one.</p><NewLine></blockquote><NewLine></aside><NewLine><p>actaully we use hooks to implement observe and fake quantize as well, so please make sure that works.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, I think so.<br/><NewLine>pre_forward hooks work with quantized data from previous layer, forward hooks work with quantized output from current layer</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>You are right, we should check whether we are trying to preserve observer or right hook.<br/><NewLine>In my <a href=""https://github.com/pytorch/pytorch/blob/93540f8bf523e2b365e500549308f321e544b120/torch/quantization/quantize.py#L342"" rel=""nofollow noopener"">PR</a> I have handled that case. Without it provided test set fails, with it works well.</p><NewLine><p>I think I should extended test set to test new functionality. Would you mind to guide me in that?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also anticipating possible questions</p><NewLine><p>I’ve introduced changes to <a href=""https://github.com/pytorch/pytorch/blob/93540f8bf523e2b365e500549308f321e544b120/torch/quantization/fuse_modules.py#L113"" rel=""nofollow noopener""><code>fuse_modules.py</code></a></p><NewLine><p>I propose that it is needed to preserve pre and post hooks on fused modules, where it possible. While it hard to define how to preserve hooks for second and third module in sequence (because input data changes and three modules are collapsing into atomic one). But we can easily preserve pre_forward hook of base module.</p><NewLine><p>What cases can we process also?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you post the PR?</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>Here it is<br/><NewLine></p><aside class=""onebox githubpullrequest""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/37233"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Pull Request""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 12 16"" width=""60""><path d=""M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/pull/37233"" rel=""nofollow noopener"" target=""_blank"">Quantization: preserving pre and post forward hooks</a><NewLine></h4><NewLine><div class=""branches""><NewLine><code>pytorch:master</code> ← <code>zetyquickly:zetyquickly/preserve-hooks</code><NewLine></div><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-04-24"" data-format=""ll"" data-time=""11:29:54"" data-timezone=""UTC"">11:29AM - 24 Apr 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/zetyquickly"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""zetyquickly"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars3.githubusercontent.com/u/25350960?v=4"" width=""20""/><NewLine>          zetyquickly<NewLine>        </a><NewLine></div><NewLine><div class=""lines"" title=""1 commits changed 2 files with 9 additions and 0 deletions""><NewLine><a href=""https://github.com/pytorch/pytorch/pull/37233/files"" rel=""nofollow noopener"" target=""_blank""><NewLine><span class=""added"">+9</span><NewLine><span class=""removed"">-0</span><NewLine></a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/zetyquickly; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/zetyquickly; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  2:28pm; <NewLine> REPLY_DATE 2: March 10, 2020,  8:52pm; <NewLine> REPLY_DATE 3: March 11, 2020,  4:52pm; <NewLine> REPLY_DATE 4: March 19, 2020,  8:52pm; <NewLine> REPLY_DATE 5: April 17, 2020, 10:26pm; <NewLine> REPLY_DATE 6: April 28, 2020,  9:58pm; <NewLine> REPLY_DATE 7: April 28, 2020,  9:59pm; <NewLine> REPLY_DATE 8: April 29, 2020, 10:44am; <NewLine> REPLY_DATE 9: April 29, 2020, 10:51am; <NewLine> REPLY_DATE 10: April 29, 2020, 11:17am; <NewLine> REPLY_DATE 11: May 12, 2020,  9:44pm; <NewLine> REPLY_DATE 12: May 12, 2020,  9:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> 
80338,Construct quantized Tensor from int_repr(),2020-05-08T14:47:15.759Z,1,97,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Dear pytorch community,<br/><NewLine>Given an integer representation and the affine transform, how do I create a corresponding torch.tensor object? The python tensor constructor seems to assume no quantization.<br/><NewLine>Thanks a lot for your help,<br/><NewLine>moritz</p><NewLine></div>",https://discuss.pytorch.org/u/v0lta,(Moritz Wolter),v0lta,"May 8, 2020,  2:47pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For posterity since I was looking here for the solution, if your main interest is in the original weight approximation one solution is to approximate the original weights manually:<br/><NewLine>restweights = (quant_ints - q_zero_point)*q_scale<br/><NewLine>This ‘hacky’ approach works for me since I was going to call dequantize anyway.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>we do have some non-public API to do this: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3862"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3862</a> and <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3868"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L3868</a></p><NewLine><p>but they we might change the API when we officially release quantization as a stable feature.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/v0lta; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  3:10pm; <NewLine> REPLY_DATE 2: May 8, 2020, 10:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
79759,Sub-8 bit quantization,2020-05-05T00:47:32.563Z,0,105,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to simulate sub-8 bit quantization. Currently, I am only doing post-training quantization, and I am doing so by creating a custom observer which is identical to the existing <code>HistogramObserver</code> except that the <code>qmin</code> and <code>qmax</code> values are changed to match the new bit width.<br/><NewLine>Running this with ResNet-50, with 4 bit activations and 8 bit weights, the top 1 accuracy is around 10%, which is significantly lower than the results reported in the whitepaper by Krishnamoorthi (36%). Are there other changes which need to be made to simulate this quantization, or does this come down to some kind of TF vs. Pytorch difference?</p><NewLine></div>",https://discuss.pytorch.org/u/zlou,(Joe Lou),zlou,"May 5, 2020, 12:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/hx89"">@hx89</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: May 8, 2020,  6:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
78505,"During QAT, how to save the float32 model without fuse module?",2020-04-26T11:56:50.508Z,2,133,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I tried QAT with this example <a href=""https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py</a>.<br/><NewLine>I would like to know whether the fp32 model can be saved during the quatization, which has no extra parameters learned by QAT?<br/><NewLine>This FP32 model without fusion, just like the normal pretrained float32 model.</p><NewLine></div>",https://discuss.pytorch.org/u/onesnow123q,(yiiiiiix),onesnow123q,"April 26, 2020, 11:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>are you asking if you can save a normal fp32 model? of course, you can find the doc for serialization here: <a href=""https://pytorch.org/docs/stable/notes/serialization.html?highlight=save"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/notes/serialization.html?highlight=save</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks!<br/><NewLine>I know how to save a normal fp32 model, but I don’t know how to save it during quantization ware training.Because the model which saved during QAT has some params such as scale , zero_points and so on.I want these params disappear in the model.But I don’t know how to change code to achieve it.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""78505"" data-username=""onesnow123q""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/o/ebca7d/40.png"" width=""20""/> onesnow123q:</div><NewLine><blockquote><NewLine><p>I know how to save a normal fp32 model, but I don’t know how to save it during quantization ware training.Because the model which saved during QAT has some params such as scale , zero_points and so on.I want these params disappear in the model.But I don’t know how to change code to achieve it.</p><NewLine></blockquote><NewLine></aside><NewLine><p>why do you want these params to disappear? we need to save scale/zero_point of qat model since it’s part of the state of the model</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/onesnow123q; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: April 28, 2020,  9:38pm; <NewLine> REPLY_DATE 2: April 29, 2020,  2:36am; <NewLine> REPLY_DATE 3: May 8, 2020,  5:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
69512,Casting from 32b to 8 bit after accumulation in a multiplication,2020-02-12T17:10:30.365Z,2,200,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I was reading about the QNNpack and FBGEMM configuration which explains really well the way to multiply, but I am left with the question what is happening after you multiply 8b x 8b and accumulate the result on 16 or 32 bits…how do you cast to 8 bits to get to another convolution?</p><NewLine></div>",https://discuss.pytorch.org/u/dassima,(Dassima),dassima,"February 12, 2020,  5:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> <a class=""mention"" href=""/u/jianyuhuang"">@jianyuhuang</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>In FBGEMM, we have requantization as a post-processing step to convert the 32 bit to 8 bit after the accumulation. The requantization basically does the following op in the language of numpy:</p><NewLine><pre><code class=""lang-auto"">X_q = (np.round(X / X_scale) + X_zp).clip(0, 255).astype(np.uint8)<NewLine></code></pre><NewLine><p>The modularized requantization wrapper is here:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L59-L185"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L59-L185"" rel=""nofollow noopener"" target=""_blank"">pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L59-L185</a></h4><NewLine><pre class=""onebox""><code class=""lang-h""><ol class=""start lines"" start=""59"" style=""counter-reset: li-counter 58 ;""><NewLine><li>template &lt;</li><NewLine><li>    bool FUSE_RELU,</li><NewLine><li>    QuantizationGranularity Q_GRAN,</li><NewLine><li>    typename BIAS_TYPE,</li><NewLine><li>    typename outT,</li><NewLine><li>    typename inT,</li><NewLine><li>    typename nextOPType&gt;</li><NewLine><li>template &lt;inst_set_t instSet&gt;</li><NewLine><li>inline int</li><NewLine><li>ReQuantizeOutput&lt;FUSE_RELU, Q_GRAN, BIAS_TYPE, outT, inT, nextOPType&gt;::f(</li><NewLine><li>    outT* out,</li><NewLine><li>    const inT* inp,</li><NewLine><li>    const block_type_t&amp; block,</li><NewLine><li>    int ld_out,</li><NewLine><li>    int ld_in) const {</li><NewLine><li>  static_assert(</li><NewLine><li>      std::is_same&lt;inT, int32_t&gt;::value,</li><NewLine><li>      ""input data type must be of int32_t type"");</li><NewLine><li>  int ncol_per_group = ncols_ / groups_;</li><NewLine><li>  assert(</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L59-L185"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><br/><NewLine>And inside it a efficient AVX2 kernel is implemented:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/FBGEMM/blob/master/src/QuantUtilsAvx2.cc#L533-L554"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/FBGEMM/blob/master/src/QuantUtilsAvx2.cc#L533-L554"" rel=""nofollow noopener"" target=""_blank"">pytorch/FBGEMM/blob/master/src/QuantUtilsAvx2.cc#L533-L554</a></h4><NewLine><pre class=""onebox""><code class=""lang-cc""><ol class=""start lines"" start=""533"" style=""counter-reset: li-counter 532 ;""><NewLine><li>/*</li><NewLine><li> * Convert scaled FP32 result to int32_t using CVTPS2DQ instruction.</li><NewLine><li> * CVTPS2DQ instruction rounds result according to nearest FP32 value</li><NewLine><li> * with ties to even (assuming default MXCSR rounding mode). However,</li><NewLine><li> * when conversion overflows, it produces INT32_MIN as a result. For</li><NewLine><li> * large positive inputs the result of conversion can become negative,</li><NewLine><li> * which affects the final requantization result. Note that on x86</li><NewLine><li> * SSE2 we have e.g. int32_t(float(INT32_MAX)) == INT32_MIN! This</li><NewLine><li> * happens because float(INT32_MAX) rounds to 2**31, which overflows</li><NewLine><li> * int32_t when it is converted back to integer.</li><NewLine><li> *</li><NewLine><li> * Thankfully, we can prove that overflow never happens in this</li><NewLine><li> * requantization scheme. The largest positive input is INT32_MAX</li><NewLine><li> * (2**31 - 1), which turns into 2**31 when converted to float. The</li><NewLine><li> * largest scale value is 0x1.FFFFFEp-1. When multiplied together, the</li><NewLine><li> * result is 2147483520 (compare to INT32_MAX = 2147483647), which</li><NewLine><li> * fits into int32_t without overflow.</li><NewLine><li> */</li><NewLine><li>__m256i x_rounded_v = _mm256_cvtps_epi32(x_scaled_v);</li><NewLine><li>__m256i y_rounded_v = _mm256_cvtps_epi32(y_scaled_v);</li><NewLine></ol></code></pre><NewLine><NewLine>  This file has been truncated. <a href=""https://github.com/pytorch/FBGEMM/blob/master/src/QuantUtilsAvx2.cc#L533-L554"" rel=""nofollow noopener"" target=""_blank"">show original</a><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>ok, this makes sense now. But this raises 3 questions for me when I computed an example:</p><NewLine><ol><NewLine><li>How do I access the scale of the re-quantization? I just managed to see how the scale of converting from 32float to 8int looks like:<br/><NewLine>BASIC EXAMPLE:</li><NewLine></ol><NewLine><pre><code class=""lang-auto"">input: tensor([[[[ 1.0074,  2.0148,  3.0222,  4.0297],<NewLine>                 [ 5.0371,  6.0445,  7.0519,  8.0593],<NewLine>                 [ 8.9408,  9.9482, 10.9556, 11.9630],<NewLine>                 [12.9704, 13.9779, 14.9853, 15.9927]]]], size=(1, 1, 4, 4),<NewLine>                 dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,<NewLine>                 scale=0.125926584005, zero_point=0)<NewLine>Weight:  &lt;bound method Conv2d.weight of QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), <NewLine>          scale=4.51094579697, zero_point=0)<NewLine>output: tensor([[[[347.3428, 392.4523],<NewLine>                  [527.7806, 572.8901]]]], size=(1, 1, 2, 2), dtype=torch.quint8,<NewLine>                  quantization_scheme=torch.per_tensor_affine, scale=4.51094579697,<NewLine>                  zero_point=0)<NewLine></code></pre><NewLine><p>PRINTING OUT THE DICTIONARY OF 8INT MODEL:</p><NewLine><pre><code class=""lang-auto"">[(u'conv1.weight', tensor([[[[0.9882, 1.9765, 2.9647],<NewLine>                             [4.0235, 5.0118, 6.0000],<NewLine>                             [6.9882, 7.9765, 8.9647]]]], size=(1, 1, 3, 3), dtype=torch.qint8,<NewLine>                             quantization_scheme=torch.per_channel_affine,<NewLine>                             scale=tensor([0.0706], dtype=torch.float64), zero_point=tensor([0]),<NewLine>                             axis=0)),<NewLine>(u'conv1.scale', tensor(4.5109)),<NewLine>(u'conv1.zero_point', tensor(0)), <NewLine>(u'conv1.bias', tensor([0.], requires_grad=True)), <NewLine>(u'quant.scale', tensor([0.1259])), <NewLine>(u'quant.zero_point', tensor([0]))]<NewLine></code></pre><NewLine><p>Because what I have is:</p><NewLine><ul><NewLine><li>scale 0.12 to convert input from 32float to 8int</li><NewLine><li>scale 0.0706 to convert weights from 32float to 8int</li><NewLine><li>scale 4.51 to convert output from 32float to 8int (just to see how all the values are in int)</li><NewLine></ul><NewLine><p><strong>I was expecting the scale of requantization to be something like 8.33 for this example.</strong><br/><NewLine>2. Are this scales trainable parameters or are computed using basic numpy operations?</p><NewLine><ol start=""2""><NewLine><li>In the FBGEMM documentation on Facebook Engineering I’ve read that the accumulation is done on 16b… so then from where is the difference?</li><NewLine></ol><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""69512"" data-username=""dassima""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dassima/40/20474_2.png"" width=""20""/> dassima:</div><NewLine><blockquote><NewLine><p>on Facebook Engineering I’ve read that the accumulation is done on 16b… so then from where is the difference?</p><NewLine></blockquote><NewLine></aside><NewLine><p>HELLO, if you already know the principle？？I am confused about it too.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jianyuhuang; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/onesnow123q; <NewLine> ,"REPLY_DATE 1: February 13, 2020,  8:21pm; <NewLine> REPLY_DATE 2: February 13, 2020, 11:51pm; <NewLine> REPLY_DATE 3: February 14, 2020,  3:42pm; <NewLine> REPLY_DATE 4: April 29, 2020,  2:56am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
74839,Quantize activations with qinit8,2020-03-30T20:26:47.738Z,4,284,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,<br/><NewLine>I was wondering if I can use observers for activations with dtype of qint8.<br/><NewLine>It seems by the <a href=""https://pytorch.org/docs/stable/quantization.html#operation-coverage"" rel=""nofollow noopener"">documentation</a> that it cannot be done (I want to make sure I got it correctly).</p><NewLine><p>When I tried it with different observers, it failed for this kind of error when evaluating:<br/><NewLine>RuntimeError: expected scalar type QUInt8 but found QInt8 (data_ptr<a>c10::quint8</a> at /pytorch/build/aten/src/ATen/core/TensorMethods.h:6322)</p><NewLine><p>Does qint8 supported for activation quantization?</p><NewLine><p>Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/Reuven_Peretz,(Reuven Peretz),Reuven_Peretz,"March 30, 2020,  8:27pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The default observer for activation is quint8, but you may overwrite it by creating a new activation observer such as:<br/><NewLine>new_activation_observer = MinMaxObserver.with_args(dtype=torch.qint8)</p><NewLine><p>and then assign this observer to the model qconfig before quantizing the model:<br/><NewLine>myModel.qconfig = QConfig(activation=new_activation_observer,<br/><NewLine>weight=default_weight_observer)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks.<br/><NewLine>Did you try it?<br/><NewLine>Because I just tried what you offered and got again the above-mentioned error:<br/><NewLine>RuntimeError: expected scalar type QUInt8 but found QInt8 (data_ptr<a>c10::quint8</a> at /pytorch/build/aten/src/ATen/core/TensorMethods.h:6322)</p><NewLine><p>I think it’s not supported, but I don’t understand why…</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>which quantized op are you calling? some ops only works for quint8</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""74839"" data-username=""hx89""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/h/f4b2a3/40.png"" width=""20""/> hx89:</div><NewLine><blockquote><NewLine><p>activation=new_activation_observer,<br/><NewLine>weight=default_weight_observer</p><NewLine></blockquote><NewLine></aside><NewLine><p>I tried this, but got the same error<br/><NewLine>RuntimeError: expected scalar type QUInt8 but found QInt8 (data_ptr<a>c10::quint8</a> at /pytorch/build/aten/src/ATen/core/TensorMethods.h:6322)<br/><NewLine>the op is<br/><NewLine>self.conv0 = nn.Conv2d(3, 3, kernel_size=1, stride=1, padding=0, bias=False)</p><NewLine><p>which quantized op are supported qint8??</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>conv only support quint8 as input, I think right now most of the ops that doing some computation on the data itself will require quint8.</p><NewLine><p>for example, conv expects quint8 activations and qint8 weight, so you need to set the dtype to quint8 for activation observer and qint8 for weight observer</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Reuven_Peretz; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/onesnow123q; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: March 31, 2020,  4:58pm; <NewLine> REPLY_DATE 2: April 1, 2020,  8:10am; <NewLine> REPLY_DATE 3: April 8, 2020, 11:44pm; <NewLine> REPLY_DATE 4: April 24, 2020, 11:03am; <NewLine> REPLY_DATE 5: April 28, 2020,  9:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
78268,How to static quantization? I have some questions after reading the tutorial,2020-04-24T18:55:12.013Z,1,194,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I quantified the model based on the tutorial, but the model’s error is a bit large. Therefore, I try to quantify the data myself, then dequantize it, and finally calculate the convolution, and find that this error is smaller than the error of the tutorial method. what is the reason? Did I miss some steps when quantifying the model? The following  is a demo.</p><NewLine><pre><code class=""lang-python"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.functional as F<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine><NewLine>class Test(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Test, self).__init__()<NewLine>        self.conv = nn.Conv2d(132, 121, 3, 1, bias=False)<NewLine>        self.quan = QuantStub()<NewLine>        self.dequan = DeQuantStub()<NewLine>        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out')<NewLine><NewLine>    def forward(self, x):<NewLine>        out = self.quan(x)<NewLine>        out = self.conv(out)<NewLine>        out = self.dequan(out)<NewLine>        return out<NewLine><NewLine>def test():<NewLine>    model = Test()<NewLine>    weight = model.conv.weight<NewLine>    conv = nn.Conv2d(132, 121, 3, 1, bias=False)<NewLine>    conv.weight = weight<NewLine>    max, min = torch.max(weight.flatten()), torch.min(weight.flatten())<NewLine>    scale = (max - min) / 256<NewLine>    temp = torch.quantize_per_tensor(weight, scale=scale.item(), zero_point=0, dtype=torch.qint8)<NewLine>    temp_de = temp.dequantize()<NewLine>    example = torch.rand(1, 132, 64, 64)<NewLine>    print(F.mse_loss(temp_de, weight))<NewLine>    # tensor(1.9924e-07, grad_fn=&lt;MeanBackward0&gt;)<NewLine>    print(F.mse_loss(F.conv2d(example, weight=temp_de, bias=None, stride=1), conv(example)))<NewLine>    # tensor(8.7799e-05, grad_fn= &lt; MeanBackward0 &gt;)<NewLine>    torch.backends.quantized.engine = 'qnnpack'<NewLine>    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine>    torch.quantization.prepare(model, inplace=True)<NewLine>    torch.quantization.convert(model, inplace=True)<NewLine>    print(F.mse_loss(conv(example), model(example)))<NewLine>    # tensor(0.4824, grad_fn= &lt; MseLossBackward &gt;)<NewLine><NewLine><NewLine>if __name__ == ""__main__"":<NewLine>    test()<NewLine>    print(torch.__version__)<NewLine>    # 1.5.0<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Silbernitrat,,Silbernitrat,"April 24, 2020,  6:56pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In the first comparison, you are not quantizing the activations (i.e the example tensor). When you call the torch.quantization.prepare and convert, the activations are quantized too. In this case, since you are skipping calibration (i.e you are calling convert after prepare), the activations are quantized with a default scale of 1.0, leading to a large quantization error. Please repeat this experiment with calibration and you should see lower error</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks. I konw that I need  to run the model before convernting it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Silbernitrat; <NewLine> ,"REPLY_DATE 1: April 25, 2020, 12:31am; <NewLine> REPLY_DATE 2: April 25, 2020,  9:31am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
78009,Net in DataParallel make training aware quantization convert model acc error,2020-04-23T08:48:33.769Z,1,111,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I run net train and test in single GPU is good, and I try to train model in multi-gpus, and I found that the process of training is good, however, when I convert model into quantization, and the model acc is 1% in CIFAR100 which means random.<br/><NewLine>My convert code is as follows:</p><NewLine><pre><code class=""lang-auto"">126     if gpus == 1:<NewLine>127         quantized_model = torch.quantization.convert(net.eval().to(device), inplace=False)<NewLine>128     else:<NewLine>129         print(""expert model from torch DataParallel"")<NewLine>130         single_net = deepcopy(net.module)<NewLine>131         #single_net = net<NewLine>132         quantized_model = torch.quantization.convert(single_net.eval().to(device), inplace=False)<NewLine>133         #quantized_model = quantized_model.module<NewLine>134     quantized_model.eval()<NewLine></code></pre><NewLine><p>if gpu number is more than one, I use net = torch.nn.DataParallel(net), so when I need to convert it, I first do single_net = deepcopy(net.module), but the acc of quantized_model is 1% , well the training acc is about 77%. Meanwhile,  it has warning:</p><NewLine><blockquote><NewLine><p>/home/autolab/anaconda3/lib/python3.7/site-packages/torch/quantization/observer.py:208: UserWarning: Must run observer before calling calculate_qparams.                           Returning default scale and zero point.<br/><NewLine>Returning default scale and zero point."")</p><NewLine></blockquote><NewLine><p>So if anyone has any ideas?</p><NewLine></div>",https://discuss.pytorch.org/u/dingyongchao,(DingYongchao),dingyongchao,"April 23, 2020,  8:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>There are currently some issues with nn.DataParallel and Quantization Aware Training. There is a WIP PR to fix it - <a href=""https://github.com/pytorch/pytorch/pull/37032"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/37032</a><br/><NewLine>You can follow the toy example here to make sure you’re following the steps for QAT correctly <a href=""https://gist.github.com/vkuzo/78b06c01f23f98ee2aaaeb37e55f8d40"" rel=""nofollow noopener"">https://gist.github.com/vkuzo/78b06c01f23f98ee2aaaeb37e55f8d40</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks， I think I has solve this problem by follows:</p><NewLine><pre><code class=""lang-auto"">178     test(net.module, epoch, device, args.gpus)<NewLine>179     qat_model = deepcopy(net)<NewLine>180     qat_model.eval().to('cpu')<NewLine>181     torch.quantization.convert(qat_model, inplace=True)<NewLine>182     test(qat_model.module, epoch, 'cpu', args.gpus)<NewLine>183     scheduler.step()<NewLine></code></pre><NewLine><p>I need to run net.module first, then the qat_model in cpu is correct</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dingyongchao; <NewLine> ,"REPLY_DATE 1: April 24, 2020,  1:19am; <NewLine> REPLY_DATE 2: April 24, 2020,  1:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
77872,Per tensor symmetric quantiation,2020-04-22T14:13:30.290Z,0,118,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello !<br/><NewLine>I can’t understand where I have the error, in the configuration I write that I want fake per_tensor_<strong>symmetric</strong> quantization, but when I display the picture of the graph, he writes that I have a FakeQuantizePerTensor<strong>Affine</strong>Backward node.</p><NewLine><p>What am I doing wrong?<br/><NewLine>Thank you in advance!</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn<NewLine>from torchviz import make_dot<NewLine><NewLine>model = nn.Sequential()<NewLine>model.add_module('W0', nn.Linear(8, 16))<NewLine>model.add_module('tanh', nn.Tanh())<NewLine>model.add_module('W1', nn.Linear(16, 1))<NewLine>model.qconfig = torch.quantization.QConfig(activation=torch.quantization.FakeQuantize.with_args(observer=torch.quantization.MinMaxObserver, <NewLine>                                                            quant_min=0,<NewLine>                                                            quant_max=255,<NewLine>                                                            qscheme=torch.per_tensor_symmetric,<NewLine>                                                            dtype=torch.quint8,<NewLine>                                                            reduce_range=False),<NewLine>                   weight=torch.quantization.FakeQuantize.with_args(observer=torch.quantization.MinMaxObserver, quant_min=-128, quant_max=127,<NewLine>                                                   dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False))<NewLine>torch.quantization.prepare_qat(model, inplace=True)<NewLine>x = torch.randn(1,8)<NewLine>make_dot(model(x), params=dict(model.named_parameters()))<NewLine></code></pre><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/8d47912ed94e09a02158837b17bf1e28d38918a7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7.png"" title=""Снимок экрана от 2020-04-22 17-07-20""><img alt=""Снимок экрана от 2020-04-22 17-07-20"" data-base62-sha1=""k9OECwu35bwPkTDRMCY0a1Tfxbx"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7_2_481x500.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7_2_481x500.png, https://discuss.pytorch.org/uploads/default/original/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/8/d/8d47912ed94e09a02158837b17bf1e28d38918a7.png 2x"" width=""481""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">Снимок экрана от 2020-04-22 17-07-20</span><span class=""informations"">696×722 41.4 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/kirilllukorovich,(Kirilllukorovich),kirilllukorovich,"April 22, 2020,  2:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>FakeQuantizePerTensorAffineBackward handles both symmetric and asymmetric. In case of symmetric the zero_point would be set to 0. So your example looks fine to me.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: April 22, 2020,  5:41pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
77122,Changing quant_max and quant_min doesn&rsquo;t have any effect,2020-04-17T00:46:18.927Z,0,88,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to have a more limited range for values in my model. I set quant_max and quant_min to the new values as an argument in FakeQuantize, but when I actually print int_repr() of values, it’s still between 0-255 (or -128 to 127).</p><NewLine></div>",https://discuss.pytorch.org/u/mg1371,,mg1371,"April 17, 2020, 12:46am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>right now the observer uses fixed range depending on dtype: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L153-L162"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L153-L162</a>, feel free to add the support for quant_min and quant_max for observer.<br/><NewLine>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> for sub 8 bit observer support.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, we have a PR in the works for supporting sub 8 bit quantization at: <a href=""https://github.com/pytorch/pytorch/pull/33743"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33743</a>, expect to land this end of next week</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: April 17, 2020,  8:48pm; <NewLine> REPLY_DATE 2: April 18, 2020,  1:40am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
76528,[quantization] how to quantize model which include not support to quantize layer,2020-04-13T08:38:25.932Z,0,521,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I have a model which include <code>prelu</code> layer, not support to quantize in current pytorch version, how to  quantize this model for x86 CPU now?  I try to define this model with the following format:</p><NewLine><pre><code class=""lang-python"">self.convbn1 = QuantizableConvBNBlock(xxx) (has defined)<NewLine>self.prelu = nn.PReLU()<NewLine>self.convbn2 = QuantizableConvBNBlock(xxx)        <NewLine>self.quant = torch.quantization.QuantStub()<NewLine>self.dequant = torch.quantization.DeQuantStub()<NewLine>        <NewLine>def forward(self, x):<NewLine>    x = self.quant(x)<NewLine>    x = self.convbn1(x) <NewLine>    x = self.dequant(x)<NewLine>    x = self.prelu(x)<NewLine>    x = self.quant(x)<NewLine>    x = self.convbn2(x)<NewLine>    ...<NewLine></code></pre><NewLine><p>but I found after perform the quantization-aware training following tutorial, eval result is very terrible, what is the reason and how to solve it ?<br/><NewLine>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/zhangyongwei93,,zhangyongwei93,"April 13, 2020,  8:38am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>you’ll need to use different quant stub module for each usage because it is recording tensor statistics at that point.<br/><NewLine>e.g.</p><NewLine><pre><code class=""lang-python"">x = self.quant1(x)<NewLine>...<NewLine>x = self.dequant(x)<NewLine>...<NewLine>x = self.quant2(x)<NewLine>...<NewLine>x = self.dequant(x)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: April 17, 2020,  8:43pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
70605,How to use a quantized model on INT8 harware?,2020-02-21T16:07:13.728Z,11,276,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Recently I used pytorch quantization-aware training to quantize my model.<br/><NewLine>The result still has good accuracy, and it uses per channel scales.</p><NewLine><p>However, our hardware colleagues told me that because it has FP scales and zero-points in channels, the hardware <strong>should still support FP</strong> in order to implement it.</p><NewLine><p>They also argued that in each internal stage, the values (in-channels) should be dequantized and converted to FP and quantized again for the next layer.</p><NewLine><p>I’m not sure if this argument is valid or if such limitation is applied on such quantization?</p><NewLine></div>",https://discuss.pytorch.org/u/babak_hss,(Bob),babak_hss,"February 21, 2020,  4:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>For the first argument you are right, since scales and zero-points are FP, hardware need to support FP for the computation.</p><NewLine><p>The second argument may not be true, for static quantization the output of the previous layer can be fed into next layer without dequantizing to FP. Maybe they are thinking about dynamic quantization, which keeps tensors between two layers in FP.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are these per channel scales determined for quantizing the network weights or for the tensors (layer outputs)?</p><NewLine><p>If it’s the first case, then can’t we compute INT8 weights offline (using the obtained scales) and upload them to the hardware without the need for having FP support? I only use the hardware for the inference phase.</p><NewLine><p>If these scales are required for quantizing the internal tensors (layers outputs/inputs), then doesn’t it mean that they have to be dequantized first in each stage to be quantized again with a different scale?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>For static quantization, you can fed directly the outputs to the next layer, but how do you add the zero points if they are 32FP? or when exactly in the flow?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>if it helps, I found out that they use the scales to convert to 8 int the weights and data (but each feature map, kernel, input has its own scale) and after that do the multiplications in 8 int, they accumulate the result to 32int for each layer and requantize to 8 int using another scale which you can find it only when you acces the model as a dictionary.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""70605"" data-username=""dassima""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/dassima/40/20474_2.png"" width=""20""/> dassima:</div><NewLine><blockquote><NewLine><p>they accumulate the result to 32int for each layer and requantize to 8 int using another scale</p><NewLine></blockquote><NewLine></aside><NewLine><p>So, does this requantization step in each layer (to convert its output to INT8 with another scale) requires the hardware to support FP32 arithmetic?</p><NewLine><p>Also, how expensive would this conversion be compared to doing everything in FP32?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the answer is yes if you want to do in hardware the whole inference at once… And I do not know how expensive it is in comparison. I think it actually depends on the piece of hardware</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, in our implementations we do a floating point multiply to requantize from 32 bit to 8 bit. You can also do this with integer arithmetic if needed. We found that on ARM and x86 CPUs, doing the requantization with fp32 is more efficient.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is there other possibilities like having INT8/INT32 scale factors or having INT32 but non-per channel quantization?</p><NewLine><p>The per-channel quantization is great regarding the accuracy, but I do not need that much of accuracy and instead need to perform all the calculations in INT8 or even INT32.</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I see, we don’t have this right now, you will need to write a new Quantizer to enable this: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/quantized/Quantizer.h"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/quantized/Quantizer.h</a></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a way to fake quantize scale factors or zero points (instead of defining new Quantizers)?</p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>if you only need this in quantization aware training, you’ll need to define your own fake quantize module(<a href=""https://github.com/pytorch/pytorch/blob/v1.3.1/torch/quantization/fake_quantize.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/v1.3.1/torch/quantization/fake_quantize.py</a>) and fake quantize op<br/><NewLine>(<a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp</a>)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/babak_hss; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/babak_hss; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/dassima; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/babak_hss; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/mg1371; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 23, 2020,  1:23am; <NewLine> REPLY_DATE 2: February 24, 2020, 11:05am; <NewLine> REPLY_DATE 3: February 24, 2020, 12:26pm; <NewLine> REPLY_DATE 4: February 24, 2020, 12:32pm; <NewLine> REPLY_DATE 5: February 24, 2020,  2:17pm; <NewLine> REPLY_DATE 6: February 24, 2020,  2:34pm; <NewLine> REPLY_DATE 7: March 3, 2020,  1:50am; <NewLine> REPLY_DATE 8: April 4, 2020,  7:34am; <NewLine> REPLY_DATE 9: April 8, 2020, 11:46pm; <NewLine> REPLY_DATE 10: April 14, 2020,  6:13pm; <NewLine> REPLY_DATE 11: April 17, 2020,  8:41pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> 
76671,QAT: evaluation during training,2020-04-14T02:15:08.042Z,0,90,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>For QAT, do we need to use .convert() at the end of each epoch to fully convert and evaluate on validation data or the model already fake quantizes the forward pass (and would produce the same results as converted model)?</p><NewLine><p>Many Thanks!</p><NewLine></div>",https://discuss.pytorch.org/u/mg1371,,mg1371,"April 14, 2020,  2:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It is not necessary to convert at the end of every epoch. The fake quantized model is expected to be very close to the final converted model, so one conversion at the end should suffice. We are in the process of aligning the numerics better between fake quant and converted model so if you are seeing some differences they should get better.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/supriyar; <NewLine> ,"REPLY_DATE 1: April 14, 2020,  4:14pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75642,Quantization for conv models with custom modules,2020-04-07T03:16:25.849Z,0,106,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In per-channel quantization for conv models (done following <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a>), the scales and zero points of weights and intermediate outputs of the model are always 1 and 0, i.e. the range is same as in the float model. I have a custom module in between convbnrelu modules, which I want to quantize separately. But the quant and dequant stubs are only rounding off or float typecasting the outputs. Since there is no scale and zero point attached, I am not able to quantize my custom model properly. Is there any other way to quantize so I can get scales and zero points (the output/weight range is spread out to -127 to 128 or 0 to 255 rather than being same) for the output of the quantized convbnrelu layer?</p><NewLine></div>",https://discuss.pytorch.org/u/pytos,,pytos,"April 7, 2020,  3:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>scales and zero_points are 1 and 0 by default. Did you run calibration step, quantize step?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: April 10, 2020,  6:54pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75805,How to see the quantized weights?,2020-04-08T06:42:44.537Z,0,158,"<div class=""post"" itemprop=""articleBody""><NewLine><p>like tensor.int_repr() can return the int values of quantized tensor, how can we get the int values of the quantized weights(_packed_params).<br/><NewLine>while printing the _packed_params, we can get a uint8 tensor, what does it stand for?</p><NewLine></div>",https://discuss.pytorch.org/u/HumberMe,(Humble Me),HumberMe,"April 8, 2020,  6:42am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. Please use &lt;q_module&gt;.weight(). It returns a quantized tensor of weights and you can use int_repr() to get the int8_t values of weights. Note: Weights are usually quantized with dtype = int8_t.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: April 10, 2020,  6:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
75937,"Quantization-aware training conv1D, LSTM support",2020-04-09T00:11:59.855Z,1,161,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am just taking a look at using Quantization-aware training. At the moment I am making my way through the tutorials.</p><NewLine><p>My current models make use of layers such as conv1D,and in some cases LSTM and GRU etc…</p><NewLine><p>According to the documention this is not currently supported for those particular layer types. As a work around, one possibility would be to replace the conv1D with conv2D. I would convert the data to 2D (actually remains 1D, but 2D in dimensions).</p><NewLine><p>With regards to LSTM, GRU, I am not yet sure.</p><NewLine></div>",https://discuss.pytorch.org/u/Paul_Creaser,(Paul Creaser),Paul_Creaser,"April 9, 2020, 12:11am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems this may partially answer my question.</p><NewLine><aside class=""quote"" data-post=""1"" data-topic=""68016""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/snakers41/40/5641_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/quantization-support-for-1d-convolutions/68016"">Quantization support for 1D convolutions?</a> <a class=""badge-wrapper bullet"" href=""/c/quantization""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is for questions, discussion and issues related to PyTorch’s quantization feature."">quantization</span></a><NewLine></div><NewLine><blockquote><NewLine>    Hi! <NewLine>Are you planning to add support for 1D convolutions for <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">quantization</a>?<NewLine>  </blockquote><NewLine></aside><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/paul_creaser"">@Paul_Creaser</a>,<br/><NewLine>Conv1d is now available in nightly builds. There is <a href=""https://pytorch.org/docs/stable/quantization.html#torch-nn-quantized-dynamic"" rel=""nofollow noopener"">LSTM available with dynamic quantization</a> and GRU is currently not available. However, an <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/dynamic/modules/rnn.py#L67"" rel=""nofollow noopener"">RNN base class is available with dynamic quantization</a>.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Paul_Creaser; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: April 9, 2020,  1:12am; <NewLine> REPLY_DATE 2: April 10, 2020, 12:10am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
74224,Linear_dynamic has some problems with qnnpack,2020-03-24T11:52:10.215Z,1,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone. I recently use the torch.quantization.quantize_dynamic(model, dtype=torch.qint8) to dynamic quantize my model. When use fbgemm（the default engine） to quantize the model, it works. However, when I change the engine from fbgemm to qnnpack, it has some problems. The way I use qnnpack is like this: model.qconfig = torch.quantization.get_default_qconfig(‘qnnpack’)<br/><NewLine>torch.backends.quantized.engine = ‘qnnpack’.<br/><NewLine>qnnpack causes the problem like that:<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5"" href=""https://discuss.pytorch.org/uploads/default/original/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5.png"" title=""image""><img alt=""image"" data-base62-sha1=""ytqrLsTZpepZCJZmA49W4c99Gmx"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5_2_10x10.png"" height=""106"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5_2_690x106.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5_2_690x106.png, https://discuss.pytorch.org/uploads/default/optimized/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5_2_1035x159.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/f/1/f19d7843b03616ae933ed2835f9d3e0a6ce1f2e5.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1200×186 32.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>Has anyone ever done any related work or met the same problem?<br/><NewLine>by the way, the torch version is 1.5.0a0+b336deb</p><NewLine><p>I’d appreciate if anybody can help me! Thanks in advance!</p><NewLine></div>",https://discuss.pytorch.org/u/huoge,,huoge,"March 25, 2020,  1:29am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/supriyar"">@supriyar</a> do you know?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>You don’t need to necessarily set the <code>qconfig</code> in this case to qnnpack. Please try using <code>default_dynamic_qconfig</code> instead and see if that helps to solve the issue. Also make sure if your build has qnnpack enabled.</p><NewLine><p>I was able to get this example model to work with qnnpack backend.</p><NewLine><pre><code class=""lang-auto"">class SingleLayerLinearDynamicModel(torch.nn.Module):<NewLine>    def __init__(self):<NewLine>        super(SingleLayerLinearDynamicModel, self).__init__()<NewLine>        self.qconfig = default_qconfig<NewLine>        self.fc1 = torch.nn.Linear(5, 5).to(dtype=torch.float)<NewLine><NewLine>    def forward(self, x):<NewLine>        x = self.fc1(x)<NewLine>        return x<NewLine><NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine>base = SingleLayerLinearDynamicModel()<NewLine>model = quantize_dynamic(base, dtype=torch.qint8)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>thx for your reply! I found this was a bug in my code，in some iteration, I pass the tensor with the shape (0, 20, 80). In fbgemm, it does not affect the result, while in qnnpack, it will report that error in the picture. so I fix the bug in my code, then everything goes well.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/huoge; <NewLine> ,"REPLY_DATE 1: March 27, 2020, 11:17pm; <NewLine> REPLY_DATE 2: March 29, 2020,  6:18pm; <NewLine> REPLY_DATE 3: March 30, 2020,  6:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
73720,Error when QAT with Dataparallel in multi-GPU,2020-03-19T07:32:33.917Z,3,306,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I follow the QAT tutorial to train a model. When I dont use Dataparallel or use Dataparallel with single GPU, it goes well. But when I train model with Dataparallel in multi-GPU, it raise an error just after <code>disable_observer</code> apply.<br/><NewLine>The output with error:</p><NewLine><pre><code class=""lang-auto"">/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/quantization/observer.py:208: UserWarning: Must run observer before calling calculate_qparams.                           Returning default scale and zero point.          Returning default scale and zero point."")<NewLine>[0.5, 0.5, 0.73046875, 0.99609375, 0.953125, 0.5, 0.5, 0.5, 0.5, 0.5]<NewLine>[0.5, 0.5, 0.73046875, 0.99609375, 0.99609375, 0.5, 0.5, 0.5, 0.5, 0.5]<NewLine>Traceback (most recent call last):<NewLine>  File ""quant_test.py"", line 50, in &lt;module&gt;<NewLine>    quant_model = torch.quantization.convert(quant_model.eval().cpu(), inplace=False)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/quantization/quantize.py"", line 313, in convert<NewLine>    reassign[name] = swap_module(mod, mapping)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/quantization/quantize.py"", line 335, in swap_module<NewLine>    new_mod = mapping[type(mod)].from_float(mod)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py"", line 49, in from_float<NewLine>    return super(ConvReLU2d, cls).from_float(mod)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/quantized/modules/conv.py"", line 250, in from_float<NewLine>    weight_post_process(mod.weight)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/home/eleflea/.pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/quantization/fake_quantize.py"", line 86, in forward<NewLine>    self.ch_axis, self.quant_min, self.quant_max)<NewLine>RuntimeError: dimensions of scale and zero-point are not consistent with input tensor<NewLine></code></pre><NewLine><p>My test code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from torch import nn, optim<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine>from copy import deepcopy<NewLine><NewLine>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<NewLine><NewLine>model = nn.Sequential(<NewLine>    QuantStub(),<NewLine>    nn.Conv2d(3, 16, 1, bias=False),<NewLine>    nn.BatchNorm2d(16),<NewLine>    nn.ReLU(),<NewLine>    nn.Conv2d(16, 10, 3, stride=2, padding=1, bias=False),<NewLine>    nn.BatchNorm2d(10),<NewLine>    nn.AvgPool2d(14),<NewLine>    nn.Sigmoid(),<NewLine>    DeQuantStub(),<NewLine>)<NewLine><NewLine>torch.quantization.fuse_modules(model, [['1', '2', '3'], ['4', '5']], inplace=True)<NewLine><NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model, inplace=True)<NewLine>optimizer = optim.Adam(model.parameters(), lr=1)<NewLine>model = nn.DataParallel(model)<NewLine>model.to(device)<NewLine># print(model)<NewLine><NewLine>criterion = nn.BCELoss()<NewLine><NewLine>for epoch in range(10):<NewLine>    model.train()<NewLine><NewLine>    inputs = torch.rand(2, 3, 28, 28)<NewLine>    labels = torch.FloatTensor([[1,1,1,1,1,0,0,0,0,0], [1,1,1,1,1,0,0,0,0,0]])<NewLine><NewLine>    inputs = inputs.to(device)<NewLine>    labels = labels.to(device)<NewLine>    loss = criterion(model(inputs).view(2, 10), labels)<NewLine>    optimizer.zero_grad()<NewLine>    loss.backward()<NewLine>    optimizer.step()<NewLine><NewLine>    if epoch &gt;= 2:<NewLine>        model.apply(torch.quantization.disable_observer)<NewLine>    if epoch &gt;= 3:<NewLine>        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    quant_model = deepcopy(model.module)<NewLine>    quant_model = torch.quantization.convert(quant_model.eval().cpu(), inplace=False)<NewLine>    with torch.no_grad():<NewLine>        out = quant_model(torch.rand(1, 3, 28, 28))<NewLine>        print(out.view(10).tolist())<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/eleflea,(Eleflea),eleflea,"March 19, 2020,  7:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>we recently fixed a bug in fake_quant: <a href=""https://github.com/pytorch/pytorch/commit/e236e1593468a68e47c5bcafd7272eca01684294"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/commit/e236e1593468a68e47c5bcafd7272eca01684294</a>, are you running with master?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My pytorch version is 1.5.0.dev20200315. This version was built after that commit. I will try latest nightly version later.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have tested again with pytorch 1.5.0, and the error persists. It gets error when I use Dataparallel.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""73720"" data-username=""eleflea""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/e/858c86/40.png"" width=""20""/> eleflea:</div><NewLine><blockquote><NewLine><p>When</p><NewLine></blockquote><NewLine></aside><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> can you take a look?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/eleflea"">@eleflea</a> this seems to be a true error. We are looking into this and will post an update when it is fixed.<br/><NewLine>Do you also see this error if you load pretrained fp32 weights and then run QAT for few iterations (using the same setup)?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/supriyar"">@supriyar</a> I have tried to load a normal trained fp32 model, and continue with QAT. I disable observer after 2 epoches, and freeze bn after 2 more epoches.<br/><NewLine>When I trained with single GPU, the loss was around 13(slightly increase after disabling observer). Everything goes smoothly.<br/><NewLine>When I trained with 2 GPUs, the loss increased to ~1200 after disabling observer. More iterations seems useless.<br/><NewLine>I further printed out the first layer during QAT. I found that quant parameters was not updated(compared to using single GPU). Scale and zero_point stuck at 1 and 0, and observer’s min_val &amp; max_val keep empty.<br/><NewLine>Part of the output is as follows:</p><NewLine><pre><code class=""lang-auto"">Sequential(<NewLine>  (conv): ConvBnReLU2d(<NewLine>    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>    (weight_fake_quant): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>  )<NewLine>  (bn): Identity()<NewLine>  (act): Identity()<NewLine>)<NewLine>lr: 0.000002    epoch: 0/80     step: 13        train_loss: 13.425(xy: 2.847, conf: 6.531, cls: 4.047)   Sequential(<NewLine>  (conv): ConvBnReLU2d(<NewLine>    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>    (weight_fake_quant): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>  )<NewLine>  (bn): Identity()<NewLine>  (act): Identity()<NewLine>)<NewLine>lr: 0.000004    epoch: 0/80     step: 26        train_loss: 13.825(xy: 2.980, conf: 6.424, cls: 4.421)   Sequential(<NewLine>  (conv): ConvBnReLU2d(<NewLine>    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>    (activation_post_process): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>    (weight_fake_quant): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')<NewLine>      (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))<NewLine>    )<NewLine>  )<NewLine>  (bn): Identity()<NewLine>  (act): Identity()<NewLine>)<NewLine>lr: 0.000006    epoch: 0/80     step: 39        train_loss: 13.173(xy: 3.110, conf: 6.385, cls: 3.678)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/eleflea; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/eleflea; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/supriyar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/eleflea; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  9:00pm; <NewLine> REPLY_DATE 2: March 20, 2020, 12:47am; <NewLine> REPLY_DATE 3: March 21, 2020,  1:48am; <NewLine> REPLY_DATE 4: March 27, 2020, 11:23pm; <NewLine> REPLY_DATE 5: March 28, 2020, 12:29am; <NewLine> REPLY_DATE 6: March 29, 2020, 12:24pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
66698,Quantization of Transformer models in Fairseq,2020-01-15T03:32:15.829Z,1,358,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Given that the model loaded from PyTorch hub:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>torch.cuda.is_available()<NewLine>en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')<NewLine>en2de.translate('hello world') # [out]: 'Hallo Welt'<NewLine><NewLine>type(en2de.models[0])   # [out]: fairseq.models.transformer.TransformerModel<NewLine></code></pre><NewLine><p>The quantization seems to be successful:</p><NewLine><pre><code class=""lang-auto"">en2de_q0 = torch.quantization.quantize_dynamic(<NewLine>    en2de.models[0], {torch.nn.Linear}, dtype=torch.qint8<NewLine>)<NewLine><NewLine>type(en2de_q0) # [out]: fairseq.models.transformer.TransformerModel<NewLine></code></pre><NewLine><p>But after trying to overwrite the model, the translate function inside TransformerModel fails:</p><NewLine><pre><code class=""lang-auto"">en2de.models[0] = en2de_q0<NewLine>en2de.translate('hello world')<NewLine></code></pre><NewLine><p>[out]:</p><NewLine><pre><code class=""lang-auto""><NewLine>---------------------------------------------------------------------------<NewLine>TypeError                                 Traceback (most recent call last)<NewLine>&lt;ipython-input-12-adf73479cb54&gt; in &lt;module&gt;<NewLine>----&gt; 1 en2de.translate('hello world')<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py in translate(self, sentences, beam, verbose, **kwargs)<NewLine>    120 <NewLine>    121     def translate(self, sentences: List[str], beam: int = 5, verbose: bool = False, **kwargs) -&gt; List[str]:<NewLine>--&gt; 122         return self.sample(sentences, beam, verbose, **kwargs)<NewLine>    123 <NewLine>    124     def sample(self, sentences: List[str], beam: int = 1, verbose: bool = False, **kwargs) -&gt; List[str]:<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py in sample(self, sentences, beam, verbose, **kwargs)<NewLine>    124     def sample(self, sentences: List[str], beam: int = 1, verbose: bool = False, **kwargs) -&gt; List[str]:<NewLine>    125         if isinstance(sentences, str):<NewLine>--&gt; 126             return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]<NewLine>    127         tokenized_sentences = [self.encode(sentence) for sentence in sentences]<NewLine>    128         batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py in sample(self, sentences, beam, verbose, **kwargs)<NewLine>    126             return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]<NewLine>    127         tokenized_sentences = [self.encode(sentence) for sentence in sentences]<NewLine>--&gt; 128         batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)<NewLine>    129         return [self.decode(hypos[0]['tokens']) for hypos in batched_hypos]<NewLine>    130 <NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py in generate(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, **kwargs)<NewLine>    159         for batch in self._build_batches(tokenized_sentences, skip_invalid_size_inputs):<NewLine>    160             batch = utils.apply_to_sample(lambda t: t.to(self.device), batch)<NewLine>--&gt; 161             translations = self.task.inference_step(generator, self.models, batch)<NewLine>    162             for id, hypos in zip(batch[""id""].tolist(), translations):<NewLine>    163                 results.append((id, hypos))<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py in inference_step(self, generator, models, sample, prefix_tokens)<NewLine>    304     def inference_step(self, generator, models, sample, prefix_tokens=None):<NewLine>    305         with torch.no_grad():<NewLine>--&gt; 306             return generator.generate(models, sample, prefix_tokens=prefix_tokens)<NewLine>    307 <NewLine>    308     def update_step(self, num_updates):<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/autograd/grad_mode.py in decorate_no_grad(*args, **kwargs)<NewLine>     47         def decorate_no_grad(*args, **kwargs):<NewLine>     48             with self:<NewLine>---&gt; 49                 return func(*args, **kwargs)<NewLine>     50         return decorate_no_grad<NewLine>     51 <NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py in generate(self, models, sample, **kwargs)<NewLine>     90         """"""<NewLine>     91         model = EnsembleModel(models)<NewLine>---&gt; 92         return self._generate(model, sample, **kwargs)<NewLine>     93 <NewLine>     94     @torch.no_grad()<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/autograd/grad_mode.py in decorate_no_grad(*args, **kwargs)<NewLine>     47         def decorate_no_grad(*args, **kwargs):<NewLine>     48             with self:<NewLine>---&gt; 49                 return func(*args, **kwargs)<NewLine>     50         return decorate_no_grad<NewLine>     51 <NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py in _generate(self, model, sample, prefix_tokens, bos_token, **kwargs)<NewLine>    130 <NewLine>    131         # compute the encoder output for each beam<NewLine>--&gt; 132         encoder_outs = model.forward_encoder(encoder_input)<NewLine>    133         new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)<NewLine>    134         new_order = new_order.to(src_tokens.device).long()<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/autograd/grad_mode.py in decorate_no_grad(*args, **kwargs)<NewLine>     47         def decorate_no_grad(*args, **kwargs):<NewLine>     48             with self:<NewLine>---&gt; 49                 return func(*args, **kwargs)<NewLine>     50         return decorate_no_grad<NewLine>     51 <NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py in forward_encoder(self, encoder_input)<NewLine>    520         if not self.has_encoder():<NewLine>    521             return None<NewLine>--&gt; 522         return [model.encoder(**encoder_input) for model in self.models]<NewLine>    523 <NewLine>    524     @torch.no_grad()<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py in &lt;listcomp&gt;(.0)<NewLine>    520         if not self.has_encoder():<NewLine>    521             return None<NewLine>--&gt; 522         return [model.encoder(**encoder_input) for model in self.models]<NewLine>    523 <NewLine>    524     @torch.no_grad()<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py in forward(self, src_tokens, src_lengths, cls_input, return_all_hiddens, **unused)<NewLine>    404             dropout_probability = random.uniform(0, 1)<NewLine>    405             if not self.training or (dropout_probability &gt; self.encoder_layerdrop):<NewLine>--&gt; 406                 x = layer(x, encoder_padding_mask)<NewLine>    407                 if return_all_hiddens:<NewLine>    408                     encoder_states.append(x)<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/transformer_layer.py in forward(self, x, encoder_padding_mask, attn_mask)<NewLine>     93         # TODO: to formally solve this problem, we need to change fairseq's<NewLine>     94         # MultiheadAttention. We will do this later on.<NewLine>---&gt; 95         x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)<NewLine>     96         x = F.dropout(x, p=self.dropout, training=self.training)<NewLine>     97         x = residual + x<NewLine><NewLine>/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)<NewLine>    539             result = self._slow_forward(*input, **kwargs)<NewLine>    540         else:<NewLine>--&gt; 541             result = self.forward(*input, **kwargs)<NewLine>    542         for hook in self._forward_hooks.values():<NewLine>    543             hook_result = hook(self, input, result)<NewLine><NewLine>~/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/multihead_attention.py in forward(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)<NewLine>    126                                                   self.embed_dim, self.num_heads,<NewLine>    127                                                   torch.empty([0]),<NewLine>--&gt; 128                                                   torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),<NewLine>    129                                                   self.bias_k, self.bias_v,<NewLine>    130                                                   self.add_zero_attn, self.dropout,<NewLine><NewLine>TypeError: expected Tensor as element 0 in argument 0, but got method<NewLine></code></pre><NewLine><p>Anyone knows how to resolve this? And quantize the model and still make it translate?</p><NewLine></div>",https://discuss.pytorch.org/u/alvations,,alvations,"January 15, 2020,  3:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Loooking at the parameters, they don’t look too different and I don’t see the quantized parameters:</p><NewLine><pre><code class=""lang-auto"">list(en2de_q0.parameters())<NewLine></code></pre><NewLine><p>[out]:</p><NewLine><pre><code class=""lang-auto"">[Parameter containing:<NewLine> tensor([[ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0251],<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [-0.0906,  0.0405, -0.0281,  ..., -0.1006, -0.0363, -0.0373],<NewLine>         ...,<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [ 0.0110,  0.0019,  0.0025,  ...,  0.0171, -0.0069, -0.0248]],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.6650, 2.0582, 2.2051,  ..., 0.3772, 0.4279, 0.7772],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0429,  0.4521, -0.2979,  ...,  0.2960,  0.0998,  0.5742],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.3566, 0.3331, 0.4156,  ..., 0.3027, 0.3934, 0.4597],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0039, -0.0139,  0.0799,  ..., -0.0354,  0.0002, -0.0507],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7108, 0.6487, 0.8303,  ..., 0.5031, 0.6099, 0.8633],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.3873,  0.0271, -0.2536,  ..., -0.1306, -0.0470,  0.1739],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4754, 0.4790, 0.5809,  ..., 0.3982, 0.4633, 0.5181],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0498,  0.0161,  0.0720,  ...,  0.0221,  0.0262, -0.0039],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7605, 0.5692, 0.7406,  ..., 0.6080, 0.7313, 0.9108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2919,  0.0165, -0.1515,  ...,  0.0878,  0.1131, -0.0286],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4510, 0.4098, 0.5094,  ..., 0.4080, 0.4713, 0.5315],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0489, 0.0145, 0.0468,  ..., 0.0118, 0.0073, 0.0143],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7937, 0.6190, 0.7745,  ..., 0.6243, 0.7671, 0.9124],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1167, -0.0649, -0.1412,  ...,  0.2262,  0.1255, -0.0199],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4677, 0.4157, 0.4952,  ..., 0.4077, 0.4837, 0.5452],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0330,  0.0138,  0.0262,  ..., -0.0178, -0.0088,  0.0108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7569, 0.6327, 0.7345,  ..., 0.5937, 0.7591, 0.8758],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0460, -0.0602, -0.1474,  ..., -0.0467,  0.1447, -0.1098],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5098, 0.4489, 0.5239,  ..., 0.4316, 0.5104, 0.5754],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0279, -0.0034,  0.0159,  ..., -0.0088, -0.0531, -0.0025],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6944, 0.6200, 0.6646,  ..., 0.6369, 0.6630, 0.7936],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0425, -0.1008, -0.0308,  ...,  0.0353, -0.0847, -0.1735],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2035, 0.1787, 0.1948,  ..., 0.1657, 0.2053, 0.2193],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0103,  0.0007, -0.0053,  ..., -0.0091, -0.0024,  0.0073],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7927, 0.8630, 0.9708,  ..., 0.9663, 0.8602, 0.8438],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5716,  0.2891,  0.2969,  ...,  0.0658,  0.3705,  0.4201],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9630, 0.8330, 0.8355,  ..., 0.9208, 0.8964, 0.9021],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5408,  0.1112, -0.0177,  ..., -0.1004,  0.0218,  0.1819],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2730, 0.3580, 0.3483,  ..., 0.4263, 0.4327, 0.4214],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0389, -0.0258, -0.0110,  ..., -0.0194, -0.0296, -0.0428],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9382, 0.8844, 0.8857,  ..., 1.0070, 0.9959, 1.0378],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1183, -0.1011,  0.1333,  ..., -0.1546,  0.1757,  0.0622],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9625, 0.8622, 0.9251,  ..., 0.8971, 0.9141, 0.9558],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1533,  0.0882,  0.0403,  ..., -0.0873,  0.1071,  0.1290],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4511, 0.4763, 0.5090,  ..., 0.5148, 0.5195, 0.5114],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0111, -0.0262, -0.0249,  ..., -0.0013, -0.0405, -0.0426],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.0736, 1.0671, 1.0402,  ..., 1.1028, 1.0349, 1.1180],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0826,  0.1648,  0.0594,  ...,  0.0423,  0.0210,  0.1997],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9743, 0.8883, 0.8962,  ..., 0.9011, 0.8924, 0.9486],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0676,  0.1354,  0.0881,  ..., -0.0470,  0.0819,  0.1521],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5064, 0.5148, 0.5453,  ..., 0.5547, 0.5714, 0.5448],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0062, -0.0382, -0.0277,  ..., -0.0148, -0.0280, -0.0436],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1094, 1.1115, 1.0514,  ..., 1.1308, 1.0555, 1.1376],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1110,  0.0899,  0.1120,  ..., -0.1669, -0.0578,  0.0723],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9829, 0.8915, 0.8747,  ..., 0.9197, 0.9132, 0.9313],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2108,  0.0420,  0.0976,  ..., -0.0915,  0.0075, -0.0368],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5855, 0.6197, 0.6315,  ..., 0.6086, 0.6402, 0.6502],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0119, -0.0300, -0.0460,  ...,  0.0003, -0.0203, -0.0224],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1550, 1.1142, 1.1391,  ..., 1.1255, 1.0924, 1.1411],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0424,  0.1162, -0.0843,  ..., -0.0932, -0.0413,  0.1667],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9111, 0.8723, 0.7811,  ..., 0.9060, 0.9096, 0.9097],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2315,  0.0282, -0.2177,  ...,  0.0301, -0.0341, -0.2896],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6610, 0.6736, 0.6347,  ..., 0.6703, 0.6840, 0.7031],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0064, -0.0492, -0.0049,  ..., -0.0257, -0.0278,  0.0006],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1620, 1.0839, 1.1276,  ..., 1.1419, 1.1669, 1.1023],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0491, -0.0879,  0.1410,  ...,  0.0672,  0.0349,  0.2100],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.8549, 0.8141, 0.7377,  ..., 0.7198, 0.7090, 0.7332],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.3034, -0.0049, -0.0414,  ...,  0.2019,  0.0855, -0.2630],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([2.0453, 2.5422, 2.4132,  ..., 0.9301, 0.8366, 0.7796],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0008, 0.0556, 0.1556,  ..., 0.0142, 0.0115, 0.0733],<NewLine>        requires_grad=True)]<NewLine></code></pre><NewLine><p>And the original model’s parameter:</p><NewLine><pre><code class=""lang-auto"">list(en2de.models[0].parameters())<NewLine></code></pre><NewLine><p>[out]:</p><NewLine><pre><code class=""lang-auto"">        requires_grad=True), Parameter containing:<NewLine> tensor([-0.3873,  0.0271, -0.2536,  ..., -0.1306, -0.0470,  0.1739],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4754, 0.4790, 0.5809,  ..., 0.3982, 0.4633, 0.5181],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0498,  0.0161,  0.0720,  ...,  0.0221,  0.0262, -0.0039],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7605, 0.5692, 0.7406,  ..., 0.6080, 0.7313, 0.9108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2919,  0.0165, -0.1515,  ...,  0.0878,  0.1131, -0.0286],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4510, 0.4098, 0.5094,  ..., 0.4080, 0.4713, 0.5315],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0489, 0.0145, 0.0468,  ..., 0.0118, 0.0073, 0.0143],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7937, 0.6190, 0.7745,  ..., 0.6243, 0.7671, 0.9124],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1167, -0.0649, -0.1412,  ...,  0.2262,  0.1255, -0.0199],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4677, 0.4157, 0.4952,  ..., 0.4077, 0.4837, 0.5452],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0330,  0.0138,  0.0262,  ..., -0.0178, -0.0088,  0.0108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7569, 0.6327, 0.7345,  ..., 0.5937, 0.7591, 0.8758],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0460, -0.0602, -0.1474,  ..., -0.0467,  0.1447, -0.1098],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5098, 0.4489, 0.5239,  ..., 0.4316, 0.5104, 0.5754],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0279, -0.0034,  0.0159,  ..., -0.0088, -0.0531, -0.0025],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6944, 0.6200, 0.6646,  ..., 0.6369, 0.6630, 0.7936],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0425, -0.1008, -0.0308,  ...,  0.0353, -0.0847, -0.1735],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2035, 0.1787, 0.1948,  ..., 0.1657, 0.2053, 0.2193],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0103,  0.0007, -0.0053,  ..., -0.0091, -0.0024,  0.0073],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7927, 0.8630, 0.9708,  ..., 0.9663, 0.8602, 0.8438],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5716,  0.2891,  0.2969,  ...,  0.0658,  0.3705,  0.4201],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9630, 0.8330, 0.8355,  ..., 0.9208, 0.8964, 0.9021],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5408,  0.1112, -0.0177,  ..., -0.1004,  0.0218,  0.1819],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2730, 0.3580, 0.3483,  ..., 0.4263, 0.4327, 0.4214],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0389, -0.0258, -0.0110,  ..., -0.0194, -0.0296, -0.0428],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9382, 0.8844, 0.8857,  ..., 1.0070, 0.9959, 1.0378],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1183, -0.1011,  0.1333,  ..., -0.1546,  0.1757,  0.0622],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9625, 0.8622, 0.9251,  ..., 0.8971, 0.9141, 0.9558],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1533,  0.0882,  0.0403,  ..., -0.0873,  0.1071,  0.1290],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4511, 0.4763, 0.5090,  ..., 0.5148, 0.5195, 0.5114],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0111, -0.0262, -0.0249,  ..., -0.0013, -0.0405, -0.0426],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.0736, 1.0671, 1.0402,  ..., 1.1028, 1.0349, 1.1180],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0826,  0.1648,  0.0594,  ...,  0.0423,  0.0210,  0.1997],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9743, 0.8883, 0.8962,  ..., 0.9011, 0.8924, 0.9486],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0676,  0.1354,  0.0881,  ..., -0.0470,  0.0819,  0.1521],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5064, 0.5148, 0.5453,  ..., 0.5547, 0.5714, 0.5448],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0062, -0.0382, -0.0277,  ..., -0.0148, -0.0280, -0.0436],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1094, 1.1115, 1.0514,  ..., 1.1308, 1.0555, 1.1376],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1110,  0.0899,  0.1120,  ..., -0.1669, -0.0578,  0.0723],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9829, 0.8915, 0.8747,  ..., 0.9197, 0.9132, 0.9313],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2108,  0.0420,  0.0976,  ..., -0.0915,  0.0075, -0.0368],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5855, 0.6197, 0.6315,  ..., 0.6086, 0.6402, 0.6502],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0119, -0.0300, -0.0460,  ...,  0.0003, -0.0203, -0.0224],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1550, 1.1142, 1.1391,  ..., 1.1255, 1.0924, 1.1411],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0424,  0.1162, -0.0843,  ..., -0.0932, -0.0413,  0.1667],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9111, 0.8723, 0.7811,  ..., 0.9060, 0.9096, 0.9097],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2315,  0.0282, -0.2177,  ...,  0.0301, -0.0341, -0.2896],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6610, 0.6736, 0.6347,  ..., 0.6703, 0.6840, 0.7031],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0064, -0.0492, -0.0049,  ..., -0.0257, -0.0278,  0.0006],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1620, 1.0839, 1.1276,  ..., 1.1419, 1.1669, 1.1023],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0491, -0.0879,  0.1410,  ...,  0.0672,  0.0349,  0.2100],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.8549, 0.8141, 0.7377,  ..., 0.7198, 0.7090, 0.7332],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.3034, -0.0049, -0.0414,  ...,  0.2019,  0.0855, -0.2630],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([2.0453, 2.5422, 2.4132,  ..., 0.9301, 0.8366, 0.7796],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0008, 0.0556, 0.1556,  ..., 0.0142, 0.0115, 0.0733],<NewLine>        requires_grad=True)]<NewLine>In [15]:<NewLine>list(en2de.models[0].parameters())<NewLine>list(en2de.models[0].parameters())<NewLine>Out[15]:<NewLine>[Parameter containing:<NewLine> tensor([[ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0251],<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [-0.0906,  0.0405, -0.0281,  ..., -0.1006, -0.0363, -0.0373],<NewLine>         ...,<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [ 0.0109,  0.0018,  0.0024,  ...,  0.0170, -0.0071, -0.0250],<NewLine>         [ 0.0110,  0.0019,  0.0025,  ...,  0.0171, -0.0069, -0.0248]],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.6650, 2.0582, 2.2051,  ..., 0.3772, 0.4279, 0.7772],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0429,  0.4521, -0.2979,  ...,  0.2960,  0.0998,  0.5742],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.3566, 0.3331, 0.4156,  ..., 0.3027, 0.3934, 0.4597],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0039, -0.0139,  0.0799,  ..., -0.0354,  0.0002, -0.0507],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7108, 0.6487, 0.8303,  ..., 0.5031, 0.6099, 0.8633],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.3873,  0.0271, -0.2536,  ..., -0.1306, -0.0470,  0.1739],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4754, 0.4790, 0.5809,  ..., 0.3982, 0.4633, 0.5181],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0498,  0.0161,  0.0720,  ...,  0.0221,  0.0262, -0.0039],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7605, 0.5692, 0.7406,  ..., 0.6080, 0.7313, 0.9108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2919,  0.0165, -0.1515,  ...,  0.0878,  0.1131, -0.0286],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4510, 0.4098, 0.5094,  ..., 0.4080, 0.4713, 0.5315],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0489, 0.0145, 0.0468,  ..., 0.0118, 0.0073, 0.0143],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7937, 0.6190, 0.7745,  ..., 0.6243, 0.7671, 0.9124],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1167, -0.0649, -0.1412,  ...,  0.2262,  0.1255, -0.0199],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4677, 0.4157, 0.4952,  ..., 0.4077, 0.4837, 0.5452],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0330,  0.0138,  0.0262,  ..., -0.0178, -0.0088,  0.0108],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7569, 0.6327, 0.7345,  ..., 0.5937, 0.7591, 0.8758],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0460, -0.0602, -0.1474,  ..., -0.0467,  0.1447, -0.1098],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5098, 0.4489, 0.5239,  ..., 0.4316, 0.5104, 0.5754],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0279, -0.0034,  0.0159,  ..., -0.0088, -0.0531, -0.0025],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6944, 0.6200, 0.6646,  ..., 0.6369, 0.6630, 0.7936],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0425, -0.1008, -0.0308,  ...,  0.0353, -0.0847, -0.1735],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2035, 0.1787, 0.1948,  ..., 0.1657, 0.2053, 0.2193],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0103,  0.0007, -0.0053,  ..., -0.0091, -0.0024,  0.0073],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.7927, 0.8630, 0.9708,  ..., 0.9663, 0.8602, 0.8438],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5716,  0.2891,  0.2969,  ...,  0.0658,  0.3705,  0.4201],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9630, 0.8330, 0.8355,  ..., 0.9208, 0.8964, 0.9021],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.5408,  0.1112, -0.0177,  ..., -0.1004,  0.0218,  0.1819],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.2730, 0.3580, 0.3483,  ..., 0.4263, 0.4327, 0.4214],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0389, -0.0258, -0.0110,  ..., -0.0194, -0.0296, -0.0428],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9382, 0.8844, 0.8857,  ..., 1.0070, 0.9959, 1.0378],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1183, -0.1011,  0.1333,  ..., -0.1546,  0.1757,  0.0622],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9625, 0.8622, 0.9251,  ..., 0.8971, 0.9141, 0.9558],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1533,  0.0882,  0.0403,  ..., -0.0873,  0.1071,  0.1290],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.4511, 0.4763, 0.5090,  ..., 0.5148, 0.5195, 0.5114],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0111, -0.0262, -0.0249,  ..., -0.0013, -0.0405, -0.0426],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.0736, 1.0671, 1.0402,  ..., 1.1028, 1.0349, 1.1180],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0826,  0.1648,  0.0594,  ...,  0.0423,  0.0210,  0.1997],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9743, 0.8883, 0.8962,  ..., 0.9011, 0.8924, 0.9486],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0676,  0.1354,  0.0881,  ..., -0.0470,  0.0819,  0.1521],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5064, 0.5148, 0.5453,  ..., 0.5547, 0.5714, 0.5448],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0062, -0.0382, -0.0277,  ..., -0.0148, -0.0280, -0.0436],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1094, 1.1115, 1.0514,  ..., 1.1308, 1.0555, 1.1376],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.1110,  0.0899,  0.1120,  ..., -0.1669, -0.0578,  0.0723],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9829, 0.8915, 0.8747,  ..., 0.9197, 0.9132, 0.9313],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2108,  0.0420,  0.0976,  ..., -0.0915,  0.0075, -0.0368],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.5855, 0.6197, 0.6315,  ..., 0.6086, 0.6402, 0.6502],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0119, -0.0300, -0.0460,  ...,  0.0003, -0.0203, -0.0224],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1550, 1.1142, 1.1391,  ..., 1.1255, 1.0924, 1.1411],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0424,  0.1162, -0.0843,  ..., -0.0932, -0.0413,  0.1667],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.9111, 0.8723, 0.7811,  ..., 0.9060, 0.9096, 0.9097],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.2315,  0.0282, -0.2177,  ...,  0.0301, -0.0341, -0.2896],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.6610, 0.6736, 0.6347,  ..., 0.6703, 0.6840, 0.7031],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([-0.0064, -0.0492, -0.0049,  ..., -0.0257, -0.0278,  0.0006],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([1.1620, 1.0839, 1.1276,  ..., 1.1419, 1.1669, 1.1023],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.0491, -0.0879,  0.1410,  ...,  0.0672,  0.0349,  0.2100],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.8549, 0.8141, 0.7377,  ..., 0.7198, 0.7090, 0.7332],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([ 0.3034, -0.0049, -0.0414,  ...,  0.2019,  0.0855, -0.2630],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([2.0453, 2.5422, 2.4132,  ..., 0.9301, 0.8366, 0.7796],<NewLine>        requires_grad=True), Parameter containing:<NewLine> tensor([0.0008, 0.0556, 0.1556,  ..., 0.0142, 0.0115, 0.0733],<NewLine>        requires_grad=True)]<NewLine><NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>put quantize_dynamic in fairseq-generate’s code and you will observe the change.<br/><NewLine>ref :<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/fairseq/issues/1901"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/fairseq</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/fairseq/issues/1901"" rel=""nofollow noopener"" target=""_blank"">Does Dynamic Quantization speed up Fairseq's Transfomer?</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2020-03-24"" data-format=""ll"" data-time=""12:17:39"" data-timezone=""UTC"">12:17PM - 24 Mar 20 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/gvskalyan"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""gvskalyan"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/42022935?v=4"" width=""20""/><NewLine>          gvskalyan<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">What is your question?<NewLine>As per this tutorial in torch, quantize_dynamic gives speed up of models (though it supports Linear and LSTM...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">question</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/alvations; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/gvskalyan; <NewLine> ,"REPLY_DATE 1: January 15, 2020,  3:34am; <NewLine> REPLY_DATE 2: March 26, 2020,  4:28am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
68016,Quantization support for 1D convolutions?,2020-01-29T12:31:07.735Z,1,179,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi!</p><NewLine><p>Are you planning to add support for 1D convolutions for <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">quantization</a>?</p><NewLine></div>",https://discuss.pytorch.org/u/snakers41,(Alexander),snakers41,"January 29, 2020, 12:31pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> does fbgemm support 1D convolution?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>1D can go though 2D path but we need some work here to make it work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a href=""https://github.com/pytorch/pytorch/pull/35093"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/35093</a> adds 1d qconvs.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: February 14, 2020, 10:22pm; <NewLine> REPLY_DATE 2: February 18, 2020,  7:20pm; <NewLine> REPLY_DATE 3: March 23, 2020, 10:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> 
64868,How to write scale and zero_point to fp32 tensor without doing quantization?,2019-12-24T05:57:32.602Z,3,289,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In PyTorch1.3, float-tensor has scale and zero_point. Could I set value to scale and zero_point to a float32 tensor without coverting fp32 tensor to quant?</p><NewLine></div>",https://discuss.pytorch.org/u/alanzhai219,(Alan Zhai),alanzhai219,"December 24, 2019,  6:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>May I know the reason for doing this?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>In PyTorch1.3, float-tensor has scale and zero_point. Could I set value to scale and zero_point to a float32 tensor without coverting fp32 tensor to quant?</p><NewLine></blockquote><NewLine><p><code>q_scale</code> and <code>q_zero_point</code> exist in FP tensor, but they do not make sense outside of the quantization context. That’s why, if you try accessing those methods on a FP tensor, you are supposed to get an error:</p><NewLine><pre><code class=""lang-python"">&gt;&gt;&gt; import torch<NewLine>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])<NewLine>&gt;&gt;&gt; x.q_scale()<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-13-a850e3e1a3e9&gt; in &lt;module&gt;<NewLine>----&gt; 1 x.q_scale()<NewLine><NewLine>RuntimeError: Could not run 'aten::q_scale' with arguments from the 'CPUTensorId' backend. 'aten::q_scale' is only available for these backends: [QuantizedCPUTensorId, VariableTensorId].<NewLine><NewLine>&gt;&gt;&gt; x.q_zero_point()<NewLine>---------------------------------------------------------------------------<NewLine>RuntimeError                              Traceback (most recent call last)<NewLine>&lt;ipython-input-15-6cbfbea33d3d&gt; in &lt;module&gt;<NewLine>----&gt; 1 x.q_zero_point()<NewLine><NewLine>RuntimeError: Could not run 'aten::q_zero_point' with arguments from the 'CPUTensorId' backend. 'aten::q_zero_point' is only available for these backends: [QuantizedCPUTensorId, VariableTensorId].<NewLine><NewLine></code></pre><NewLine><p>As <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> asked, is there a specific reason you would want to do that? Maybe there is another way of accomplish what you are trying to do</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your reply. It is because our operation quantization compiler has too much rules.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>I don’t think I can suggest an alternative, as I don’t know what rules you are referring to. Generally, to answer your question, we don’t allow setting scale and zero_point for a float tensor, and frankly can’t even imagine a use case <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/>.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""64868"" data-username=""Zafar""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zafar/40/11532_2.png"" width=""20""/> Zafar:</div><NewLine><blockquote><NewLine><p><code>q_scale</code> and <code>q_zero_point</code> exist in FP tensor, but they do not make sense outside of the quantization context. That’s why, if you try accessing those methods on a FP tensor, you are supposed to get an error:</p><NewLine><pre><code class=""lang-auto""><NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>are you saying you want to have floating point zero_point? I think we’ll have quantizer support for that</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/alanzhai219; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: January 13, 2020,  7:49pm; <NewLine> REPLY_DATE 2: March 20, 2020,  2:29am; <NewLine> REPLY_DATE 3: January 14, 2020,  3:54am; <NewLine> REPLY_DATE 4: March 20, 2020,  2:29am; <NewLine> REPLY_DATE 5: March 19, 2020,  9:03pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
73737,RuntimeError: Didn&rsquo;t find engine for operation quantized::conv2d_prepack NoQEngine (operator() at /pytorch/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp:63),2020-03-19T09:26:28.994Z,0,303,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to run tutorial <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> and faced this problem.</p><NewLine><p>I ran at a supercomputer,cuda 10.1,pytorch 1.4,torchvision 0.5.</p><NewLine><p>total information is</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “q.py”, line 327, in <br/><NewLine>torch.quantization.convert(myModel,inplace=True)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/quantization/quantize.py”, line 316, in convert<br/><NewLine>convert(mod, mapping, inplace=True)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/quantization/quantize.py”, line 316, in convert<br/><NewLine>convert(mod, mapping, inplace=True)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/quantization/quantize.py”, line 317, in convert<br/><NewLine>reassign[name] = swap_module(mod, mapping)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/quantization/quantize.py”, line 339, in swap_module<br/><NewLine>new_mod = mapping[type(mod)].from_float(mod)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py”, line 49, in from_float<br/><NewLine>return super(ConvReLU2d, cls).from_float(mod)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py”, line 257, in from_float<br/><NewLine>mod.bias is not None, mod.padding_mode)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py”, line 29, in <strong>init</strong><br/><NewLine>padding_mode=padding_mode)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py”, line 187, in <strong>init</strong><br/><NewLine>groups, bias, padding_mode)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py”, line 48, in <strong>init</strong><br/><NewLine>self.set_weight_bias(qweight, bias_float)<br/><NewLine>File “/SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py”, line 195, in set_weight_bias<br/><NewLine>w, b, self.stride, self.padding, self.dilation, self.groups)<br/><NewLine>RuntimeError: Didn’t find engine for operation quantized::conv2d_prepack NoQEngine (operator() at /pytorch/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp:63)<br/><NewLine>frame <span class=""hashtag"">#0:</span> c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f414c66d193 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libc10.so)<br/><NewLine>frame <span class=""hashtag"">#1:</span>  + 0x1e5c2ba (0x7f414e91c2ba in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#2:</span>  + 0x1e5d083 (0x7f414e91d083 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#3:</span>  + 0x33eb009 (0x7f414feab009 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#4:</span>  + 0x40e8547 (0x7f4150ba8547 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch.so)<br/><NewLine>frame <span class=""hashtag"">#5:</span>  + 0x6de077 (0x7f4197a4e077 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#6:</span>  + 0x6a95c4 (0x7f4197a195c4 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine>frame <span class=""hashtag"">#7:</span>  + 0x2961c4 (0x7f41976061c4 in /SISDC_GPFS/Home_SE/songxy-jnu/hejy-jnu/.conda/envs/mjx/lib/python3.6/site-packages/torch/lib/libtorch_python.so)<br/><NewLine><br/><NewLine>frame <span class=""hashtag"">#61:</span> __libc_start_main + 0xf5 (0x7f41abb49445 in /lib64/libc.so.6)</p><NewLine></div>",https://discuss.pytorch.org/u/JachinMa,,JachinMa,"March 19, 2020,  9:26am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This means you didn’t compile fbgemm, can you print <code>torch.backends.quantized.supported_engines</code>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  8:59pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
72149,How to quantize only specific layers,2020-03-05T14:14:10.592Z,0,144,"<div class=""post"" itemprop=""articleBody""><NewLine><p>How to quantize weights in pointwise convolution layer in MobileNet V2, not all the entire model’s parameters. I searched a lot, but most of the source code quantized the entire model’s parameters.</p><NewLine></div>",https://discuss.pytorch.org/u/Suyoung_Park,(Suyoung Park),Suyoung_Park,"March 5, 2020,  2:14pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>you can set qconfig only for the sub module that you want to quantize. e.g.</p><NewLine><pre><code class=""lang-auto"">class M(torch.nn.Module):<NewLine>   def __init__(self):<NewLine>      self.conv1 = ...<NewLine>      self.conv2 = ...<NewLine>m  = M()<NewLine>m.conv1 = qconfig # if you only want to quantize conv1<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: March 19, 2020,  8:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
71579,"I try to run quantizitzed model ,but some error happend",2020-03-01T02:27:56.955Z,8,405,"<div class=""post"" itemprop=""articleBody""><NewLine><p>as title<br/><NewLine>the crash message is as below:<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/guanx/pycharmProject/erfnet/venv/erfnet_pytorch-lidarseg/eval/eval_lidar_seg.py”, line 435, in<br/><NewLine>evaluate(parser.parse_args())<br/><NewLine>File “/home/guanx/pycharmProject/erfnet/venv/erfnet_pytorch-lidarseg/eval/eval_lidar_seg.py”, line 292, in evaluate<br/><NewLine>outputs = model(inputs)<br/><NewLine>File “/home/guanx/anaconda3/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py”, line 532, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>RuntimeError: Could not run ‘aten::max_pool2d_with_indices’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::max_pool2d_with_indices’ is only available for these backends: [CPUTensorId, VariableTensorId].<br/><NewLine>The above operation failed in interpreter.<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 63<br/><NewLine>dilation: List[int],<br/><NewLine>ceil_mode: bool):<br/><NewLine>output, indices = torch.max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode)</p><NewLine><pre><code class=""lang-auto"">def backward(grad_output):<NewLine>grad_self = torch.max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)<NewLine><NewLine>The above operation failed in interpreter.</code></pre><NewLine></div>",https://discuss.pytorch.org/u/sptoyoursoul,(guanxiang),sptoyoursoul,"March 1, 2020,  2:27am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, can you show how the model and forward looks like?</p><NewLine><p>Also is it already converted quantized model after calling</p><NewLine><blockquote><NewLine><p>torch.quantization.convert()</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, it looks like you are trying to use backward with quantized operators, which is not supported. Quantization works only for forward methods right now.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi<br/><NewLine>Quantiziton is used to inference in my code . This error happened ,When I sent data to net</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you run</p><NewLine><blockquote><NewLine><p>net.eval()</p><NewLine></blockquote><NewLine><p>before quantization?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""71579"" data-username=""sptoyoursoul""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/d9b06d/40.png"" width=""20""/> sptoyoursoul:</div><NewLine><blockquote><NewLine><p>max_pool2d_with_indices</p><NewLine></blockquote><NewLine></aside><NewLine><p>I don’t think there is a quantized kernel for this op</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><h1>ERFNET full network definition for Pytorch</h1><NewLine><h1>Sept 2017</h1><NewLine><h1>Eduardo Romera</h1><NewLine><p>#######################</p><NewLine><p>import torch<br/><NewLine>import torch.nn as nn<br/><NewLine>import torch.nn.init as init<br/><NewLine>import torch.nn.functional as F<br/><NewLine>import numpy as np</p><NewLine><p>from torch.quantization import QuantStub, DeQuantStub<br/><NewLine>import os</p><NewLine><p>class DownsamplerBlock (nn.Module):<br/><NewLine>def <strong>init</strong>(self, ninput, noutput, caller_name=’’):<br/><NewLine>super().<strong>init</strong>()</p><NewLine><pre><code>    self.conv = nn.Conv2d(ninput, noutput - ninput, (3, 3), stride=2, padding=1, bias=True)<NewLine><NewLine>    self.pool = nn.MaxPool2d(2, stride=2)<NewLine>    self.caller_name = caller_name<NewLine><NewLine>def forward(self, input):<NewLine><NewLine>    output = self.conv(input)<NewLine>    conv_output = F.relu(output)<NewLine>    output = self.pool(input)<NewLine>    pool_output = F.relu(output)<NewLine>    output = torch.cat([conv_output, pool_output], 1)<NewLine><NewLine>    return output<NewLine></code></pre><NewLine><p>class non_bottleneck_1d (nn.Module):<br/><NewLine>def <strong>init</strong>(self, chann, dropprob, dilated, caller_name=’’):<br/><NewLine>super().<strong>init</strong>()</p><NewLine><pre><code>    self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)<NewLine><NewLine><NewLine>    self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)<NewLine><NewLine>    self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1 * dilated, 0), bias=True,<NewLine>                               dilation=(dilated, 1))<NewLine><NewLine>    self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1 * dilated), bias=True,<NewLine>                               dilation=(1, dilated))<NewLine><NewLine>    self.caller_name = caller_name<NewLine><NewLine><NewLine>def hook(self, conv, input, out):<NewLine>    print(""in hook"")<NewLine><NewLine>def forward(self, input):<NewLine><NewLine>    output = self.conv3x1_1(input)<NewLine><NewLine>    output = F.relu(output)<NewLine><NewLine>    output = self.conv1x3_1(output)<NewLine><NewLine>    output = F.relu(output)<NewLine><NewLine><NewLine>    output = self.conv3x1_2(output)<NewLine>    output = F.relu(output)<NewLine><NewLine>    output = self.conv1x3_2(output)<NewLine><NewLine>    output = output + input<NewLine><NewLine><NewLine>    return F.relu(output)    # +input = identity (residual connection)<NewLine></code></pre><NewLine><p>class Encoder(nn.Module):<br/><NewLine>def <strong>init</strong>(self, num_classes):<br/><NewLine>super().<strong>init</strong>()<br/><NewLine>self.initial_block = DownsamplerBlock(4, 16)</p><NewLine><pre><code>    self.layers = nn.ModuleList()<NewLine><NewLine>    self.layers.append(DownsamplerBlock(16, 64))<NewLine><NewLine>    for x in range(0, 5):  # 5 times<NewLine>        self.layers.append(non_bottleneck_1d(64, 0.03, 1))<NewLine><NewLine>    self.layers.append(DownsamplerBlock(64, 128))<NewLine><NewLine>    for x in range(0, 2):  # 2 times<NewLine>        self.layers.append(non_bottleneck_1d(128, 0.3, 2))<NewLine>        self.layers.append(non_bottleneck_1d(128, 0.3, 4))<NewLine>        self.layers.append(non_bottleneck_1d(128, 0.3, 8))<NewLine><NewLine>    # only for encoder mode:<NewLine>    self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)<NewLine><NewLine>def forward(self, input, predict=torch.tensor(0)):<NewLine>    output = self.initial_block(input)<NewLine><NewLine>    for layer in self.layers:<NewLine>        output = layer(output)<NewLine><NewLine>    if predict:<NewLine>        output = self.output_conv(output)<NewLine><NewLine>    return output<NewLine></code></pre><NewLine><p>class UpsamplerBlock(nn.Module):<br/><NewLine>def <strong>init</strong>(self, ninput, noutput, caller_name=’’):<br/><NewLine>super().<strong>init</strong>()<br/><NewLine>self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)<br/><NewLine>self.caller_name = caller_name</p><NewLine><pre><code>def forward(self, input):<NewLine>    output = self.conv(input)<NewLine><NewLine>    return F.relu(output)<NewLine></code></pre><NewLine><p>class Decoder(nn.Module):<br/><NewLine>def <strong>init</strong>(self, num_classes):<br/><NewLine>super().<strong>init</strong>()</p><NewLine><pre><code>    self.layers = nn.ModuleList()<NewLine><NewLine>    self.layers.append(UpsamplerBlock(128, 64))<NewLine>    self.layers.append(non_bottleneck_1d(64, 0, 1))<NewLine>    self.layers.append(non_bottleneck_1d(64, 0, 1))<NewLine><NewLine>    self.layers.append(UpsamplerBlock(64, 16))<NewLine>    self.layers.append(non_bottleneck_1d(16, 0, 1))<NewLine>    self.layers.append(non_bottleneck_1d(16, 0, 1))<NewLine><NewLine>    self.output_conv = nn.ConvTranspose2d(16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)<NewLine><NewLine>def forward(self, input):<NewLine>    output = input<NewLine><NewLine>    for layer in self.layers:<NewLine>        output = layer(output)<NewLine><NewLine>    output = self.output_conv(output)<NewLine><NewLine>    return output<NewLine></code></pre><NewLine><h1>ERFNet</h1><NewLine><p>class ERFNet(nn.Module):<br/><NewLine>def <strong>init</strong>(self, num_classes, encoder=None):  # use encoder to pass pretrained encoder<br/><NewLine>super().<strong>init</strong>()<br/><NewLine>if encoder == None:<br/><NewLine>self.encoder = Encoder(num_classes)<br/><NewLine>else:<br/><NewLine>self.encoder = encoder<br/><NewLine>self.decoder = Decoder(num_classes)<br/><NewLine>self.quant = QuantStub()<br/><NewLine>self.dequant = DeQuantStub()</p><NewLine><pre><code>def forward(self, input, only_encode=torch.tensor(0)):<NewLine>    input = self.quant(input)<NewLine>    if only_encode:<NewLine>        x = self.encoder.forward(input, predict=torch.tensor(1))<NewLine>        x = self.dequant(x)<NewLine>        return (x)<NewLine>    else:<NewLine>        output = self.encoder(input)    # predict=False by default<NewLine>        x = self.decoder.forward(output)<NewLine>        x = self.dequant(x)<NewLine>        return (x)<NewLine></code></pre><NewLine><p>if <strong>name</strong> == ‘<strong>main</strong>’:<br/><NewLine>print(os.getcwd())<br/><NewLine>print(cur_quant_dir)</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>there is a maxpool in may code . It seems that pytorch could not handle maxpool quantiztion.how could I do</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>It’s hard to read the unformatted code. Do you think it wold be possible for you to format the code? Also, how are you quantizing? Are you going through the prepare/convert flow?</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>I do not know how to format the code. I capture the code as picture<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/91c3daa47440a2257659dc08a0d0634d5275e4b7"" href=""https://discuss.pytorch.org/uploads/default/original/3X/9/1/91c3daa47440a2257659dc08a0d0634d5275e4b7.png"" title=""image""><img alt=""image"" data-base62-sha1=""kNuQC811zfBpXuJVNLFPjZNcPDF"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/9/1/91c3daa47440a2257659dc08a0d0634d5275e4b7_2_10x10.png"" height=""357"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/1/91c3daa47440a2257659dc08a0d0634d5275e4b7.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">779×404 8.49 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>I capture the code as picture as below<div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/35119414471d1309a1ff0407b223398a4d7b01fb"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/5/35119414471d1309a1ff0407b223398a4d7b01fb.png"" title=""1""><img alt=""1"" data-base62-sha1=""7zsXZhEbailBomtnSlPw1221HxV"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/5/35119414471d1309a1ff0407b223398a4d7b01fb_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/5/35119414471d1309a1ff0407b223398a4d7b01fb.png"" width=""587""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">1</span><span class=""informations"">792×674 13.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/70ce9cc445514f0af041ee5c8a0878ca41a30e95"" href=""https://discuss.pytorch.org/uploads/default/original/3X/7/0/70ce9cc445514f0af041ee5c8a0878ca41a30e95.png"" title=""2""><img alt=""2"" data-base62-sha1=""g5W8Mvr6qBXXYUuk2AcHCXtaUDz"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/7/0/70ce9cc445514f0af041ee5c8a0878ca41a30e95_2_10x10.png"" height=""499"" src=""https://discuss.pytorch.org/uploads/default/original/3X/7/0/70ce9cc445514f0af041ee5c8a0878ca41a30e95.png"" width=""515""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">2</span><span class=""informations"">900×873 18 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/5837e8d21d704a44f50b311d393ec74cf8d094c3"" href=""https://discuss.pytorch.org/uploads/default/original/3X/5/8/5837e8d21d704a44f50b311d393ec74cf8d094c3.png"" title=""3""><img alt=""3"" data-base62-sha1=""cApNMbflKFK9SF4YBKMwS4aRv1h"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/5/8/5837e8d21d704a44f50b311d393ec74cf8d094c3_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/8/5837e8d21d704a44f50b311d393ec74cf8d094c3.png"" width=""516""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">3</span><span class=""informations"">702×680 14.1 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/d116b8189bf9b8a59b693f5238d06b6834dc0c2c"" href=""https://discuss.pytorch.org/uploads/default/original/3X/d/1/d116b8189bf9b8a59b693f5238d06b6834dc0c2c.png"" title=""4""><img alt=""4"" data-base62-sha1=""tPGsWtEkLI31uMjigQtqsLTpPeQ"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/d/1/d116b8189bf9b8a59b693f5238d06b6834dc0c2c_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/1/d116b8189bf9b8a59b693f5238d06b6834dc0c2c.png"" width=""573""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">4</span><span class=""informations"">905×789 16.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div> <div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/19cbd94f301bfa888c1474ca434e3f3f78b36455"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/9/19cbd94f301bfa888c1474ca434e3f3f78b36455.png"" title=""5""><img alt=""5"" data-base62-sha1=""3GcGdwU0ibSjWY2PQM16Smmpovr"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/9/19cbd94f301bfa888c1474ca434e3f3f78b36455_2_10x10.png"" height=""437"" src=""https://discuss.pytorch.org/uploads/default/original/3X/1/9/19cbd94f301bfa888c1474ca434e3f3f78b36455.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">5</span><span class=""informations"">800×507 11 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>; <NewLine> REPLY 11: <div class=""post"" itemprop=""articleBody""><NewLine><p>pytorch does support quantization for maxpool, you don’t need to change anything and it should just work when you pass a quantized Tensor to maxpool because the output quantization parametere for quantized maxpool can be inferred from input Tensors.</p><NewLine></div>; <NewLine> REPLY 12: <div class=""post"" itemprop=""articleBody""><NewLine><p>hi jerry<br/><NewLine>thanks for you replay<br/><NewLine>I do not add any QuantStub, DeQuantStub in my new code. but it does not work still. and the error information is here <a href=""https://github.com/pytorch/pytorch/issues/34583%E3%80%82I"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/34583。I</a> also use Intel‘s distiller to quantize my mode, the quantiztion information is as below：<br/><NewLine>but when I change the qconfig from “torch.quantization.default_qconfig”  to “torch.quantization.get_default_qconfig(‘fbgemm’)”. It seems that the code hang on somewhere.<br/><NewLine>I also use Intel’s distiller to quntize my mode. I found some information as below:<br/><NewLine>encoder.layers.1.conv1x3_2.output_zero_point: 0.0<br/><NewLine>encoder.layers.2.conv3x1_1.output_scale: .inf<br/><NewLine>encoder.layers.2.conv3x1_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.2.conv1x3_1.output_scale: .inf<br/><NewLine>encoder.layers.2.conv1x3_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.2.conv3x1_2.output_scale: .inf<br/><NewLine>encoder.layers.2.conv3x1_2.output_zero_point: 0.0<br/><NewLine>encoder.layers.2.conv1x3_2.output_scale: 808.11376953125<br/><NewLine>encoder.layers.2.conv1x3_2.output_zero_point: 0.0<br/><NewLine>encoder.layers.3.conv3x1_1.output_scale: .inf<br/><NewLine>encoder.layers.3.conv3x1_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.3.conv1x3_1.output_scale: .inf<br/><NewLine>encoder.layers.3.conv1x3_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.3.conv3x1_2.output_scale: .inf<br/><NewLine>encoder.layers.3.conv3x1_2.output_zero_point: 0.0<br/><NewLine>encoder.layers.3.conv1x3_2.output_scale: 819.0990600585938<br/><NewLine>encoder.layers.3.conv1x3_2.output_zero_point: 0.0<br/><NewLine>encoder.layers.4.conv3x1_1.output_scale: .inf<br/><NewLine>encoder.layers.4.conv3x1_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.4.conv1x3_1.output_scale: .inf<br/><NewLine>encoder.layers.4.conv1x3_1.output_zero_point: 0.0<br/><NewLine>encoder.layers.4.conv3x1_2.output_scale: .inf<br/><NewLine>encoder.layers.4.conv3x1_2.output_zero_point: 0.0<br/><NewLine>it seems that some weight is very small</p><NewLine></div>; <NewLine> REPLY 13: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""13"" data-topic=""71579"" data-username=""sptoyoursoul""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/d9b06d/40.png"" width=""20""/> sptoyoursoul:</div><NewLine><blockquote><NewLine><p>but when I change the qconfig from “torch.quantization.default_qconfig” to “torch.quantization.get_default_qconfig(‘fbgemm’)”. It seems that the code hang on somewhere.</p><NewLine></blockquote><NewLine></aside><NewLine><p>you’ll need to place <code>QuantStub</code> <code>DeQuantStub</code> manually in the code to use eager mode quantization, please follow the tutorials here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/stihl1210; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/stihl1210; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 11: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 12: https://discuss.pytorch.org/u/sptoyoursoul; <NewLine> REPLIER 13: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  9:28pm; <NewLine> REPLY_DATE 2: March 3, 2020,  1:41am; <NewLine> REPLY_DATE 3: March 3, 2020,  8:46am; <NewLine> REPLY_DATE 4: March 3, 2020, 10:07pm; <NewLine> REPLY_DATE 5: March 3, 2020, 10:13pm; <NewLine> REPLY_DATE 6: March 4, 2020,  6:50am; <NewLine> REPLY_DATE 7: March 4, 2020,  6:52am; <NewLine> REPLY_DATE 8: March 4, 2020,  6:30pm; <NewLine> REPLY_DATE 9: March 5, 2020,  9:26am; <NewLine> REPLY_DATE 10: March 6, 2020,  5:56am; <NewLine> REPLY_DATE 11: March 10, 2020,  8:55pm; <NewLine> REPLY_DATE 12: March 13, 2020,  1:55am; <NewLine> REPLY_DATE 13: March 19, 2020,  8:55pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: ; <NewLine> REPLY 11 LIKES: ; <NewLine> REPLY 12 LIKES: ; <NewLine> REPLY 13 LIKES: ; <NewLine> 
72089,RuntimeError: Python builtin &lt;built-in method apply of FunctionMeta object at 0x55dad1b31680&gt; is currently not supported in Torchscript:,2020-03-05T02:58:37.633Z,1,151,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was running others code in <a href=""https://github.com/Xilinx/brevitas/tree/master/examples,and"" rel=""nofollow noopener"">https://github.com/Xilinx/brevitas/tree/master/examples,and</a> encounted this question.Is anyone know how to deal with that?</p><NewLine><p>my pytorch version is 1.4 for GPU,and it works well.CUDA version is 10.1,GPU is 2080ti.</p><NewLine><p>very thanks</p><NewLine><p>Traceback (most recent call last):<br/><NewLine>File “imagenet_val.py”, line 146, in <br/><NewLine>main()<br/><NewLine>File “imagenet_val.py”, line 42, in main<br/><NewLine>model = models<a>arch</a><br/><NewLine>File “/home/mjx/brevitasmaster/examples/models/mobilenetv1.py”, line 121, in quant_mobilenet_v1<br/><NewLine>net = MobileNet(channels=channels,first_stage_stride=first_stage_stride,bit_width=bit_width)<br/><NewLine>File “/home/mjx/brevitasmaster/examples/models/mobilenetv1.py”, line 75, in <strong>init</strong><br/><NewLine>weight_bit_width=FIRST_LAYER_BIT_WIDTH,                        activation_scaling_per_channel=True,act_bit_width=bit_width)<br/><NewLine>File “/home/mjx/brevitasmaster/examples/models/mobilenetv1.py”, line 28, in <strong>init</strong><br/><NewLine>self.conv = make_quant_conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=padding,groups=groups,bias=False,bit_width=weight_bit_width)<br/><NewLine>File “/home/mjx/brevitasmaster/examples/models/common.py”, line 74, in make_quant_conv2d<br/><NewLine>weight_scaling_min_val=weight_scaling_min_val)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/nn/quant_conv.py”, line 179, in <strong>init</strong><br/><NewLine>override_pretrained_bit_width=weight_override_pretrained_bit_width)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/proxy/parameter_quant.py”, line 357, in <strong>init</strong><br/><NewLine>self.re_init_tensor_quant()<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/proxy/parameter_quant.py”, line 360, in re_init_tensor_quant<br/><NewLine>self.tensor_quant = self.lazy_tensor_quant_init(tracked_parameter_list=self._tracked_parameter_list)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/proxy/parameter_quant.py”, line 146, in _weight_quant_init_impl<br/><NewLine>affine=scaling_impl_type == ScalingImplType.AFFINE_STATS)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1453, in init_then_script<br/><NewLine>original_init(self, *args, **kwargs)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/core/scaling.py”, line 246, in <strong>init</strong><br/><NewLine>stats_output_shape=stats_output_shape)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1453, in init_then_script<br/><NewLine>original_init(self, *args, **kwargs)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/core/scaling.py”, line 171, in <strong>init</strong><br/><NewLine>self.restrict_scaling = RestrictValue(restrict_scaling_type, FloatToIntImplType.CEIL, scaling_min_val)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1453, in init_then_script<br/><NewLine>original_init(self, *args, **kwargs)<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/core/restrict_val.py”, line 82, in <strong>init</strong><br/><NewLine>float_to_int_impl = CeilSte()<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1456, in init_then_script<br/><NewLine>self.<strong>dict</strong>[""_actual_script_module""] = torch.jit._recursive.create_script_module(self, stubs)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 296, in create_script_module<br/><NewLine>return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 340, in create_script_module_impl<br/><NewLine>create_methods_from_stubs(concrete_type, stubs)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 259, in create_methods_from_stubs<br/><NewLine>concrete_type._create_methods(defs, rcbs, defaults)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/_recursive.py”, line 555, in try_compile_fn<br/><NewLine>return torch.jit.script(fn, _rcb=rcb)<br/><NewLine>File “/home/mjx/.conda/envs/py/lib/python3.7/site-packages/torch/jit/<strong>init</strong>.py”, line 1281, in script<br/><NewLine>fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))<br/><NewLine>RuntimeError:<br/><NewLine>Python builtin &lt;built-in method apply of FunctionMeta object at 0x55dad1b31680&gt; is currently not supported in Torchscript:<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/function/ops_ste.py”, line 93</p><NewLine><pre><code>""""""<NewLine>return ceil_ste_fn.apply(x)<NewLine>       ~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine></code></pre><NewLine><p>‘ceil_ste’ is being compiled since it was called from ‘CeilSte.forward’<br/><NewLine>File “/home/mjx/brevitasmaster/brevitas/core/function_wrapper.py”, line 79<br/><NewLine><span class=""mention"">@torch.jit.script_method</span><br/><NewLine>def forward(self, x: torch.Tensor):<br/><NewLine>return ceil_ste(x)<br/><NewLine>~~~~~~~~~~ &lt;— HERE</p><NewLine></div>",https://discuss.pytorch.org/u/JachinMa,,JachinMa,"March 5, 2020,  2:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I don’t think torchscript supports custom autograd Function. Was that working before?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Sorry I dont know，I run other guy‘s code。</p><NewLine><p>so maybe I can try to modify that</p><NewLine><p>thanks a lot</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JachinMa; <NewLine> ,"REPLY_DATE 1: March 5, 2020,  3:00pm; <NewLine> REPLY_DATE 2: March 15, 2020,  1:51pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
71950,Will Pytorch support exporting quantized model?,2020-03-04T06:45:58.013Z,0,314,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I see that pytorch use onnx internally to transport quantized model(using pytorch quantization api) to caffe2. And I can export this internal quantized model representation. Like below:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/c9bf81dc06953ec7635d4f1409bd945425c8bf9b"" href=""https://discuss.pytorch.org/uploads/default/original/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b.png"" title=""pytorch_internal_qmodel""><img alt=""pytorch_internal_qmodel"" data-base62-sha1=""sMKgZb99bgcDBHkmDaqDZPceYRB"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b_2_10x10.png"" height=""332"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b_2_690x332.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b_2_690x332.png, https://discuss.pytorch.org/uploads/default/original/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/c/9/c9bf81dc06953ec7635d4f1409bd945425c8bf9b.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">pytorch_internal_qmodel</span><span class=""informations"">884×426 33 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>As we can see, all operators in the model are custom op which can directly transport to caffe2, but this is not that flexible for using this quantized model as exchange format.</p><NewLine><p>AFAIK, Tensorflow can export QAT model that contains FakeQuant Op, and transport the model to TFLite. In my opinion, we can export a quantized model that only contains FakeQuant Op(in ONNX Custom Op) and Standard ONNX Ops. This make the quantized model more flexible.</p><NewLine><p>What’s your opinion about it? Thanks.</p><NewLine></div>",https://discuss.pytorch.org/u/Zehaos,(Zehao Shi),Zehaos,"March 4, 2020,  6:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/><img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/><img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/><img alt="":smiley:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title="":smiley:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""71950"" data-username=""Zehaos""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/zehaos/40/10102_2.png"" width=""20""/> Zehaos:</div><NewLine><blockquote><NewLine><p>AFAIK, Tensorflow can export QAT model that contains FakeQuant Op, and transport the model to TFLite. In my opinion, we can export a quantized model that only contains FakeQuant Op(in ONNX Custom Op) and Standard ONNX Ops. This make the quantized model more flexible.</p><NewLine></blockquote><NewLine></aside><NewLine><p>For qat we have fake quant ops, but for inference we don’t, we have quant/dequant ops instead.<br/><NewLine>If you are talking about preserving quant/dequant op without fusing them into quantized ops(like quantized::conv2d), we do have this support in graph mode quantization, which will come up pretty soon</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Zehaos; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: March 8, 2020,  2:27am; <NewLine> REPLY_DATE 2: March 10, 2020,  8:44pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
64188,How do I save and load quantization model,2019-12-16T09:13:00.674Z,4,1057,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have quantized resenet50, quntize_per_channel_resent50 model is giving good accuracy same as floating-point.  If I do torch jit save then I can load torch jit load. and do the inference.</p><NewLine><p>How can I use a torch.save and torch.load model on a quantized model?<br/><NewLine>Will the entire state dict have same scale and zero points?<br/><NewLine>How can I get each layer scale and zero points from the quantized model?</p><NewLine></div>",https://discuss.pytorch.org/u/Tiru_B,(Tiru B),Tiru_B,"December 16, 2019,  9:13am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""64188""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tiru_b/40/18731_2.png"" width=""20""/> Tiru_B:</div><NewLine><blockquote><NewLine><p>How can I use a torch.save and torch.load model on a quantized model?</p><NewLine></blockquote><NewLine></aside><NewLine><blockquote><NewLine><p>How can I use a torch.save and torch.load model on a quantized model?</p><NewLine></blockquote><NewLine><p>Currently we only support torch.save(model.state_dict()) and model.load_state_dict(…) I think. torch.save/torch.load model directly is not yet supported I believe.</p><NewLine><blockquote><NewLine><p>Will the entire state dict have same scale and zero points?</p><NewLine></blockquote><NewLine><p>No,  they’ll have scale/zero_point that’s calculated from the calibration step.</p><NewLine><blockquote><NewLine><p>How can I get each layer scale and zero points from the quantized model?</p><NewLine></blockquote><NewLine><p>you can print the quantized model and it will show scale and zero_point, e.g.:</p><NewLine><pre><code class=""lang-auto"">&gt; print(torch.nn.quantized.Conv2d(3, 3, 3))<NewLine>QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""2"" data-topic=""64188""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/jerryzh168/40/15217_2.png"" width=""20""/> jerryzh168:</div><NewLine><blockquote><NewLine><p>save</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a></p><NewLine><p>I was able to save with model.state_dict() but not able to lad the model with same model.load_state_dict(). It was giving keyError.</p><NewLine><p>Secondly if I save with  torch.jit.save(torch.jit.script(pcqmodel),“quantization_per_channel_model.pth”)</p><NewLine><p>I am not able to see the Quantization  info after loading the model . Referred in this issue<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/28331"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/28331"" rel=""nofollow noopener"" target=""_blank"">How to save quantized model in PyTorch1.3 with quantization information</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-19"" data-format=""ll"" data-time=""07:55:01"" data-timezone=""UTC"">07:55AM - 19 Oct 19 UTC</span><NewLine></div><NewLine><div class=""date""><NewLine>          closed <span class=""discourse-local-date"" data-date=""2019-10-23"" data-format=""ll"" data-time=""17:05:26"" data-timezone=""UTC"">05:05PM - 23 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/vippeterhou"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""vippeterhou"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars2.githubusercontent.com/u/29670604?v=4"" width=""20""/><NewLine>          vippeterhou<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">❓ How to save the quantized model in PyTorch1.3 with quantization information<NewLine>Is there any way to save the quantized model in...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">quantization</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>are you using the most recent version? could you try again with PyTorch nightly builds?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also, check if it is just the <code>__repr__</code> that is not showing the info or are the quant params really missing – try getting the scale and zero_point directly.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Be sure you do the whole post training preparation process (by running layer fusion, torch.quantization.prepare() and torch.quantization.convert() ) <strong>before</strong> loading the state_dict.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Tiru_B; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Zafar; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/wolffadam; <NewLine> ,"REPLY_DATE 1: December 18, 2019, 12:29am; <NewLine> REPLY_DATE 2: January 8, 2020,  5:58am; <NewLine> REPLY_DATE 3: March 3, 2020,  5:43pm; <NewLine> REPLY_DATE 4: March 3, 2020, 10:17pm; <NewLine> REPLY_DATE 5: March 9, 2020,  9:21am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
72288,Pytorch model quantization for mobile device,2020-03-06T13:15:28.433Z,0,108,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there.<br/><NewLine>I’m wondering can we quantize any model in pytorch or there are some constraints on it?<br/><NewLine>If so then what are those constraints?<br/><NewLine>thanks in advance…</p><NewLine></div>",https://discuss.pytorch.org/u/Chame_call,(chame_call),Chame_call,"March 6, 2020,  4:42pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The constraints I can think of are:</p><NewLine><ul><NewLine><li>Op support, if some ops in the model don’t have quantized version, they need to be implemented or otherwise skipped during quantization.</li><NewLine><li>Accuracy constraint, quantization will introduce some errors to the output, if the quantized model has error larger than tolerance, you may need to skip the quantization of some operators in the model to get acceptable accuracy.</li><NewLine></ul><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: March 6, 2020, 10:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
71047,Help needed: Specific function call causing 20X slowdown of computation,2020-02-25T20:18:49.296Z,1,89,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am looking for help with the following code:</p><NewLine><blockquote><NewLine><p>SRAM_rows = 256<br/><NewLine>step_size = 1<br/><NewLine>prob_table = torch.eye(SRAM_rows2+1).cuda()<br/><NewLine>levels = torch.tensor([i-SRAM_rows for i in range(SRAM_rows2+1)]).cuda().float()<br/><NewLine>cmprob = 0<br/><NewLine>Expected_outputs = torch.tensor([0]*257).cuda().float()</p><NewLine><p>def quant_XNORSRAM(x, prob_table, levels, step_size, lower_bound):<br/><NewLine>x_ind = (x.type(torch.int64) - lower_bound) / step_size<br/><NewLine>num_levels = len(levels)<br/><NewLine>x_cdf = prob_table[x_ind, 0:num_levels-1].cumsum(dim=-1)<br/><NewLine>x_rand = torch.rand(x.shape, device=‘cuda:0’)<br/><NewLine>x_rand = torch.stack([x_rand] * (num_levels-1), dim=-1)<br/><NewLine>x_comp = (x_rand &gt; x_cdf).type(torch.int64).sum(dim=-1)<br/><NewLine><span class=""hashtag"">#import</span> pdb; pdb.set_trace()</p><NewLine><pre><code>#here if cmprob is set, we add the ideal value + the noise<NewLine>if(cmprob):<NewLine>    y = Expected_outputs[x_ind] + levels[x_comp]<NewLine>else:<NewLine>    y = levels[x_comp]<NewLine>return y<NewLine></code></pre><NewLine></blockquote><NewLine><p>Just by calling the quant_XNORSRAM function, my compute time is increased 20 times, can you please help me identify the issue causing this and how to fix it?</p><NewLine><p>Thanks</p><NewLine></div>",https://discuss.pytorch.org/u/Sai_Kiran,(Sai Kiran),Sai_Kiran,"February 25, 2020,  8:19pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This doesn’t seem related to quantization. Computation time increased 20x wrt what baseline?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is kind of quantization, as I am transforming X to Y using a specific quantization, the scenario is compute time increases 20 times if I use the function shown above  compared to the case where I don’t use it (no quantization)</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is not related to <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener""><code>PyTorch Quantization</code></a>.<br/><NewLine>To get help faster, I think it is better to add a different tag. Maybe “CUDA”?</p><NewLine><p>Meanwhile, <a class=""mention"" href=""/u/sai_kiran"">@Sai_Kiran</a> can you explain what is the 20x slow down compared to (as Daya asked: what’s the baseline)?</p><NewLine><p>Also, CUDA doesn’t really benefit from quantization unless you are using TensorRT (<a href=""https://docs.nvidia.com/cuda/cuda-c-programming-guide/#arithmetic-instructions"" rel=""nofollow noopener"">see this for throughputs in CUDA</a>)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Sai_Kiran; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: February 25, 2020,  9:40pm; <NewLine> REPLY_DATE 2: February 25, 2020, 10:30pm; <NewLine> REPLY_DATE 3: March 3, 2020, 10:33pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
71688,Supported quantized tensor operations,2020-03-02T08:22:37.361Z,0,581,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Is there a list of currently supported operations for quantized tensors?</p><NewLine><p>I run into issues quantizing a network requiring tensor additions:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Could not run 'aten::add.Tensor' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::add.Tensor' is only available for these backends: [SparseCPUTensorId, CPUTensorId, VariableTensorId, MkldnnCPUTensorId].<NewLine></code></pre><NewLine><p>Others have reported running into issues with div and cat operations - I presume these are also not supported atm.</p><NewLine></div>",https://discuss.pytorch.org/u/kwojcicki,(Kamil Wojcicki),kwojcicki,"March 2, 2020, 10:40am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>In case this is helpful to anyone, there are:</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.FloatFunctional"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.FloatFunctional</a></p><NewLine><p>and</p><NewLine><p><a class=""onebox"" href=""https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.QFunctional"" rel=""nofollow noopener"" target=""_blank"">https://pytorch.org/docs/stable/quantization.html#torch.nn.quantized.QFunctional</a></p><NewLine><p>that support add and other operations.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Add is supported, but not as a <code>at::add.Tensor</code>. The reason is that addition (or any arithmetic) requires output scale/zero_point. Also, often times quantized ops need to be stateful. Hence, there are stateful <code>FloatFunctional</code> and <code>QFunctional</code></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/kwojcicki; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  6:00pm; <NewLine> REPLY_DATE 2: March 5, 2020,  7:33pm; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
71430,When I run a quantiztized model error happened,2020-02-28T12:13:53.613Z,0,238,"<div class=""post"" itemprop=""articleBody""><NewLine><p>when I run a quantiztized model , an error happend , trace is as below<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “/home/guanx/pycharmProject/erfnet/venv/erfnet_pytorch-lidarseg/eval/eval_lidar_seg.py”, line 435, in <br/><NewLine>evaluate(parser.parse_args())<br/><NewLine>File “/home/guanx/pycharmProject/erfnet/venv/erfnet_pytorch-lidarseg/eval/eval_lidar_seg.py”, line 292, in evaluate<br/><NewLine>outputs = model(inputs)<br/><NewLine>File “/home/guanx/anaconda3/envs/python37/lib/python3.7/site-packages/torch/nn/modules/module.py”, line 532, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>RuntimeError: Could not run ‘aten::max_pool2d_with_indices’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::max_pool2d_with_indices’ is only available for these backends: [CPUTensorId, VariableTensorId].<br/><NewLine>The above operation failed in interpreter.<br/><NewLine>Traceback (most recent call last):<br/><NewLine>File “”, line 63<br/><NewLine>dilation: List[int],<br/><NewLine>ceil_mode: bool):<br/><NewLine>output, indices = torch.max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode)<br/><NewLine>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;— HERE<br/><NewLine>def backward(grad_output):<br/><NewLine>grad_self = torch.max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)</p><NewLine><p>The above operation failed in interpreter.</p><NewLine><p>who could help me to handle this error</p><NewLine></div>",https://discuss.pytorch.org/u/sptoyoursoul,(guanxiang),sptoyoursoul,"February 28, 2020, 12:13pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""71430"" data-username=""sptoyoursoul""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/s/d9b06d/40.png"" width=""20""/> sptoyoursoul:</div><NewLine><blockquote><NewLine><p>max_pool2d_with_indices</p><NewLine></blockquote><NewLine></aside><NewLine><p>this means we don’t have quantized implementation for <code>torch.max_pool2d_with_indices</code>, feel free to open an issue in PyTorch repo and we can have someone working on this. cc <a class=""mention"" href=""/u/zafar"">@Zafar</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is dup: <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/i-try-to-run-quantizitzed-model-but-some-error-happend/71579"">I try to run quantizitzed model ,but some error happend</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: March 3, 2020,  5:45pm; <NewLine> REPLY_DATE 2: March 3, 2020, 10:15pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
67862,The parameters saved in the checkpoint are different from the ones in the fused model,2020-01-28T05:35:14.412Z,3,288,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello everyone,<br/><NewLine>I hope you are having a great day,<br/><NewLine>I’m having difficulties loading a quantized model.<br/><NewLine>When I investigated I noted that the chekpoint file has 236 parameter keys, while the model, after being fused as only 112 parameter names.</p><NewLine><pre><code class=""lang-auto"">(base) marian@u04-2:/mnt/s3user/Pytorch_Retinaface_quantized# python test_widerface.py --trained_model ./weights/mobilenet0.25_Final_quantized.pth --network mobile0.25layers:  <NewLine>Loading pretrained model from ./weights/mobilenet0.25_Final_quantized.pth<NewLine>remove prefix 'module.'<NewLine>Missing keys:235<NewLine>Unused checkpoint keys:171<NewLine>Used keys:65<NewLine>Traceback (most recent call last):<NewLine>  File ""/root/.vscode/extensions/ms-python.python-2020.1.58038/pythonFiles/ptvsd_launcher.py"", line 43, in &lt;module&gt;<NewLine>    main(ptvsdArgs)<NewLine>  File ""/root/.vscode/extensions/ms-python.python-2020.1.58038/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py"", line 432, in main<NewLine>    run()<NewLine>  File ""/root/.vscode/extensions/ms-python.python-2020.1.58038/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py"", line 316, in run_file<NewLine>    runpy.run_path(target, run_name='__main__')<NewLine>  File ""/root/anaconda3/lib/python3.7/runpy.py"", line 263, in run_path<NewLine>    pkg_name=pkg_name, script_name=fname)<NewLine>  File ""/root/anaconda3/lib/python3.7/runpy.py"", line 96, in _run_module_code<NewLine>    mod_name, mod_spec, pkg_name, script_name)<NewLine>  File ""/root/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code<NewLine>    exec(code, run_globals)<NewLine>  File ""/mnt/f3user/Pytorch_Retinaface_quantized/test_widerface.py"", line 114, in &lt;module&gt;<NewLine>    net = load_model(net, args.trained_model, args.cpu)<NewLine>  File ""/mnt/f3user/Pytorch_Retinaface_quantized/test_widerface.py"", line 95, in load_model<NewLine>    model.load_state_dict(pretrained_dict, strict=False)<NewLine>  File ""/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 830, in load_state_dict<NewLine>    self.__class__.__name__, ""\n\t"".join(error_msgs)))<NewLine>RuntimeError: Error(s) in loading state_dict for RetinaFace:<NewLine>        While copying the parameter named ""ssh1.conv3X3.0.weight"", whose dimensions in the model are torch.Size([32, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 64, 3, 3]).<NewLine>        While copying the parameter named ""ssh1.conv5X5_2.0.weight"", whose dimensions in the model are torch.Size([16, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 16, 3, 3]).<NewLine>        While copying the parameter named ""ssh1.conv7x7_3.0.weight"", whose dimensions in the model are torch.Size([16, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 16, 3, 3]).<NewLine>        While copying the parameter named ""ssh2.conv3X3.0.weight"", whose dimensions in the model are torch.Size([32, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 64, 3, 3]).<NewLine>        While copying the parameter named ""ssh2.conv5X5_2.0.weight"", whose dimensions in the model are torch.Size([16, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([16, 16, 3, 3]).<NewLine>.....<NewLine><NewLine></code></pre><NewLine><p>The full list can be found <a href=""https://paste.ee/p/zg0uR"" rel=""nofollow noopener"">here</a>.<br/><NewLine>basically the weights cant be found. plus the scale and zero_point which are missing from the fused model.</p><NewLine><p>The following snippet is the actual training loop which was used to train and save the model :</p><NewLine><pre><code class=""lang-python"">if __name__ == '__main__':<NewLine>    # train()<NewLine>    ...<NewLine>    net = RetinaFace(cfg=cfg)<NewLine>    print(""Printing net..."")<NewLine>    print(net)<NewLine><NewLine>    net.fuse_model()<NewLine>    ...<NewLine>    <NewLine>    net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>    torch.quantization.prepare_qat(net, inplace=True)<NewLine>    print(f'quantization preparation done.')<NewLine><NewLine>    ... <NewLine>    <NewLine>    quantized_model = net <NewLine>    for i in range(max_epoch):<NewLine>        net = net.to(device)<NewLine>        train_one_epoch(net, data_loader, optimizer, criterion, cfg, gamma, i, step_index, device)<NewLine>        if i in stepvalues:<NewLine>            step_index += 1<NewLine>        if i &gt; 3 :<NewLine>            net.apply(torch.quantization.disable_observer)<NewLine>        if i &gt; 2 :<NewLine>            net.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine>        net=net.cpu()<NewLine>        quantized_model = torch.quantization.convert(net.eval(), inplace=False)<NewLine>        quantized_model.eval()<NewLine>        # evaluate on test set ?!<NewLine>    <NewLine>    torch.save(net.state_dict(), save_folder + cfg['name'] + '_Final.pth')<NewLine>    torch.save(quantized_model.state_dict(), save_folder + cfg['name'] + '_Final_quantized.pth')<NewLine>    #torch.jit.save(torch.jit.script(quantized_model), save_folder + cfg['name'] + '_Final_quantized_jit.pth')<NewLine><NewLine><NewLine></code></pre><NewLine><p>by the way <code>test_widerface.py</code> can be accessed <a href=""https://github.com/biubug6/Pytorch_Retinaface/blob/master/test_widerface.py"" rel=""nofollow noopener"">here</a></p><NewLine><p>You can view keys <a href=""https://paste.ee/p/zg0uR"" rel=""nofollow noopener"">here</a></p><NewLine><p>Why has this happened? How should this be taken care of?</p><NewLine></div>",https://discuss.pytorch.org/u/Shisho_Sama,(A curious guy here!),Shisho_Sama,"January 28, 2020,  6:33am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I checked the name, and created a new state_dict dictionary and inserted the 112 keys that were in both checkpoint and model using the snippet below :</p><NewLine><pre><code class=""lang-python"">new_state_dict  = {}<NewLine>checkpoint_state_dict = torch.load(checkpoint_path, map_location=lambda storage, loc: storage) <NewLine>for (ck, cp) in checkpoint_state_dict.items():<NewLine>    for (mk, mp) in model.state_dict().items():<NewLine>        kname,kext = os.path.splitext(ck)<NewLine>        mname,mext = os.path.splitext(mk)<NewLine>        # check the two parameter and see if they are the same<NewLine>        # then use models key naming scheme and use checkpoints weights<NewLine>        if kname+kext == mname+mext or kname+'.0'+kext == mname+mext:<NewLine>            new_state_dict[mname+mext] = cp <NewLine>        else: <NewLine>             if kext in ('.scale','.zero_point'):<NewLine>                 new_state_dict[ck] = cp<NewLine><NewLine></code></pre><NewLine><p>and then use  this new state_dict! yet I’m getting  the ver same exact errors!<br/><NewLine>meaning errors like  this :</p><NewLine><pre><code>RuntimeError: Error(s) in loading state_dict for RetinaFace:<NewLine>        While copying the parameter named ""ssh1.conv3X3.0.weight"", whose dimensions in the model are torch.Size([32, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 64, 3, 3]).<NewLine></code></pre><NewLine><p>This is really frustrating and there is no documentation concerning this! I’m completely clueless here.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks to dear God, after hours of debugging I finally found out the cause:<br/><NewLine>The error messages with the form of :</p><NewLine><blockquote><NewLine><p>While copying the parameter named “xxx.weight”, whose<br/><NewLine>dimensions in the model are torch.Size([yyy]) and whose<br/><NewLine>dimensions in the checkpoint are torch.Size([yyy]).</p><NewLine></blockquote><NewLine><p>are actually generic messages, only returned when an exception has occured while copying the parameters in question.<br/><NewLine>Pytorch developers could easily, add the actual exception args into this spurious yet unhelpful message, so it could actually help better debug the issue at hand.<br/><NewLine>Anyway, looking at the exception which was by the way :</p><NewLine><pre><code>""copy_"" not implemented for \'QInt8' <NewLine></code></pre><NewLine><p>you’ll now know what the actual issue is/was!</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>glad you found the problem, the error message is indeed confusing</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/shisho_sama"">@Shisho_Sama</a> is this in master?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, this is fixed now</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Shisho_Sama; <NewLine> ,"REPLY_DATE 1: January 28, 2020,  8:09am; <NewLine> REPLY_DATE 2: March 3, 2020, 10:34pm; <NewLine> REPLY_DATE 3: February 24, 2020, 11:22pm; <NewLine> REPLY_DATE 4: March 3, 2020,  3:05am; <NewLine> REPLY_DATE 5: March 3, 2020,  4:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> 
67456,How to extract individual weights after per channel static quantization?,2020-01-23T10:47:24.356Z,0,342,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I quantized my CNN network with the static quantization module and the <strong>per channel quantization</strong> . Here is a part of my quantized network:</p><NewLine><pre><code class=""lang-auto"">Sequential(<NewLine>  (0): QuantizedConv2d(2, 2, kernel_size=(1, 8), stride=(1, 2), scale=0.43247514963150024, zero_point=62, padding=(0, 3), groups=2)<NewLine>  (1): QuantizedConvReLU2d(2, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.0933830738067627, zero_point=0)<NewLine>  (2): Identity()<NewLine>  (3): Identity()<NewLine>  (4): Dropout(p=0.5, inplace=False)<NewLine>)<NewLine></code></pre><NewLine><p>In the end, I tried to extract the weights and quantization parameters of each convolution kernel.</p><NewLine><pre><code class=""lang-auto"">In   [518]: layers[0].weight()<NewLine>Out[518]: <NewLine>tensor([[[[ 0.5521, -0.4270, -0.9423, -0.8687, -0.4932, -0.3313, -0.3755,<NewLine>            0.0221]]],<NewLine><NewLine>        [[[-0.4360, -0.6763, -0.7154, -0.5980, -0.6372, -0.0447, -0.1733,<NewLine>           -0.2962]]]], size=(2, 1, 1, 8), dtype=torch.qint8,<NewLine>       quantization_scheme=torch.per_channel_affine,<NewLine>       scale=tensor([0.0074, 0.0056], dtype=torch.float64),<NewLine>       zero_point=tensor([0, 0]), axis=0)<NewLine></code></pre><NewLine><p>I tried to read the weights, but I got this error:</p><NewLine><pre><code class=""lang-auto"">In [562]: layers[0].weight().data[0,0,0,0]<NewLine>Traceback (most recent call last):<NewLine><NewLine>  File ""&lt;ipython-input-562-742a141c2263&gt;"", line 1, in &lt;module&gt;<NewLine>    layers[0].weight().data[0,0,0,0]<NewLine><NewLine>RuntimeError: Setting strides is possible only on uniformly quantized tensor<NewLine></code></pre><NewLine><p>Also, I can get a single scale for the whole layer[0], but I do not know what it is related to compared to the per-channel scale (q_per_channel_scales)?</p><NewLine><pre><code class=""lang-auto"">In [567]: layers[0].scale<NewLine><NewLine>Out[567]: 0.43247514963150024<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/babak_hss,(Bob),babak_hss,"January 23, 2020, 10:51am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>OK. It seems like we can use<br/><NewLine><code>layer[0].weight().int_repr().data[i,j,l,m]</code><br/><NewLine>to get the INT8 representation of the weight entries.</p><NewLine><p>Also,<br/><NewLine><code>layer[0].weight().dequantize()</code><br/><NewLine>gives the tensor of the weights in FP format to have element-wise access to its contents.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/babak_hss; <NewLine> ,"REPLY_DATE 1: February 26, 2020,  1:16pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
69830,[caffe2] Post train quantization,2020-02-15T13:59:25.353Z,0,99,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello there,</p><NewLine><p>Since <code>pytorch==1.3</code> has released there is an ability to perform quantization of the model during training time. And as I understand quantization works with QNNPACK.<br/><NewLine>Before that we had a way to run caffe2 models using QNNPACK too.</p><NewLine><p>The question is following is there a way to quantize caffe2 model (already trained) today?</p><NewLine></div>",https://discuss.pytorch.org/u/zetyquickly,(Emil Bogomolov),zetyquickly,"February 15, 2020,  1:59pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""69830"" data-username=""zetyquickly""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/z/3da27b/40.png"" width=""20""/> zetyquickly:</div><NewLine><blockquote><NewLine><p>The question is following is there a way to quantize caffe2 model (already trained) today?</p><NewLine></blockquote><NewLine></aside><NewLine><p>We have internal script to quantize these models but it’s not open sourced I think. Our recommendation is to switch to PyTorch</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 24, 2020, 11:18pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
69977,Not able to load quantized model in android,2020-02-17T00:09:38.719Z,0,213,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am able to load models in android, but when I quantize the same model with PyTorch and not able to load it in android. Below is the stack trace</p><NewLine><pre><code class=""lang-auto"">java.lang.RuntimeException: Unable to start activity ComponentInfo{com.fitnest.testprediction/com.fitnest.testprediction.MainActivity}: java.lang.RuntimeException: weight.qscheme() == kPerTensorAffine CHECK FAILED at ../aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp<NewLine>    The above operation failed in interpreter, with the following stack trace:<NewLine>    at code/__torch__/torch/nn/intrinsic/quantized/modules/conv_relu.py:70:10<NewLine>        _22 = self.stride<NewLine>        _23 = self.padding<NewLine>        _24 = self.dilation<NewLine>        _25 = self.groups<NewLine>        _26, _27, = _22<NewLine>        _28 = [_26, _27]<NewLine>        _29, _30, = _23<NewLine>        _31 = [_29, _30]<NewLine>        _32, _33, = _24<NewLine>        _34 = ops.quantized.conv_prepack(_20, _21, _28, _31, [_32, _33], _25)<NewLine>              ~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        self._packed_params = _34<NewLine>        self.scale = (state)[12]<NewLine>        self.zero_point = (state)[13]<NewLine>        self.training = (state)[14]<NewLine>        return None<NewLine>      def _weight_bias(self: __torch__.torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d) -&gt; Tuple[Tensor, Optional[Tensor]]:<NewLine>        _35, _36 = ops.quantized.conv_unpack(self._packed_params)<NewLine>        return (_35, _36)<NewLine>      def set_weight_bias(self: __torch__.torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d,<NewLine>    Compiled from code at /home/un270/anaconda2/envs/py37/lib/python3.7/site-packages/torch/nn/quantized/modules/conv.py:110:30<NewLine>        def set_weight_bias(self, w, b):<NewLine>            # type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None<NewLine>            self._packed_params = torch.ops.quantized.conv_prepack(<NewLine>                                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>                w, b, self.stride, self.padding, self.dilation, self.groups)<NewLine>    <NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3447)<NewLine>        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3594)<NewLine>        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:83)<NewLine>        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)<NewLine>        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)<NewLine>        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2146)<NewLine>        at android.os.Handler.dispatchMessage(Handler.java:107)<NewLine>        at android.os.Looper.loop(Looper.java:237)<NewLine>        at android.app.ActivityThread.main(ActivityThread.java:7777)<NewLine>        at java.lang.reflect.Method.invoke(Native Method)<NewLine>        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)<NewLine>        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1047)<NewLine>     Caused by: java.lang.RuntimeException: weight.qscheme() == kPerTensorAffine CHECK FAILED at ../aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp<NewLine>    The above operation failed in interpreter, with the following stack trace:<NewLine>    at code/__torch__/torch/nn/intrinsic/quantized/modules/conv_relu.py:70:10<NewLine>        _22 = self.stride<NewLine>        _23 = self.padding<NewLine>        _24 = self.dilation<NewLine>        _25 = self.groups<NewLine>        _26, _27, = _22<NewLine>        _28 = [_26, _27]<NewLine>        _29, _30, = _23<NewLine>        _31 = [_29, _30]<NewLine>        _32, _33, = _24<NewLine>        _34 = ops.quantized.conv_prepack(_20, _21, _28, _31, [_32, _33], _25)<NewLine>              ~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>        self._packed_params = _34<NewLine>        self.scale = (state)[12]<NewLine>        self.zero_point = (state)[13]<NewLine>        self.training = (state)[14]<NewLine>        return None<NewLine>      def _weight_bias(self: __torch__.torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d) -&gt; Tuple[Tensor, Optional[Tensor]]:<NewLine>        _35, _36 = ops.quantized.conv_unpack(self._packed_params)<NewLine>        return (_35, _36)<NewLine>      def set_weight_bias(self: __torch__.torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d,<NewLine>    Compiled from code at /home/un270/anaconda2/envs/py37/lib/python3.7/site-packages/torch/nn/quantized/modules/conv.py:110:30<NewLine>        def set_weight_bias(self, w, b):<NewLine>            # type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None<NewLine>            self._packed_params = torch.ops.quantized.conv_prepack(<NewLine>                                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE<NewLine>02-16 18:41:53.781 21251-21251/com.fitnest.testprediction E/AndroidRuntime:             w, b, self.stride, self.padding, self.dilation, self.groups)<NewLine>    <NewLine>        at org.pytorch.Module$NativePeer.initHybrid(Native Method)<NewLine>        at org.pytorch.Module$NativePeer.&lt;init&gt;(Module.java:70)<NewLine>        at org.pytorch.Module.&lt;init&gt;(Module.java:25)<NewLine>        at org.pytorch.Module.load(Module.java:21)<NewLine>        at com.fitnest.testprediction.MainActivity.onCreate(MainActivity.java:101)<NewLine>        at android.app.Activity.performCreate(Activity.java:7981)<NewLine>        at android.app.Activity.performCreate(Activity.java:7970)<NewLine>        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1307)<NewLine>        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3422)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/utkarshnath,(Utkarsh Nath),utkarshnath,"February 17, 2020, 12:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>looks like you are passing a per tensor quantized weight to set_weight_bias, which is not supported.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 24, 2020, 11:17pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66905,Current status of automatic quantization support,2020-01-16T21:31:57.554Z,2,293,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I remember from one of the talks at the last dev con or elsewhere that PyTorch v1.4 would ship with support for automatic quantization of jitted models. v1.4 is out now, but it is not clear from looking at the release note whether this feature is official.</p><NewLine><p>Is automatic quantization ready for experimental use? Can it already quantize an entire network from torchvision for example?</p><NewLine><p>Thanks<br/><NewLine>masa</p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"January 16, 2020,  9:31pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, we are very close to having graph mode ready, there are still a few changes that havent been completed yet. (cc <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> for updates)</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, graph mode quantization will be ready for testing after <a href=""https://github.com/pytorch/pytorch/pull/32303"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/32303</a> lands.<br/><NewLine>Next we are going to test more models and fix the issues we see in the process, this might take a few more weeks to a month or so, but in the meantime, feel free to checkout the master and test the graph mode quantization on your model as well.<br/><NewLine>I’ll reply here when this is ready for trying out.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the replies. I’m already using the master build so I’ll try it out. I have an image segmentation model to test, it always uncovers issues that folks working with imagenet models do not see.</p><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> Are you open for contribution? I’ve never contributed to such a large project as PyTorch, but I’m interested in the opportunity.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes contributions are very welcome, especially expanding the coverage of the graph mode quantization to more models. Currently we are still trying to make the backbone support work, so a good point might be after <a href=""https://github.com/pytorch/pytorch/pull/32816"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/32816</a>. After this PR we are already pretty close to eager mode result, there might be more PRs coming up but shouldn’t be too many.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a>, I tried graph mode quantization test cases in test_quantization.py. In <code>test_conv</code>, I dumped the quantized IR like so:</p><NewLine><pre><code class=""lang-auto"">model_quantized = quantize_script(<NewLine>    model_under_test,<NewLine>    qconfig_dict,<NewLine>    default_eval_fn,<NewLine>    [self.img_data],<NewLine>    inplace=False)<NewLine>self.assertEqual(model_quantized(self.img_data[0][0]), result_eager)<NewLine><NewLine>torch._C._jit_pass_inline(model_quantized.graph)<NewLine>print(model_quantized.graph)<NewLine></code></pre><NewLine><p>But the output is confusing. There are some quantization related operators inserted, but they are just doing quantize/dequantize round trip and the actual convolution is done in fp32.</p><NewLine><pre><code class=""lang-auto"">graph(%self.1 : __torch__.torch.nn.modules.module.___torch_mangle_7.Module,<NewLine>      %input : Float(2, 3, 10, 10)):<NewLine>  %2 : __torch__.torch.nn.modules.module.___torch_mangle_8.Module = prim::GetAttr[name=""conv""](%self.1)<NewLine>  %4 : float = prim::GetAttr[name=""input.1_scale""](%2)<NewLine>  %5 : int = prim::GetAttr[name=""input.1_zero_point""](%2)<NewLine>  %6 : int = prim::GetAttr[name=""input.1_scalar_type""](%2)<NewLine>  %input.1.quant : Tensor = aten::quantize_per_tensor(%input, %4, %5, %6)<NewLine>  %input.1.dequant.0 : Tensor = aten::dequantize(%input.1.quant)<NewLine>  %9 : bool = prim::Constant[value=1](), scope: __module.conv # /home/masa/work/pytorch/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %10 : bool = prim::Constant[value=0](), scope: __module.conv # /home/masa/work/pytorch/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %11 : int = prim::Constant[value=1](), scope: __module.conv # /home/masa/work/pytorch/pytorch/torch/nn/modules/conv.py:345:0<NewLine>  %12 : None = prim::Constant(), scope: __module.conv<NewLine>  %13 : Tensor = prim::GetAttr[name=""weight""](%2)<NewLine>  %14 : float = prim::GetAttr[name=""8_scale""](%2)<NewLine>  %15 : int = prim::GetAttr[name=""8_zero_point""](%2)<NewLine>  %16 : int = prim::GetAttr[name=""8_scalar_type""](%2)<NewLine>  %8.quant : Tensor = aten::quantize_per_tensor(%13, %14, %15, %16)<NewLine>  %18 : int[] = prim::Constant[value=[1, 1]]()<NewLine>  %19 : int[] = prim::Constant[value=[0, 0]]()<NewLine>  %20 : int[] = prim::Constant[value=[1, 1]]()<NewLine>  %21 : int[] = prim::Constant[value=[0, 0]]()<NewLine>  %22 : Tensor = quantized::conv2d_prepack(%8.quant, %12, %18, %19, %20, %11)<NewLine>  %23 : Tensor, %24 : Tensor? = quantized::conv2d_unpack(%22)<NewLine>  %25 : Tensor = aten::dequantize(%23)<NewLine>  %26 : Tensor = aten::conv2d(%input.1.dequant.0, %25, %24, %18, %19, %20, %11)<NewLine>  %27 : float = prim::GetAttr[name=""15_scale""](%2)<NewLine>  %28 : int = prim::GetAttr[name=""15_zero_point""](%2)<NewLine>  %29 : int = prim::GetAttr[name=""15_scalar_type""](%2)<NewLine>  %15.quant : Tensor = aten::quantize_per_tensor(%26, %27, %28, %29)<NewLine>  %15.dequant.0 : Tensor = aten::dequantize(%15.quant)<NewLine>  return (%15.dequant.0)<NewLine></code></pre><NewLine><p>Is this expected? I was hoping to see <code>quantized::conv2d</code> op there.</p><NewLine><p>I’m using torch built from source and the commit is at:</p><NewLine><pre><code class=""lang-auto"">In [2]: torch.__version__                                                                                                                                                                     <NewLine>Out[2]: '1.5.0a0+c75d06d'<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""6"" data-topic=""66905"" data-username=""masahi""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/masahi/40/19725_2.png"" width=""20""/> masahi:</div><NewLine><blockquote><NewLine><p>I’m using torch built from source and the commit is at:</p><NewLine><pre><code class=""lang-auto""><NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Yes, graph mode quantization is still in active development, we should be getting something working very soon though, please stay tuned.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/masahi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: January 17, 2020,  6:30pm; <NewLine> REPLY_DATE 2: January 17, 2020, 10:18pm; <NewLine> REPLY_DATE 3: January 17, 2020, 10:18pm; <NewLine> REPLY_DATE 4: January 31, 2020, 10:59pm; <NewLine> REPLY_DATE 5: February 17, 2020,  8:34am; <NewLine> REPLY_DATE 6: February 21, 2020,  5:23am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> 
68631,Decrease in the Speed of Quantization Model,2020-02-04T12:11:19.502Z,1,189,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have my base model as Squeezenet after porting it to android the execution time of model is around 160ms and after Quantizing the model generally the time should decrease but the time is increased to 220ms. Is there is something wrong or this can be possible?</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"February 4, 2020, 12:11pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>are you sure everything is properly quantized? could you print the model before and after quantization?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> I printed the model and everything is properly quantized.<br/><NewLine>In squeezenet I have to replace torch.cat to nn.FloatFunctional.cat so I think that function is giving issues which is also mentioned in other threads also</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/mohit7"">@mohit7</a> My answer at <a class=""inline-onebox"" href=""https://discuss.pytorch.org/t/quantized-cat-running-time-is-slower-than-fp32-model/68717/4"">Quantized::cat running time is slower than fp32 model</a> may help answer your question.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/masahi; <NewLine> ,"REPLY_DATE 1: February 14, 2020,  6:52pm; <NewLine> REPLY_DATE 2: February 18, 2020,  9:08am; <NewLine> REPLY_DATE 3: February 20, 2020,  4:14am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
70211,Conv2d_unpack and conv2d_prepack behavior,2020-02-18T17:54:32.517Z,0,250,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey I have a question about</p><NewLine><p><code>torch.ops.quantized.conv2d_unpack(self._packed_params)</code></p><NewLine><p>Why returned weight in int8 and bias in fp32? How can i convert bias to fixed point?</p><NewLine><p>Also i wanted to understand how <code>torch.ops.quantized.conv2d_prepack</code> this function is structured and what how packed_params is created.</p><NewLine></div>",https://discuss.pytorch.org/u/stihl1210,(Rafał Pilarczyk),stihl1210,"February 18, 2020,  5:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Bias is kept in fp32 format for eager mode quantization and dynamically quantized while computing quantized FC/Conv. It’s returned in fp32 because that’s how it’s passed in to an operator as well. The reason for keeping bias in fp32 is the unavailability of input scale until the operator has executed so we can’t quantize bias until then.</p><NewLine><p>To convert bias to quantized format, use input_scale * weight_scale with a zero_point = 0. See this <a href=""https://github.com/pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L104-L108"" rel=""nofollow noopener"">https://github.com/pytorch/FBGEMM/blob/master/include/fbgemm/OutputProcessing-inl.h#L104-L108</a> code for converting bias with act_times_weight scale.</p><NewLine><p>Check out the code in <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp</a> file for prepack function. If USE_FBGEMM is true, fbgemm_conv_prepack function is called for doing prepacking.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: March 2, 2020,  9:22pm; <NewLine> ",REPLY 1 LIKES: 3 Likes; <NewLine> 
68717,Quantized::cat running time is slower than fp32 model,2020-02-05T08:48:25.156Z,0,378,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, I am working on quantized my model. My model is running on Mobile devices. The issue is when running quantized::cat op, the running speed is much slower than the dequantized one.<br/><NewLine>I have successfully print the running time conparsion between these two ops.</p><NewLine><p>Logs for Quantized one:</p><NewLine><blockquote><NewLine><p>Blockquote<br/><NewLine>OP total_time :146462us<br/><NewLine>—RUNNING 2244 OP 588 # %852 : Tensor[] = prim::ListConstruct(%c2_ffm.1, %851, %843, %835)<br/><NewLine>—input  Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];<br/><NewLine>—output  TensorList;<br/><NewLine>OP total_time :15us<br/><NewLine>—RUNNING 2248 OP 589 # %input107.1 : Tensor = quantized::cat(%852, %8, %5, %6) # lib/python2.7/site-packages/torch/nn/quantized/modules/functional_modules.py:157:0<br/><NewLine>—input  TensorList;Int;Double;Int;<br/><NewLine>—output  Tensor:[1, 512, 240, 320];<br/><NewLine>OP total_time :<strong>3226438us</strong></p><NewLine></blockquote><NewLine><p>Logs for dequantized one:</p><NewLine><blockquote><NewLine><p>Blockquote<br/><NewLine>—RUNNING 4103 OP 684 # %1264 : Tensor[] = prim::ListConstruct(%c2_ffm.1, %c3.1, %c4.1, %c50.1)<br/><NewLine>—input  Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];Tensor:[1, 128, 240, 320];<br/><NewLine>—output  TensorList;<br/><NewLine>OP total_time :15us<br/><NewLine>—RUNNING 4105 OP 685 # %input189.1 : Tensor = aten::cat(%1264, %8)<br/><NewLine>—input  TensorList;Int;<br/><NewLine>—output  Tensor:[1, 512, 240, 320];<br/><NewLine>OP total_time :<strong>281129us</strong></p><NewLine></blockquote><NewLine><p>I followed the offical quantization document used nn.quantized.FloatFunctional(), and call FloatFunctional.cat to concate all my tensors into one.<br/><NewLine>I wonder why the quantized::cat running time is much slower than the dequantized one.<br/><NewLine>I could dequant all my tensor first, and use torch.cat, which will save running time on concatation. But, Since all my tensor’s size is too large, I cannot afford to dequant all tensors first, which will make the running time even slower.</p><NewLine><p>I’m using torch==1.3.1, torchvision==0.4.2</p><NewLine><p>Thanks in Advance.</p><NewLine></div>",https://discuss.pytorch.org/u/meis725,(Meis725),meis725,"February 5, 2020,  8:48am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/zafar"">@Zafar</a> can you take a look?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Did you find any workaround or fix to this problem? We are also facing speed issue, when Squeezenet is quantized. Speed of quantized Squeezenet is less than the speed of FP32 model on android device. Squeezenet is using ‘Concat’ operation at multiple places.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is because torch quantized concat op just piggy back to FP32 by</p><NewLine><pre><code class=""lang-auto"">dequnatize all inputs -&gt; do concat in FP32 -&gt; quantize concatenated tensor<NewLine></code></pre><NewLine><p>So it will never be faster than FP32 concat. See the implementation<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/f6c46df856d0588360db5b807960d1fc5e888c36/aten/src/ATen/native/quantized/cpu/qconcat.cpp#L61-L66"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/f6c46df856d0588360db5b807960d1fc5e888c36/aten/src/ATen/native/quantized/cpu/qconcat.cpp#L61-L66"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/f6c46df856d0588360db5b807960d1fc5e888c36/aten/src/ATen/native/quantized/cpu/qconcat.cpp#L61-L66</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""61"" style=""counter-reset: li-counter 60 ;""><NewLine><li>  xs.push_back(qx.dequantize());</li><NewLine><li>}</li><NewLine><li>const Tensor y = at::cat(xs, dim);</li><NewLine><li>Tensor qy;</li><NewLine><li>AT_DISPATCH_QINT_TYPES(x_dtype, ""qcat"", [&amp;]() {</li><NewLine><li>  qy = at::quantize_per_tensor(y, scale, zero_point, SCALAR_TYPE);</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>There are other operators that follow the same FP32 fallback approach (hence slower than FP32) such as quantized elemwise add, mul etc.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/vgsprasad; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/masahi; <NewLine> ,"REPLY_DATE 1: February 14, 2020,  6:51pm; <NewLine> REPLY_DATE 2: February 17, 2020,  5:42am; <NewLine> REPLY_DATE 3: February 27, 2020,  8:39am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
68096,RuntimeError: Unimplemented backend QuantizedCPU,2020-01-30T07:45:12.181Z,0,209,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tried to check the type of quantized tensor for testing! I have CNN model which I convert to quantized form. When I simply print the weights it shows dtype as qint8. But when I tired this <code>tensor.type()</code> , It throws this error…</p><NewLine><pre><code class=""lang-auto"">File ""test.py"", line 321, in test<NewLine>    print((am['ConvNet.conv4_1.weight'].type()))<NewLine>RuntimeError: Unimplemented backend QuantizedCPU<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Ashish_Gupta1,(Ashish Gupta),Ashish_Gupta1,"January 30, 2020,  7:45am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah, we probably haven’t implement this method… feel free to submit a patch <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 14, 2020, 10:21pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
68203,AssertionError: torch.nn.quantized.ReLU does not support inplace,2020-01-31T07:14:54.719Z,0,155,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I tries to set relu in qconfig in prepare method of quantisation and came across this errors during .convert</p><NewLine><pre><code class=""lang-auto"">args=args=[criterion,evaluation_loader, converter, opt, True]<NewLine>            self.validation(fuse_m,args)<NewLine>            <NewLine>            # Convert to Quantize Model<NewLine>            quantise_model=torch.quantization.convert(fuse_m, inplace=True)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">Traceback (most recent call last):<NewLine>  File ""test_5step_quant.py"", line 474, in &lt;module&gt;<NewLine>    m.model(""//media/ai/OCR_DATA/IIIT5k"",15)<NewLine>  File ""test_5step_quant.py"", line 448, in model<NewLine>    self.test(opt)<NewLine>  File ""test_5step_quant.py"", line 315, in test<NewLine>    quantise_model=torch.quantization.convert(fuse_m, inplace=False)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/quantization/quantize.py"", line 293, in convert<NewLine>    convert(mod, mapping, inplace=True)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/quantization/quantize.py"", line 293, in convert<NewLine>    convert(mod, mapping, inplace=True)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/quantization/quantize.py"", line 294, in convert<NewLine>    reassign[name] = swap_module(mod, mapping)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/quantization/quantize.py"", line 316, in swap_module<NewLine>    new_mod = mapping[type(mod)].from_float(mod)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/nn/quantized/modules/activation.py"", line 44, in from_float<NewLine>    return ReLU(mod.inplace)<NewLine>  File ""/home/remote/.local/lib/python3.6/site-packages/torch/nn/quantized/modules/activation.py"", line 34, in __init__<NewLine>    assert not inplace, 'torch.nn.quantized.ReLU does not support inplace'<NewLine>AssertionError: torch.nn.quantized.ReLU does not support inplace<NewLine><NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Ashish_Gupta1,(Ashish Gupta),Ashish_Gupta1,"January 31, 2020,  7:15am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>should be fixed in <a href=""https://github.com/pytorch/pytorch/pull/33105"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/33105</a>, cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: April 20, 2020,  5:57pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
68491,Quantized convolution and NHWC layout,2020-02-03T10:13:51.200Z,0,232,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m confused with the comment in FbgemmConv function below:<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/ecbf6f99e6a4e373105133b31534c9fb50f2acca/aten/src/ATen/native/quantized/cpu/qconv.cpp#L268-L276"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/ecbf6f99e6a4e373105133b31534c9fb50f2acca/aten/src/ATen/native/quantized/cpu/qconv.cpp#L268-L276"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/ecbf6f99e6a4e373105133b31534c9fb50f2acca/aten/src/ATen/native/quantized/cpu/qconv.cpp#L268-L276</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""268"" style=""counter-reset: li-counter 267 ;""><NewLine><li>// Quantized kernels are all written with NHWC (channels last) layout in</li><NewLine><li>// mind. Ideally, we'd be compatible with conv2d behavior and preserve the</li><NewLine><li>// inputs layout as is (doing necessary upconversions).</li><NewLine><li>//</li><NewLine><li>// However, to be more robust, for now we just force output layout to always</li><NewLine><li>// be NHWC (channels last), thus opportunistically improving perf.</li><NewLine><li>//</li><NewLine><li>// This might change when full memory format support lands</li><NewLine><li>// See https://github.com/pytorch/pytorch/issues/23403</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>It says quantized convolution operates on NHWC layout and an output are kept in NHWC. But since the input is assumed to be in NCHW layout, there has to be a conversion step from NHWC back to NCHW. Where does this layout conversion happens?</p><NewLine><p>I’m comparing outputs of quantized convolution in Pytorch and the same operations translated to and executed on TVM (via TVM’s WIP torch frontend). Even though the operations are just quantize, qconv, and dequantize, the two results doesn’t match. I’m trying to figure out where the difference comes from.</p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"February 3, 2020, 10:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> .</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the logical layout is still NHWC, using memory_format we are re-arranging the physical memory format so that it will help kernels implementations.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 3, 2020,  9:51pm; <NewLine> REPLY_DATE 2: February 14, 2020,  7:25pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
59113,RuntimeError: No function is registered for schema aten::thnn_conv2d_forward,2019-10-24T10:05:32.659Z,4,697,"<div class=""post"" itemprop=""articleBody""><NewLine><p>1, I fuse conv2d and batchnorm2d<br/><NewLine>2,Specify the configuration of the quantization methods<br/><NewLine>3, Use the [ <code>torch.quantization.prepare()</code> ]<br/><NewLine>4,Calibrate the model by running inference against a calibration dataset<br/><NewLine>but it comes to setp 4, i got this<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/3d425393bb6b39623f6fc2dd5d023c06de8d067a"" href=""https://discuss.pytorch.org/uploads/default/original/3X/3/d/3d425393bb6b39623f6fc2dd5d023c06de8d067a.png"" title=""111.png""><img alt=""111"" data-base62-sha1=""8JVeeeJTx49fERYOwuutmZENeJ4"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/3/d/3d425393bb6b39623f6fc2dd5d023c06de8d067a_2_10x10.png"" height=""149"" src=""https://discuss.pytorch.org/uploads/default/original/3X/3/d/3d425393bb6b39623f6fc2dd5d023c06de8d067a.png"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">111.png</span><span class=""informations"">917×199 31.6 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p>I don’t know why the code runs with torch.nn.modules.conv instead of torch.quantized<br/><NewLine>That`s the code:</p><NewLine><blockquote><NewLine><p>model.to(‘cpu’)<br/><NewLine>model.eval()<br/><NewLine>model.qconfig = torch.quantization.get_default_qconfig(‘fbgemm’)<br/><NewLine>torch.quantization.prepare(model, inplace=True)<br/><NewLine>evaluate(model, data_loader_test, 10)<br/><NewLine>torch.quantization.convert(model, inplace=True)<br/><NewLine>top1, top5, loss = evaluate(model, data_loader_test)   —&gt; WRONG</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/jiacheng1gujiaxin,(Jiacheng1gujiaxin),jiacheng1gujiaxin,"October 24, 2019, 10:06am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""59113""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/j/f17d59/40.png"" width=""20""/> jiacheng1gujiaxin:</div><NewLine><blockquote><NewLine><p>4,Calibrate the model by running inference against a calibration dataset</p><NewLine></blockquote><NewLine></aside><NewLine><p>Did you add QuantStub and DeQuantStub correctly in the original model? Please follow <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> to do quantization.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello! I’m just following the tutorial to quantize, and I got the same error.<br/><NewLine><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/a9002a812edf06e6755b08657c598ca5616079b4"" href=""https://discuss.pytorch.org/uploads/default/original/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4.png"" title=""image""><img alt=""image"" data-base62-sha1=""o732ZoTqeHrslcn9GpSNlB0NALG"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4_2_10x10.png"" height=""168"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4_2_690x168.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4_2_690x168.png, https://discuss.pytorch.org/uploads/default/optimized/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4_2_1035x252.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/a/9/a9002a812edf06e6755b08657c598ca5616079b4_2_1380x336.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">image</span><span class=""informations"">1834×449 101 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div><br/><NewLine>I try to use a simple Conv + BN + ReLU like this</p><NewLine><pre><code class=""lang-auto"">class ConvBNReLU(nn.Sequential):<NewLine>	def __init__(self, in_channel, out_channel, kernel_size, stride):<NewLine>		padding = (kernel_size - 1) // 2<NewLine>		super(ConvBNReLU, self).__init__(<NewLine>			nn.Conv2d(<NewLine>				in_channel,<NewLine>				out_channel,<NewLine>				kernel_size,<NewLine>				stride,<NewLine>				padding,<NewLine>				bias=False<NewLine>			),<NewLine>			nn.BatchNorm2d(out_channel),<NewLine>			nn.ReLU(inplace=False)<NewLine>		)<NewLine><NewLine>class CNN(nn.Module):<NewLine>	def __init__(self):<NewLine>		super(CNN, self).__init__()<NewLine>		# input size 3 * 32 * 32<NewLine>		self.conv1 = ConvBNReLU(3, 16, 3, 1)<NewLine><NewLine>		self.conv2 = ConvBNReLU(16, 32, 3, 1)<NewLine><NewLine>		self.quant = QuantStub()<NewLine>		self.dequant = DeQuantStub()<NewLine><NewLine>		self.out = nn.Linear(32 * 32 * 32, 10)<NewLine><NewLine>	def forward(self, x):<NewLine>		x = self.quant(x)<NewLine>		x = self.conv1(x)<NewLine>		x = self.conv2(x)<NewLine>		x = x.contiguous()<NewLine>		x = x.view(-1, 32 * 32 * 32)<NewLine>		x = self.out(x)<NewLine>		x = self.dequant(x)<NewLine>		return x<NewLine><NewLine>	def fuse_model(self):<NewLine>		for m in self.modules():<NewLine>			if type(m) == ConvBNReLU:<NewLine>				torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)<NewLine></code></pre><NewLine><p>model after adding observer</p><NewLine><pre><code class=""lang-auto"">ConvBNReLU(<NewLine>  (0): ConvBnReLU2d(<NewLine>    (0): Conv2d(<NewLine>      3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>      (observer): MinMaxObserver(min_val=None, max_val=None)<NewLine>    )<NewLine>    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (2): ReLU(<NewLine>      (observer): MinMaxObserver(min_val=None, max_val=None)<NewLine>    )<NewLine>  )<NewLine>  (1): Identity()<NewLine>  (2): Identity()<NewLine>)<NewLine></code></pre><NewLine><p>model after quantized convert</p><NewLine><pre><code class=""lang-auto"">ConvBNReLU(<NewLine>  (0): ConvBnReLU2d(<NewLine>    (0): Conv2d(<NewLine>      3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>      (observer): MinMaxObserver(min_val=-3.1731998920440674, max_val=3.2843430042266846)<NewLine>    )<NewLine>    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (2): ReLU(<NewLine>      (observer): MinMaxObserver(min_val=0.0, max_val=13.862381935119629)<NewLine>    )<NewLine>  )<NewLine>  (1): Identity()<NewLine>  (2): Identity()<NewLine>)<NewLine></code></pre><NewLine><p>The remaining works are just following the tutorial.<br/><NewLine>I wonder why the layers did not be converted to quantized layer.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We can’t quantize batchnorm, you’ll need to fuse batchnorm layer first, please follow the tutorial(<a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture</a>) to fuse the conv and batchnorm relu first. You can also take a look at our test to see how to use the fusion API: <a href=""https://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L949"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L949</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello Jerry, thanks for reply!<br/><NewLine>I have check the test doing fusion, but I still have some quesion<br/><NewLine>Here is my model before fused</p><NewLine><pre><code class=""lang-auto"">CNN(<NewLine>  (conv1): ConvBNReLU(<NewLine>    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (2): ReLU()<NewLine>  )<NewLine>  (conv2): ConvBNReLU(<NewLine>    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>    (2): ReLU()<NewLine>  )<NewLine>  (quant): QuantStub()<NewLine>  (dequant): DeQuantStub()<NewLine>  (out): Linear(in_features=32768, out_features=10, bias=True)<NewLine>)<NewLine><NewLine></code></pre><NewLine><p>And here is after fused</p><NewLine><pre><code class=""lang-auto"">CNN(<NewLine>  (conv1): ConvBNReLU(<NewLine>    (0): ConvBnReLU2d(<NewLine>      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>    )<NewLine>    (1): Identity()<NewLine>    (2): Identity()<NewLine>  )<NewLine>  (conv2): ConvBNReLU(<NewLine>    (0): ConvBnReLU2d(<NewLine>      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (2): ReLU()<NewLine>    )<NewLine>    (1): Identity()<NewLine>    (2): Identity()<NewLine>  )<NewLine>  (quant): QuantStub()<NewLine>  (dequant): DeQuantStub()<NewLine>  (out): Linear(in_features=32768, out_features=10, bias=True)<NewLine>)<NewLine><NewLine></code></pre><NewLine><p>It seems like conv BN ReLU have been converted to ConvBNReLU2d, did I miss anything?<br/><NewLine>I’m doing with post training, is it only conv ReLU can be quantized?</p><NewLine><p>And the model in section “Define dataset and data loaders”(<a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#define-dataset-and-data-loaders"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#define-dataset-and-data-loaders</a>)<br/><NewLine>I am confused why the model before fusion is conv BN ReLU, but after fusion it turned out only ConvReLU2d.<br/><NewLine>Did it remove BN while doing fusion?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""5"" data-topic=""59113""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/ea5d25/40.png"" width=""20""/> PhoneJam:</div><NewLine><blockquote><NewLine><p>I’m doing with post training, is it only conv ReLU can be quantized?</p><NewLine></blockquote><NewLine></aside><NewLine><p>the fusion result looks correct.<br/><NewLine>Yes, fusion means we are going to fuse parameters of batchnorm into conv since batchnorm is just a linear transformation of the input at inference time. see <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/fusion.py#L7"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/fusion.py#L7</a> for code to fuse conv and bn.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I found that I forgot to change my model into evaluate mode.<br/><NewLine>It works now!<br/><NewLine>Thanks for help!</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to fuse nn.BatchNorm1d(512)  with Linear and Relu ?</p><NewLine><p>I am getting the following error:<br/><NewLine>NotImplementedError: Cannot fuse modules: (&lt;class ‘torch.nn.modules.linear.Linear’&gt;, &lt;class ‘torch.nn.modules.batchnorm.BatchNorm1d’&gt;, &lt;class ‘torch.nn.modules.activation.ReLU’&gt;)</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""59113"" data-username=""PhoneJam""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/ea5d25/40.png"" width=""20""/> PhoneJam:</div><NewLine><blockquote><NewLine><p>evaluate mode.<br/><NewLine>It works now!<br/><NewLine>Thanks for help!</p><NewLine></blockquote><NewLine></aside><NewLine><aside class=""quote no-group"" data-post=""7"" data-topic=""59113"" data-username=""PhoneJam""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/ea5d25/40.png"" width=""20""/> PhoneJam:</div><NewLine><blockquote><NewLine><p>evaluate</p><NewLine></blockquote><NewLine></aside><NewLine><p>No, we only have fusion between conv and batchnorm right now</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/PhoneJam; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/PhoneJam; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/PhoneJam; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/remya; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: November 1, 2019,  6:10pm; <NewLine> REPLY_DATE 2: December 17, 2019,  7:46am; <NewLine> REPLY_DATE 3: December 18, 2019, 12:12am; <NewLine> REPLY_DATE 4: December 18, 2019,  8:46am; <NewLine> REPLY_DATE 5: December 18, 2019,  7:13pm; <NewLine> REPLY_DATE 6: December 19, 2019,  3:31am; <NewLine> REPLY_DATE 7: February 4, 2020, 10:58am; <NewLine> REPLY_DATE 8: February 14, 2020,  6:50pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: 1 Like; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
67734,When quantized::max_pool2d is used?,2020-01-26T23:10:35.306Z,0,137,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In my IR that came from the quantized resnet model from torchvision, I have one max pool op which operates on quantized tensors:</p><NewLine><pre><code class=""lang-auto"">  %input.3 : QUInt8(1, 64, 56, 56) = aten::max_pool2d(%input.2, %1279, %1282, %1285, %1288, %1289), scope: __module.maxpool # /home/masa/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:488:0<NewLine></code></pre><NewLine><p>Does this dispatch into <code>quantized::max_pool2d</code> below?<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qpool.cpp#L414"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qpool.cpp#L414"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qpool.cpp#L414</a></h4><NewLine><pre class=""onebox""><code class=""lang-cpp""><ol class=""start lines"" start=""404"" style=""counter-reset: li-counter 403 ;""><NewLine><li>    #ifdef USE_PYTORCH_QNNPACK</li><NewLine><li>    if (at::globalContext().qEngine() == at::QEngine::QNNPACK &amp;&amp; qx.scalar_type() == kQUInt8) {</li><NewLine><li>      return qnnpack_maxpool(qx, kernel_size, stride, padding, dilation, ceil_mode);</li><NewLine><li>    }</li><NewLine><li>    #endif</li><NewLine><li>    return at::max_pool2d(qx, kernel_size, stride, padding, dilation, ceil_mode);</li><NewLine><li>  }</li><NewLine><li>};</li><NewLine><li><NewLine></li><NewLine><li>static auto registry = torch::RegisterOperators().op(</li><NewLine><li class=""selected"">    ""quantized::max_pool2d(Tensor qx, ""</li><NewLine><li>    ""int[] kernel_size, ""</li><NewLine><li>    ""int[] stride, ""</li><NewLine><li>    ""int[] padding, ""</li><NewLine><li>    ""int[] dilation,""</li><NewLine><li>    ""bool ceil_mode) -&gt; Tensor"",</li><NewLine><li>    torch::RegisterOperators::options().kernel&lt;QMaxPool2D_arr_args&gt;(</li><NewLine><li>        DispatchKey::QuantizedCPUTensorId));</li><NewLine><li><NewLine></li><NewLine><li>} // namespace</li><NewLine><li>} // namespace native</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>Or more generally, I want to know about aten dispatch mechanism. Any pointer is appreciated.</p><NewLine><p>cc <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a></p><NewLine></div>",https://discuss.pytorch.org/u/masahi,,masahi,"January 26, 2020, 11:10pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, that is correct. it is dispatch here: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Pooling.cpp#L128"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Pooling.cpp#L128</a><br/><NewLine>We have multiple ways to do dispatch right now in PyTorch, one common place is in native_functions.yaml, you can take a look at: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: February 14, 2020, 10:49pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
58995,AssertionError: min nan should be less than max nan,2019-10-23T07:23:39.379Z,0,415,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you. After I merged conv and batchnorm. I have solved this problem, but I have encountered this problem in training - aware quantification. Do you have any suggestions?</p><NewLine><blockquote><NewLine><p>File “/home/g/anaconda3/lib/python3.7/site-packages/torch/quantization/observer.py”, line 165, in _calculate_qparams<br/><NewLine>zero_point = qmin - round(min_val / scale)<br/><NewLine>ValueError: cannot convert float NaN to integer</p><NewLine></blockquote><NewLine><p>Is there a problem with my data?</p><NewLine><p>Need to add exception capture?</p><NewLine><p>It was later found that Nan existed in conv2d.weight.<br/><NewLine>And that’s happened in the process of training.</p><NewLine></div>",https://discuss.pytorch.org/u/jiacheng1gujiaxin,(Jiacheng1gujiaxin),jiacheng1gujiaxin,"October 23, 2019,  7:23am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If your weights got a NaN value, this might be due to a NaN input or a faulty weight update caused by e.g. a high learning rate.<br/><NewLine>Did you observe the loss during training?<br/><NewLine>If some weights are exploding, you would usually see a NaN loss.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I have the similar problem when model use the training - aware quantification and the training loss is not nan. How to solve it?<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 540, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “model-code/resnet34/train_age_quan.py”, line 1130, in forward<br/><NewLine>x = self.ConvBNReLU1(x)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 540, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/container.py”, line 100, in forward<br/><NewLine>input = module(input)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 540, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py”, line 243, in forward<br/><NewLine>return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py”, line 95, in _forward<br/><NewLine>conv = self._conv_forward(input, self.weight_fake_quant(scaled_weight))<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 540, in <strong>call</strong><br/><NewLine>result = self.forward(*input, **kwargs)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/quantization/fake_quantize.py”, line 81, in forward<br/><NewLine>self.scale, self.zero_point = self.calculate_qparams()<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/quantization/fake_quantize.py”, line 76, in calculate_qparams<br/><NewLine>return self.activation_post_process.calculate_qparams()<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/quantization/observer.py”, line 481, in calculate_qparams<br/><NewLine>return self._calculate_per_channel_qparams(self.min_vals, self.max_vals)<br/><NewLine>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/quantization/observer.py”, line 150, in _calculate_per_channel_qparams<br/><NewLine>), “min {} should be less than max {}”.format(min_vals[i], max_vals[i])<br/><NewLine>AssertionError: min nan should be less than max nan</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""4"" data-topic=""58995"" data-username=""chihyu""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/c/4bbf92/40.png"" width=""20""/> chihyu:</div><NewLine><blockquote><NewLine><p>File “/mnt/storage1/doris/miniconda3/envs/torch-nightly-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py”, line 540, in <strong>call</strong></p><NewLine></blockquote><NewLine></aside><NewLine><p>can you check if values in weights/activations contains nan?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/chihyu; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: October 24, 2019,  9:16am; <NewLine> REPLY_DATE 2: February 5, 2020,  3:41pm; <NewLine> REPLY_DATE 3: February 14, 2020,  6:33pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
67334,Pretrained quantized models&rsquo; export to ONNX fails,2020-01-22T05:07:53.444Z,0,523,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am trying to export the pretrained quantized models to ONNX, but it fails. I’ve tried for GoogleNet, ResNet18 and Mobilenet v2, but none of those exported.</p><NewLine><p>I got the following error for GoogleNet and ResNet18</p><NewLine><p>RuntimeError: self.qscheme() == at::kPerTensorAffine INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/quantized/QTensor.cpp:162, please report a bug to PyTorch. clone for quantized Tensor only works for PerTensorAffine scheme right now</p><NewLine></div>",https://discuss.pytorch.org/u/Zeenath_Shaikh,(Zeenath Shaikh),Zeenath_Shaikh,"January 22, 2020,  5:07am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>did you find a solution for this?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey, i’ve changed qconfig to torch.quantization.get_default_qconfig(‘qnnpack’) and error RuntimeError: self.qscheme() == at::kPerTensorAffine INTERNAL ASSERT FAILED is not an issue any more. This is just workaround.</p><NewLine><p>Now i have an error KeyError: ‘conv2d_relu’ after small changed.</p><NewLine><p>I fused layers for static quantization and it seems onnx doesn’t support fused layers in this format.  <a href=""https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv"" rel=""nofollow noopener"">https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv</a></p><NewLine><p>This key doesn’t exist in symbolic_opset (any) so i don’t expect it would work.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>We probably need a patch to support clone for per channel quantized tensor: <a href=""https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/QTensor.cpp#L160"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/QTensor.cpp#L160</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>created an issue: <a href=""https://github.com/pytorch/pytorch/issues/33309"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/33309</a><br/><NewLine>I’ll fix it when I have time, but feel free to submit a PR as well and I’m happy to review it.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ggeor; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/stihl1210; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: January 29, 2020,  9:55pm; <NewLine> REPLY_DATE 2: February 10, 2020, 11:23pm; <NewLine> REPLY_DATE 3: February 13, 2020,  8:28pm; <NewLine> REPLY_DATE 4: February 13, 2020,  8:32pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
68630,8 bit quantization - modulo 256 inside convolutions?,2020-02-04T11:56:55.927Z,0,126,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I am trying to understand how the PyTorch multiplications on 8 bits work when a model is quantized. Inside each convolution, do you do modulo 256 operations, which means keeping the LSB?</p><NewLine><p>I tried to do simple ByteTensor operations and I saw that the LSBs are kept always. But I need to know if it is the same inside a quantized model.</p><NewLine><p>Thanks a lot.<div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/bb1f9949fb550514f0ebd0c462a125e5147a71b5"" href=""https://discuss.pytorch.org/uploads/default/original/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5.jpeg"" title=""IMG_20200131_152514""><img alt=""IMG_20200131_152514"" data-base62-sha1=""qHmZcfLtNmecEv58RenyeY9V9Z3"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5_2_10x10.png"" height=""500"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5_2_375x500.jpeg"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5_2_375x500.jpeg, https://discuss.pytorch.org/uploads/default/optimized/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5_2_562x750.jpeg 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/b/b/bb1f9949fb550514f0ebd0c462a125e5147a71b5_2_750x1000.jpeg 2x"" width=""375""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">IMG_20200131_152514</span><span class=""informations"">2976×3968 237 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/dassima,(Dassima),dassima,"February 5, 2020,  9:58am",,,,,
68333,Creat a tensor with random number of 1 and -1,2020-02-01T11:29:24.121Z,1,84,"<div class=""post"" itemprop=""articleBody""><NewLine><p>In order to add noise to the XNOR-Net, I need to modify the trained weights which contains only 1 and -1. So I think the problem is how to  generate a tensor with random number of 1 and -1, and then multiply this tensor with the trained weights. The solution of mine is following:</p><NewLine><pre><code class=""lang-auto""> def add_noise_to_weights(m):<NewLine>            s = m.data.size()<NewLine>            n = m.data.nelement()<NewLine>            r = round(n*0.01) #0.01 is the noise ratio<NewLine>            mask = -torch.ones([1, r]).type_as(m.data)<NewLine>            neg_mask = torch.ones([1, n-r]).type_as(m.data)<NewLine>            t = torch.cat((mask, neg_mask),1).reshape(s)<NewLine>            idx = torch.randperm(t.nelement())<NewLine>            t = t.view(-1)[idx].view(t.size())<NewLine>            m.data = m.data.mul(t)<NewLine>           return m<NewLine></code></pre><NewLine><p>Despite it works, the code is too complicated, so could you guys have some simply solutions?</p><NewLine></div>",https://discuss.pytorch.org/u/mingren200323,(Mingren200323),mingren200323,"February 1, 2020, 11:30am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>If I understand correctly, you just want to generate a tensor which contains -1 and 1 values ?</p><NewLine><p>Would something like this work for you ?</p><NewLine><pre><code class=""lang-auto"">a = torch.Tensor([-1, 1])<NewLine>idx = np.random.randint(2, size=your_shape)<NewLine>noise = a[idx]<NewLine></code></pre><NewLine><p>Edit :<br/><NewLine>Furthermore, if you want to have some control on the number of 1 or -1 in your tensor, I would suggest:</p><NewLine><pre><code class=""lang-auto"">a = torch.Tensor([-1, 1])<NewLine>idx = np.random.choice(2, size=your_shape, p=[r, 1-r])<NewLine>noise = a[idx]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""3"" data-topic=""68333""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/tux/40/2364_2.png"" width=""20""/> tux:</div><NewLine><blockquote><NewLine><p>idx = np.random.choice(2, size=your_shape, p=[r, 1-r])</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thank you very much for your reply! Based on your reply,I think this may be the right  answer to my question.</p><NewLine><pre><code class=""lang-auto"">s = x.data.size()<NewLine>t = torch.from_numpy(numpy.random.choice([-1, 1], size=s, p=[0.5, 0.5])).type_as(x)<NewLine>x = x.data.mul(t)<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tux; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mingren200323; <NewLine> ,"REPLY_DATE 1: February 1, 2020,  3:57pm; <NewLine> REPLY_DATE 2: February 1, 2020,  1:08pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> 
63578,How does PyTorch implement Quantization?,2019-12-10T06:14:36.364Z,8,899,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello, I want to know how does PyTorch implement quantization without rewrite the layer definition.<br/><NewLine>Actually, because of certain reasons, I have to write a quantization framework with PyTorch for my project. Here is an simple example of my implementaion:</p><NewLine><pre><code class=""lang-python"">def quantization_decorator(func):<NewLine>    @wraps(func)<NewLine>    def wrapper(_, x):<NewLine>        do_quantization(x)<NewLine>        x = func(x)<NewLine>        do_quantization(x)<NewLine>        return x<NewLine>    return wrapper<NewLine><NewLine>class Conv2d(nn.Conv2d):<NewLine>    def __init__(self, *args, **kwargs):<NewLine>        super().__init__(*args, **kwargs)<NewLine>    <NewLine>    @quantization_decorator<NewLine>    def forward(self, x):<NewLine>        super().forward(x)<NewLine></code></pre><NewLine><p>Since my quantization method is defined as a decorator,<br/><NewLine><strong>ADWANTAGE</strong>: I can add it to <code>forward</code> to choose whether to quantize this module.<br/><NewLine><strong>DISAWANTAGE</strong>: I have to rewrite each layer, e.g. <code>class Conv2d</code> in my example.</p><NewLine><p>I want to know how does PyTorch implement quantization, so that I can optimize my framework (simpler, faster and more customizable).</p><NewLine><p>Thank you.</p><NewLine></div>",https://discuss.pytorch.org/u/Eta_C,,Eta_C,"December 10, 2019,  6:14am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><ul><NewLine><li>My quantization method is different from PyTorch.</li><NewLine><li>My custom layers have to be supported.</li><NewLine></ul><NewLine><p>That’s why I have to write a new implementation.<img alt="":see_no_evil:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/see_no_evil.png?v=9"" title="":see_no_evil:""/></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/eta_c"">@Eta_C</a>,</p><NewLine><p>Please look at the flow of operation for quantization here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#post-training-static-quantization"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#post-training-static-quantization</a></p><NewLine><p>The main steps for post training quantization are: 1) fusing of modules (e.g., conv; bn; relu =&gt; conv_bn_relu) 2) Observing tensor values to quantize tensors 3) Actual replacing of modules from float to quantized.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I know how to use PyTorch quantization tools.<br/><NewLine>I want to know how does PyTorch implement it.<br/><NewLine>OK, maybe I have to read some source code about it…</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes. Reading source code on how it’s implemented is a good way. Here are some pointers for the code: Python related: <a href=""https://github.com/pytorch/pytorch/tree/master/torch/quantization"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/torch/quantization</a><br/><NewLine>C++ kernels: <a href=""https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu</a></p><NewLine><p>You may also want to check the recent changes to these file and comments on related PRs to get some context.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you.<br/><NewLine>I will take the time to read the source code carefully.<img alt="":grinning:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/grinning.png?v=9"" title="":grinning:""/></p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to quantize a vgg model , following the steps given in <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><p>I did the first step as follows:</p><NewLine><p>import torch.nn as nn<br/><NewLine><span class=""hashtag"">#from</span> .utils import load_state_dict_from_url</p><NewLine><p><strong>all</strong> = [<br/><NewLine>‘Cifar_VGG’, ‘cifar_rvgg11’, ‘cifar_rvgg11_bn’, ‘cifar_vgg11’, ‘cifar_vgg11_bn’, ‘cifar_vgg13’, ‘cifar_vgg13_bn’, ‘cifar_vgg16’, ‘cifar_vgg16_bn’,<br/><NewLine>‘cifar_vgg19_bn’, ‘cifar_vgg19’,<br/><NewLine>]</p><NewLine><p>model_urls = {</p><NewLine><pre><code>'cifar_vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',<NewLine></code></pre><NewLine><p>}</p><NewLine><p>class Cifar_VGG(nn.Module):</p><NewLine><pre><code>def __init__(self, features, num_classes=1000, init_weights=True):<NewLine>    super(Cifar_VGG, self).__init__()<NewLine>    self.features = features<NewLine>    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))<NewLine>    self.quant = QuantStub()<NewLine>    self.dequant = DeQuantStub()<NewLine>    self.classifier = nn.Sequential(<NewLine>        nn.Linear(512, 512),<NewLine>        nn.BatchNorm1d(512),<NewLine>        nn.ReLU(True),<NewLine>        nn.Linear(512, num_classes),<NewLine>    )<NewLine>    if init_weights:<NewLine>        self._initialize_weights()<NewLine><NewLine>def forward(self, x):<NewLine>    x = self.quant(x)<NewLine>    x = self.features(x)<NewLine>    x = self.avgpool(x)<NewLine>    x = x.view(x.size(0), -1)<NewLine>    x = self.classifier(x)<NewLine>    x = self.dequant(x)<NewLine><NewLine>    return x<NewLine><NewLine>def _initialize_weights(self):<NewLine>    for m in self.modules():<NewLine>        if isinstance(m, nn.Conv2d):<NewLine>            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')<NewLine>            if m.bias is not None:<NewLine>                nn.init.constant_(m.bias, 0)<NewLine>        elif isinstance(m, nn.BatchNorm2d):<NewLine>            nn.init.constant_(m.weight, 1)<NewLine>            nn.init.constant_(m.bias, 0)<NewLine>        elif isinstance(m, nn.Linear):<NewLine>            nn.init.normal_(m.weight, 0, 0.01)<NewLine>            nn.init.constant_(m.bias, 0)<NewLine><NewLine>def fuse_model(self):<NewLine>    torch.quantization.fuse_modules(self, [['conv2d', 'BatchNorm2d', 'ReLU'],<NewLine>                                        ],inplace=True)<NewLine></code></pre><NewLine><p>def make_layers(cfg, batch_norm=False):<br/><NewLine>layers = []<br/><NewLine>in_channels = 3<br/><NewLine>for v in cfg:<br/><NewLine>if v == ‘M’:<br/><NewLine>layers += [nn.MaxPool2d(kernel_size=2, stride=2)]<br/><NewLine>else:<br/><NewLine>conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)<br/><NewLine>if batch_norm:<br/><NewLine>layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]<br/><NewLine>else:<br/><NewLine>layers += [conv2d, nn.ReLU(inplace=True)]<br/><NewLine>in_channels = v<br/><NewLine>return nn.Sequential(*layers)</p><NewLine><p>cfgs = {<br/><NewLine>‘RA’:[512,‘M’, 512, ‘M’, 512, 512, ‘M’, 512, 512, ‘M’, 512, 512, ‘M’],<br/><NewLine>‘A’: [64, ‘M’, 128, ‘M’, 256, 256, ‘M’, 512, 512, ‘M’, 512, 512, ‘M’],<br/><NewLine>‘B’: [64, 64, ‘M’, 128, 128, ‘M’, 256, 256, ‘M’, 512, 512, ‘M’, 512, 512, ‘M’],<br/><NewLine>‘D’: [64, 64, ‘M’, 128, 128, ‘M’, 256, 256, 256, ‘M’, 512, 512, 512, ‘M’, 512, 512, 512, ‘M’],<br/><NewLine>‘E’: [64, 64, ‘M’, 128, 128, ‘M’, 256, 256, 256, 256, ‘M’, 512, 512, 512, 512, ‘M’, 512, 512, 512, 512, ‘M’],<br/><NewLine>}</p><NewLine><p>def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):<br/><NewLine>if pretrained:<br/><NewLine>kwargs[‘init_weights’] = False<br/><NewLine>model = Cifar_VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)<br/><NewLine>if pretrained:<br/><NewLine>state_dict = load_state_dict_from_url(model_urls[arch],<br/><NewLine>progress=progress)<br/><NewLine>model.load_state_dict(state_dict)<br/><NewLine>return model</p><NewLine><p>def cifar_rvgg11_bn(pretrained=False, progress=True, **kwargs):<br/><NewLine>return _vgg(‘cifar_rvgg11’, ‘RA’, True, pretrained, progress, **kwargs)</p><NewLine><p>net.fuse_model()<br/><NewLine>Gives the following error</p><NewLine><p>‘Cifar_VGG’ object has no attribute ‘conv2d’</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think the fusion is not defined correctly for your model, please read the fusion section of the tutorial again and see if you can find the problem</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Now I am getting the following error<br/><NewLine>RuntimeError: No function is registered for schema aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, CUDATensorId, MkldnnCPUTensorId, VariableTensorId</p><NewLine></div>; <NewLine> REPLY 9: <div class=""post"" itemprop=""articleBody""><NewLine><p>How to merge batch_norm layer with conv layer?</p><NewLine></div>; <NewLine> REPLY 10: <div class=""post"" itemprop=""articleBody""><NewLine><p>Now i am trying ResNet18<br/><NewLine>def fuse_model(self):<br/><NewLine>for m in self.modules():<br/><NewLine>if type(m) == BasicBlock:<br/><NewLine>torch.quantization.fuse_modules(m, [[‘conv1’, ‘bn1’, ‘relu’],[‘conv2’, ‘bn2’]], inplace=True)<br/><NewLine>conv = getattr(m, ‘downsample’)<br/><NewLine>if(conv) :<br/><NewLine>torch.quantization.fuse_modules(conv, [‘0’, ‘1’], inplace=True)</p><NewLine><p>But fusion is not giving any error.<br/><NewLine>But the infernce shows the error (which i posted above)</p><NewLine><p>Got it !!!<br/><NewLine>I missed the following:<br/><NewLine>torch.quantization.fuse_modules(self, [‘conv1’, ‘bn1’, ‘relu’], inplace=True)</p><NewLine><p>Thank you</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Eta_C; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Eta_C; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Eta_C; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/remya; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/remya; <NewLine> REPLIER 9: https://discuss.pytorch.org/u/remya; <NewLine> REPLIER 10: https://discuss.pytorch.org/u/remya; <NewLine> ,"REPLY_DATE 1: December 10, 2019,  6:22am; <NewLine> REPLY_DATE 2: December 10, 2019,  5:23pm; <NewLine> REPLY_DATE 3: December 11, 2019,  2:03am; <NewLine> REPLY_DATE 4: December 14, 2019, 12:47am; <NewLine> REPLY_DATE 5: December 14, 2019,  4:26am; <NewLine> REPLY_DATE 6: January 26, 2020,  4:25pm; <NewLine> REPLY_DATE 7: January 31, 2020, 10:54pm; <NewLine> REPLY_DATE 8: February 1, 2020,  4:44am; <NewLine> REPLY_DATE 9: February 1, 2020,  4:46am; <NewLine> REPLY_DATE 10: February 1, 2020,  7:27am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: 1 Like; <NewLine> REPLY 8 LIKES: ; <NewLine> REPLY 9 LIKES: ; <NewLine> REPLY 10 LIKES: 1 Like; <NewLine> 
60770,Best way to quantize Transformer architecture,2019-11-12T17:54:05.754Z,0,367,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi there,</p><NewLine><p>I’ve been playing around with the experimental quantization features introduced in v1.3, and trying to apply it to some Transformer model.</p><NewLine><p>Dynamic quantization works quite well, speeding up to a factor 1.5-2.5x depending on configurations.</p><NewLine><p>As for static quantization, it seems some parts of the model are not compatible (LayerNorm, some div/matmul in the multi-head attention setup). I’m wondering what would be the best way of tackling such a case.</p><NewLine><ul><NewLine><li>Can we specify some submodules to ignore with Qconfig for instance?</li><NewLine><li>Is there a roadmap to implement the missing operators and modules (matmul, div, nn.LayerNorm, etc.)?</li><NewLine></ul><NewLine><p>Thanks!<br/><NewLine>François</p><NewLine></div>",https://discuss.pytorch.org/u/Francois_Hernandez,(François Hernandez),Francois_Hernandez,"November 12, 2019,  5:54pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Yes, you can set the corresponding module’s qconfig=None to bypass the quantization. If you just want to do it for some instances, you can also do that in the parent module’s init. Also make sure the quant and dequant are set correctly on the boundary.</p><NewLine><p>For more operators support, we are working on enhancing this part. Also feel free to contribute.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I am using a standard out-of-the-box nn.transformer module (it was a choice when 1.2 was released - I did not need BERT, so I decided to opt for a standard module) like this:</p><NewLine><pre><code class=""lang-auto"">layer = nn.TransformerEncoderLayer(d_model=size,<NewLine>                                   nhead=8,<NewLine>                                   dim_feedforward=size * decoder_girth,<NewLine>                                   dropout=dropout)<NewLine>self.decoder = nn.TransformerEncoder(layer, decoder_layers<NewLine></code></pre><NewLine><p>Now I have encountered this error. It is a bit cryptic, but is my assumption correct that the <a href=""https://github.com/pytorch/pytorch/blob/ef5637f85e43dff55b03276176ba2949bd83a085/torch/nn/functional.py#L1362-L1383"" rel=""nofollow noopener"">erroneous</a> function should just be replaced with a <code>nn.module</code> version? Or am I missing sth?</p><NewLine><p>Maybe someone had this sort of error? It is probably going to be fixed in <code>1.5.0</code>, but I would need to monkey patch it somehow now</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/snakers41; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  7:31am; <NewLine> REPLY_DATE 2: January 28, 2020,  2:17pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> 
66413,Can&rsquo;t load model after dynamic quantization,2020-01-11T22:50:55.903Z,1,434,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello!  I’m trying to do dynamic quantization as described <a href=""https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html"" rel=""nofollow noopener"">here</a>.</p><NewLine><p>To quantize my own fine-tuned Bert model, I do this:</p><NewLine><pre><code>model = BertForSequenceClassification.from_pretrained('model_dir')<NewLine>model.to('cpu')<NewLine>quantized_model = torch.quantization.quantize_dynamic(<NewLine>    model, {torch.nn.Linear}, dtype=torch.qint8)<NewLine>quantized_model.save_pretrained('model_dir_q')<NewLine></code></pre><NewLine><p>But when I later load the model, I get this error:</p><NewLine><pre><code class=""lang-auto"">  File ""/.../bert.py"", line 329, in main<NewLine>    model = BertForSequenceClassification.from_pretrained(args.output_dir)<NewLine>  File ""/.../transformers/modeling_utils.py"", line 486, in from_pretrained<NewLine>    model.__class__.__name__, ""\n\t"".join(error_msgs)))<NewLine>RuntimeError: Error(s) in loading state_dict for BertForSequenceClassification:<NewLine>	While copying the parameter named ""bert.encoder.layer.0.attention.self.query.weight"", whose dimensions in the model are torch.Size([768, 768]) and whose dimensions in the checkpoint are torch.Size([768, 768]).<NewLine>	While copying the parameter named ""bert.encoder.layer.0.attention.self.key.weight"", whose dimensions in the model are torch.Size([768, 768]) and whose dimensions in the checkpoint are torch.Size([768, 768]).<NewLine>...<NewLine></code></pre><NewLine><p>Would appreciate any guidance for how to fix this.</p><NewLine></div>",https://discuss.pytorch.org/u/JeffO,,JeffO,"January 11, 2020, 10:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>The error message seems to be at least misleading.<br/><NewLine>Maybe <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a> knows, what’s going on.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I should add that I’m using Torch version 1.3.1 in case it matters.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/jeffo"">@JeffO</a> Is this model available publicly so that we can try the steps locally?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a>, my model isn’t public, but you can recreate the error with just Bert base like this:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>from transformers import BertConfig, BertForSequenceClassification<NewLine><NewLine>config = BertConfig.from_pretrained('bert-base-uncased')<NewLine><NewLine>model = BertForSequenceClassification.from_pretrained(<NewLine>    'bert-base-uncased', config=config)<NewLine><NewLine>model.to('cpu')<NewLine><NewLine>quantized_model = torch.quantization.quantize_dynamic(<NewLine>    model, {torch.nn.Linear}, dtype=torch.qint8)<NewLine><NewLine>quantized_model.save_pretrained('model_dir_q')<NewLine><NewLine>model = BertForSequenceClassification.from_pretrained('model_dir_q')<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>I also ran into the same error.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I tried again with Torch 1.4.0.  The previous error is no longer present, but quantization breaks the model.  Performance goes from 68.6% down to 3.8% on my task.</p><NewLine><p>I’ll dig deeper to see if I can figure out why, but any suggestions would be greatly appreciated.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/JeffO; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/JeffO; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/mhajiaghayi; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/JeffO; <NewLine> ,"REPLY_DATE 1: January 12, 2020,  3:11am; <NewLine> REPLY_DATE 2: January 12, 2020,  6:51pm; <NewLine> REPLY_DATE 3: January 13, 2020,  7:51pm; <NewLine> REPLY_DATE 4: January 13, 2020, 11:58pm; <NewLine> REPLY_DATE 5: January 14, 2020,  7:08am; <NewLine> REPLY_DATE 6: January 23, 2020,  3:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
67295,Exception: must run observer before calling calculate_qparams!,2020-01-21T18:09:17.098Z,1,268,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Running the tutorial at <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a> raises exception</p><NewLine><blockquote><NewLine><p>/usr/local/lib/python3.6/dist-packages/torch/quantization/observer.py in calculate_qparams(self)<br/><NewLine>44         n_levels = 255.0<br/><NewLine>45         if self.max_val is None or self.min_val is None:<br/><NewLine>—&gt; 46             raise Exception(‘must run observer before calling calculate_qparams!’)<br/><NewLine>47         max_val, min_val = self.max_val.item(), self.min_val.item()<br/><NewLine>48         if max_val == min_val:</p><NewLine><p>Exception: must run observer before calling calculate_qparams!</p><NewLine></blockquote><NewLine></div>",https://discuss.pytorch.org/u/srohit0,(Rohit Sharma),srohit0,"January 21, 2020,  6:09pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Which version of PyTorch are you using? Seems there are some differences between the code you pasted and the master: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: January 21, 2020, 11:00pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
66722,Cannot quantize nn.Conv2d with dynamic Quantization,2020-01-15T10:04:59.231Z,0,359,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve read the pytorch <a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noopener"">quantization document</a> , and I think it should quantize nn.Conv2d module as well as nn.Linear. But, when I try the dynamic quantization, it only converts the nn.Linear.</p><NewLine><p>I used the following simple dummy test:</p><NewLine><pre><code class=""lang-auto"">class dumy_CNN(nn.Module):<NewLine>    def __init__(self, ni, no):<NewLine>        super().__init__()              <NewLine>        self.conv = nn.Conv2d(ni, no, 8, 2, 3)<NewLine>        self.lin = nn.Linear(1024, 4)<NewLine>    def forward(self, x):<NewLine>        out = self.lin(self.conv(x))<NewLine>        return out    <NewLine><NewLine>model_test = dumy_CNN(2,10)<NewLine>model_qn = torch.quantization.quantize_dynamic(<NewLine>        model_test, {nn.Linear, nn.Conv2d} , dtype= torch.qint8<NewLine>        )<NewLine></code></pre><NewLine><p>But the model_qn looks like:</p><NewLine><pre><code class=""lang-auto"">dumy_CNN(<NewLine> (conv): Conv2d(2, 10, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))<NewLine>  (lin): DynamicQuantizedLinear(in_features=1024, out_features=4, scale=1.0, zero_point=0)<NewLine>)<NewLine></code></pre><NewLine><p>I also checked the weights to make sure about the above issue:</p><NewLine><pre><code class=""lang-auto"">model_qn.conv.weight.data[0,0,0,0].item()<NewLine>0.02230245992541313<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/babak_hss,(Bob),babak_hss,"January 15, 2020, 10:04am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/babak_hss"">@babak_hss</a>, Dynamic quantization is currently supported only for nn.Linear and nn.LSTM, please see: <a href=""https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: January 20, 2020, 11:30am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> 
65872,How to set quantization aware training scaling factors?,2020-01-06T05:58:48.554Z,0,177,"<div class=""post"" itemprop=""articleBody""><NewLine><p>when i use quantization aware training , The weight tensor scaling factors is a standard floating point number.<br/><NewLine>I want to convert my model as 8bit at FPGA, so the weight tensor scaling factor must be an integer power-of-two value exponent. Is there such an option? what should I do</p><NewLine></div>",https://discuss.pytorch.org/u/sunkr1995,(Sunkr1995),sunkr1995,"January 6, 2020,  5:58am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>It seems that the quantization scheme is a little bit different. You can see from this <a href=""https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Depending on the fixed-point arithmetic you use, you can convert float multiplier to quantized_multiplier (integer) and right shift (integer). Please checkout <a href=""https://github.com/pytorch/FBGEMM/blob/master/src/QuantUtils.cc#L107-L157"" rel=""nofollow noopener"">https://github.com/pytorch/FBGEMM/blob/master/src/QuantUtils.cc#L107-L157</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/robotcator123; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: January 9, 2020,  5:03am; <NewLine> REPLY_DATE 2: January 13, 2020,  7:57pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
63804,No module named &lsquo;torchvision.models.quantization&rsquo;,2019-12-12T02:09:52.797Z,2,795,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torchvision.models.quantization as models<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>ModuleNotFoundError: No module named 'torchvision.models.quantization'<NewLine>&gt;&gt;&gt; torchvision.__version__<NewLine>Traceback (most recent call last):<NewLine>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;<NewLine>NameError: name 'torchvision' is not defined<NewLine>&gt;&gt;&gt; import torchvision<NewLine>&gt;&gt;&gt; torchvision.__version__<NewLine>'0.4.2+cpu'<NewLine></code></pre><NewLine><p>What is the version of torchvision needed in this <a href=""https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html#part-2-finetuning-the-quantizable-model"" rel=""nofollow noopener"">toturial</a></p><NewLine></div>",https://discuss.pytorch.org/u/dalalaa,(dai),dalalaa,"December 12, 2019,  2:09am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Can you try version 0.5? It could work on my local.</p><NewLine><pre><code class=""lang-auto"">&gt;&gt;&gt; import torchvision<NewLine>&gt;&gt;&gt; torchvision.__version__<NewLine>'0.5.0a0+333af7a'<NewLine>&gt;&gt;&gt; import torchvision.models.quantization as models<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thank you for you reply, but when I run <code>pip install torchvision==0.5.0</code>, I got the following error:</p><NewLine><pre><code class=""lang-auto"">Looking in indexes: http://mirrors.aliyun.com/pypi/simple/<NewLine>ERROR: Could not find a version that satisfies the requirement torchvision==0.5.0 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.0, 0.4.1, 0.4.2)<NewLine>ERROR: No matching distribution found for torchvision==0.5.0`<NewLine></code></pre><NewLine><p>Should I build torchvision from source?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/lly-zero-one"">@lly-zero-one</a> <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/fmassa"">@fmassa</a>: Can you help answer this question?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dalalaa; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: December 13, 2019, 10:13pm; <NewLine> REPLY_DATE 2: December 15, 2019,  1:20pm; <NewLine> REPLY_DATE 3: December 18, 2019, 12:36am; <NewLine> REPLY_DATE 4: December 19, 2019,  4:26am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
64231,Object detection quantization,2019-12-16T15:44:28.938Z,0,1244,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I followed the quantization steps to qunatized faster-rcnn, but I got the following errors in the RegionProposalNetwork part of the network when calling the box_coder.decode(pred_bbox_deltas.detach(), anchors) function.</p><NewLine><p>Could not run ‘aten::empty_strided’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::empty_strided’ is only available for these backends: [CPUTensorId, VariableTensorId]</p><NewLine><p>Could not run ‘aten::div.Tensor’ with arguments from the ‘QuantizedCPUTensorId’ backend. ‘aten::div.Tensor’ is only available for these backends: [CPUTensorId, SparseCPUTensorId, VariableTensorId].</p><NewLine></div>",https://discuss.pytorch.org/u/amin_sabet,(Amin Sabet),amin_sabet,"December 16, 2019,  3:44pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>let’s track the progress here: <a href=""https://github.com/pytorch/pytorch/issues/31316"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/31316</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: December 18, 2019, 12:18am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
63931,Steps to Create Quantized Model,2019-12-13T05:56:22.555Z,2,653,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Can anyone give the details and steps need to do quantization.<br/><NewLine>As there is some confusion - for MobilenetV2 example there is different ways and in the other example there is different example.<br/><NewLine>Is there some general way?</p><NewLine></div>",https://discuss.pytorch.org/u/mohit7,(Mohit Ranawat),mohit7,"December 13, 2019,  5:56am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Mohit,<br/><NewLine>Are you interested in quantizing CV models? Are you targeting mobile devices?<br/><NewLine>If so, you can follow the tutorial here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine><p>In general, you can start with post training quantization (the available options depend on whether it is server or mobile). You can get the recommended qconfig to use for post training quantization by calling:</p><NewLine><pre><code class=""lang-auto""># Gets the recommended qconfig for post training quantization<NewLine>model.qconfig = torch.quantization.get_default_qconfig('fbgemm') #'fbgemm' for server and 'qnnpack' for mobile<NewLine>#Also, remember to set your backend engine to match what you use here:<NewLine>torch.backends.quantization.engine = 'fbgemm' <NewLine></code></pre><NewLine><p>If the accuracy is not good enough, you will have to do quantization aware training, which is more involved. You can see reference code at: <a href=""https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py</a></p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hey <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> , instead of giving wrong answers don’t give the answers. It is misguiding to others. The API you mentioned doesn’t even exist. I wasted my 2 hours on this.<br/><NewLine>Thanks,<br/><NewLine>Mohit Ranawat</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Mohit,<br/><NewLine>Looks like I made a mistake in the instructions. This should work:</p><NewLine><pre><code class=""lang-python"">qconfig = torch.quantization.get_default_qconfig('fbgemm')<NewLine>print(torch.backends.quantized.supported_engines) # Prints the quantized backends that are supported<NewLine># Set the backend to what is needed. This needs to be consistent with the option you used to select the qconfig<NewLine>torch.backends.quantized.engine='fbgemm'<NewLine></code></pre><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: December 13, 2019, 11:09pm; <NewLine> REPLY_DATE 2: December 16, 2019, 11:44am; <NewLine> REPLY_DATE 3: December 16, 2019,  7:47pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
63613,Generic static quantization,2019-12-10T10:21:31.333Z,2,344,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>I’m trying to quantise pre-trained models from torchvision, and I’ve hit an obstacle I 'm struggling to get past. I’ve been able to fuse layers and replace relus as needed, I’ve then used Quantwrapper to get the quant and dequant around the forward function and then I can prepare and convert using the quantisation tools.</p><NewLine><p>The problem I’m at now is that I get an error every time I try to run a model with an operation that needs to be quantised. For example inceptionV3 uses a some operations during the forward pass and errors will be like:</p><NewLine><p>'RuntimeError: Didn’t find kernel to dispatch for operator ‘aten::mul’. Tried to look up kernel for dispatch key ‘QuantizedCPUTensorID.’</p><NewLine><p>I have bee able to run these networks by editing the model to replace operations with nn.quantised.FloatFunctional() operations, is there any way I can get past this without editing each network individually?</p><NewLine></div>",https://discuss.pytorch.org/u/Joe_Heyward,(Joe Heyward),Joe_Heyward,"December 10, 2019, 10:21am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/joe_heyward"">@Joe_Heyward</a>,</p><NewLine><p>Currently, replacing these non-parametric operations with FloatFunctional is the only way. More details are here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture</a></p><NewLine><p>The reason behind the need for replacement is that to quantize these operations we need to observe input/output values.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>We are also working on automating this with quantization after scripting (graph mode quantization), but this feature is still in progress.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Ok, thanks! What’s the best way to keep up to date on this?</p><NewLine><p>Ps I thought I would be able to get past this if I could get the quant and dequant tight enough around the network forward pass, so I wrote a function similar to: <a href=""https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#add_quant_dequant"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#add_quant_dequant</a>, where a quantwrapper is added around every submodule to a certain level but this gives me a separate error:</p><NewLine><p>RuntimeError: size mistmatch, m1: [20160 x 224] , m2: [1024 x 100] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Keeping an eye on release notes here (<a href=""https://github.com/pytorch/pytorch/releases"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/releases</a>) is one way to keep up to date on this. Another more involved way is checking quantization related diffs.</p><NewLine><p>It’s hard to say what’s wrong here without looking at the code.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Joe_Heyward; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: December 10, 2019,  5:31pm; <NewLine> REPLY_DATE 2: December 10, 2019,  6:01pm; <NewLine> REPLY_DATE 3: December 11, 2019, 10:57am; <NewLine> REPLY_DATE 4: December 14, 2019, 12:44am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
62729,Quantized Squeeze block MobilenetV3,2019-12-02T09:47:11.410Z,1,341,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi, I’m trying to quantize mobilenet V3 but get stuck in quantizing Squeeze Block. When I train it in subset dataset and use convert to quantized model, it works. but when I evaluate quantized model in eval set, It throws an error of Mul operation. Can you tell me how can I implement Squeeze block (SElayer). thank you so much.</p><NewLine><pre><code class=""lang-auto"">class SqueezeBlock(nn.Module):<NewLine>    def __init__(self, exp_size, divide=4):<NewLine>        super(SqueezeBlock, self).__init__()<NewLine>        self.avg_pool = nn.AdaptiveAvgPool2d(1)<NewLine>        self.dense = nn.Sequential(<NewLine>            nn.Linear(exp_size, exp_size // divide),<NewLine>            nn.ReLU(inplace=False),<NewLine>            nn.Linear(exp_size // divide, exp_size),<NewLine>            h_sigmoid()<NewLine>        )<NewLine>        self.mul = torch.nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, x):<NewLine>        batch, channels, height, width = x.size()<NewLine>        out = self.avg_pool(x).view(batch, channels)<NewLine>        out = self.dense(out)<NewLine>        out = out.view(batch, channels, 1, 1)<NewLine>        return self.mul.mul(x,out)<NewLine></code></pre><NewLine><pre><code class=""lang-auto"">File ""/home/X/Documents/thancuong/mb3_quantized/quantized_mb3.py"", line 121, in forward<NewLine>    return self.mul.mul(x,out)<NewLine>  File ""/home/X/.virtualenvs/torch_0.4/lib/python3.5/site-packages/torch/nn/quantized/modules/functional_modules.py"", line 146, in mul<NewLine>    zero_point=self.zero_point)<NewLine>RuntimeError: Mul operands must be the same size! (check_inputs at /pytorch/aten/src/ATen/native/quantized/cpu/qmul.cpp:20)<NewLine>frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f077b622813 in /home/X/.virtualenvs/torch_0.4/lib/python3.5/site-packages/torch/lib/libc10.so)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/thancaocuong,(cuongtc),thancaocuong,"December 2, 2019,  9:47am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can actually try to comment out the two lines as <a href=""https://github.com/pytorch/pytorch/pull/30442"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/30442</a>, since the tensor iterator supports broadcast.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thank you, I will try it</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/thancaocuong; <NewLine> ,"REPLY_DATE 1: December 4, 2019,  5:57am; <NewLine> REPLY_DATE 2: December 4, 2019,  5:57am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
58539,Pytorch1.3 Quantization error in Resnet50,2019-10-18T02:32:11.637Z,3,1129,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When applying quantization supported by pytorch 1.3 to Resnet 50, the following error occurs.</p><NewLine><p>RuntimeError: Didn’t find engine for operation quantized::linear_prepack NoQEngine (operator () at …\aten\src\ATen\native\quantized\cpu\qlinear_prepack.cpp:202)<br/><NewLine>(no backtrace available)</p><NewLine><p>Does anyone know how</p><NewLine></div>",https://discuss.pytorch.org/u/kcw4875,(chanwoong Kwak),kcw4875,"October 18, 2019,  2:32am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Could you try set USE_FBGEMM=1 before running python setup.py install? If it doesn’t work, please file a Github issue.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I installed pytorch 1.3 using anconda,<br/><NewLine>and there seems to be no environment for python setup. I think it need additional library(FBGMM) settings, do you know how?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I think FBGEMM should be turned for machines/compilers that support it by default, can you print <code>torch.backends.quantized.supported_engines</code> to see if fbgemm is there?</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Got the same error. torch.backends.quantized.supported_engines is showing None<br/><NewLine>I am running on windows. How to enable it on windows?</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>cc <a class=""mention"" href=""/u/hx89"">@hx89</a> <a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> do you know how to do this?</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>We haven’t yet tested it on windows.</p><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/FBGEMM/issues/150"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/FBGEMM</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/FBGEMM/issues/150"" rel=""nofollow noopener"" target=""_blank"">Is this available on windows?</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-25"" data-format=""ll"" data-time=""06:13:12"" data-timezone=""UTC"">06:13AM - 25 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/snaik2016"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""snaik2016"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars0.githubusercontent.com/u/18183245?v=4"" width=""20""/><NewLine>          snaik2016<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">Can this library be built on windows?</p><NewLine></div><NewLine><div class=""labels""><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> this issue is resolved?<br/><NewLine>If yes, can you tell?</p><NewLine><p>Thanks,<br/><NewLine>Mohit</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Mohit,</p><NewLine><p>I responded to the question here in detail: <a href=""https://github.com/pytorch/FBGEMM/issues/150"" rel=""nofollow noopener"">https://github.com/pytorch/FBGEMM/issues/150</a>.</p><NewLine><p>It still needs some work from our side to make it work on windows.</p><NewLine><p>Thanks<br/><NewLine>Daya</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/hx89; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/kcw4875; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Saurabh_Naik; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/mohit7; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/dskhudia; <NewLine> ,"REPLY_DATE 1: October 19, 2019, 12:08am; <NewLine> REPLY_DATE 2: October 21, 2019,  1:22am; <NewLine> REPLY_DATE 3: October 24, 2019,  1:03am; <NewLine> REPLY_DATE 4: October 25, 2019, 12:00am; <NewLine> REPLY_DATE 5: November 1, 2019,  6:06pm; <NewLine> REPLY_DATE 6: November 1, 2019,  9:03pm; <NewLine> REPLY_DATE 7: December 2, 2019, 11:54am; <NewLine> REPLY_DATE 8: December 2, 2019,  6:06pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
62324,How to difine self defined loss?,2019-11-27T15:33:44.358Z,2,141,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I defined a loss function, it can run normally. however, it produce bad results, while nll can produce accurate results in the same net work. So any one can take look at my code is right?</p><NewLine><p>class CrpsLossExt(Function):<br/><NewLine>def forward(outEjectF, targetEjectF,nClass):</p><NewLine><pre><code>    targetEjectF = targetEjectF.cpu().numpy()<NewLine>    predEjectF = outEjectF.cpu().numpy()<NewLine>   <NewLine>   # following sentences are not torch function, they are from python,numpy or SCI functions<NewLine>    predictCdf = utils_heart.real_to_cdf(predEjectF,nClass)<NewLine>    targetHeavySide = utils_heart.heaviside_function(targetEjectF,nClass)<NewLine>    crplLoss = utils_heart.crps(predictCdf,targetHeavySide)<NewLine>    tensorCrplLoss = torch.from_numpy(np.array(crplLoss))<NewLine>   <NewLine>    tensorCrplLoss = tensorCrplLoss.requires_grad_()       <NewLine>    return tensorCrplLoss   <NewLine><NewLine>def backward(grad_output): <NewLine>    return grad_output</code></pre><NewLine></div>",https://discuss.pytorch.org/u/lzh21cen,(lzh21cen),lzh21cen,"November 27, 2019,  3:35pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi lzh21cen!</p><NewLine><p>Please take a look at the ""Extending <code>torch.autograd</code>"" section<br/><NewLine>of <a href=""https://pytorch.org/docs/stable/notes/extending.html"" rel=""nofollow noopener"">Extending PyTorch</a>.</p><NewLine><aside class=""quote no-group quote-modified"" data-full=""true"" data-post=""1"" data-topic=""62324""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/b5e925/40.png"" width=""20""/> lzh21cen:</div><NewLine><blockquote><NewLine><p>I defined a loss function, it can run normally. however, it produce bad results, while nll can produce accurate results in the same net work.</p><NewLine><p>…</p><NewLine><pre><code class=""lang-python"">       # following sentences are not torch function, they are from python,numpy or SCI functions<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Once you move out of pytorch and into something like numpy,<br/><NewLine>autograd can no longer track your gradients, so you (your loss<br/><NewLine>function) will have to do it yourself.  That’s what <code>backward</code> is for.</p><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><pre><code class=""lang-python"">    def backward(grad_output): <NewLine>        return grad_output<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>Your <code>backward</code> doesn’t do anything (except return its input as<br/><NewLine>its output).  So the gradients that get passed to the optimizer don’t<br/><NewLine>know anything about the structure of your loss function.  That is,<br/><NewLine>your gradients are incorrect, so the optimizer won’t be moving<br/><NewLine>your weights to a lower loss.</p><NewLine><p>You either need to rewrite your loss function using pytorch<br/><NewLine><code>Tensor</code> functions so that autograd can track and calculate<br/><NewLine>the gradients automatically for you, or you have do calculus<br/><NewLine>on your loss function to get its gradients and implement them<br/><NewLine>by hand in your <code>backward</code> function.</p><NewLine><p>Good luck.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>thanks a lot, K. Frank<br/><NewLine>If I define loss function in another way which will be inherited from nn.Module, must I use torch functions in all forward steps?</p><NewLine><p>class CrpsLossModule(nn.Module):<br/><NewLine>def <strong>init</strong>(self, nClass, reduce=True):<br/><NewLine>super(CrpsLossModule, self).<strong>init</strong>()<br/><NewLine>self.nClass = nClass<br/><NewLine>self.reduce = reduce</p><NewLine><pre><code>def forward(self, outEjectF, targetEjectF):<NewLine>    <NewLine>    # forward code here<NewLine><NewLine>    if self.reduce:<NewLine>        return torch.mean(F_loss)<NewLine>    else:<NewLine>        return F_loss</code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello lzh21cen!</p><NewLine><aside class=""quote no-group"" data-full=""true"" data-post=""3"" data-topic=""62324""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/l/b5e925/40.png"" width=""20""/> lzh21cen:</div><NewLine><blockquote><NewLine><p>If I define loss function in another way which will be inherited from nn.Module, must I use torch functions in all forward steps?</p><NewLine></blockquote><NewLine></aside><NewLine><p>You could try it out and see.  Pass a <code>requires_grad = True</code> tensor<br/><NewLine>through your loss function and see if <code>.backward()</code> gives the correct<br/><NewLine>result.  Something like:</p><NewLine><pre><code class=""lang-python"">input = torch.randn ((2, 5), requires_grad=True)<NewLine>target = ...<NewLine>loss = my_custom_loss_function (input, target)<NewLine>loss.backward()<NewLine>print (input.grad)<NewLine></code></pre><NewLine><aside class=""quote no-group""><NewLine><blockquote><NewLine><pre><code class=""lang-python"">        # forward code here<NewLine><NewLine>        if self.reduce:<NewLine>            return torch.mean(F_loss)<NewLine>        else:<NewLine>            return F_loss<NewLine></code></pre><NewLine></blockquote><NewLine></aside><NewLine><p>(As a side note, I have no idea what <code>F_loss</code> means here, so I have<br/><NewLine>no idea whether this code makes sense or would work the way you<br/><NewLine>want.)</p><NewLine><p>Best.</p><NewLine><p>K. Frank</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks, your answers are extremely helpful! I think I had to look in detail the documentation.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/lzh21cen; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/KFrank; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/lzh21cen; <NewLine> ,"REPLY_DATE 1: November 27, 2019,  6:03pm; <NewLine> REPLY_DATE 2: November 27, 2019,  6:49pm; <NewLine> REPLY_DATE 3: November 27, 2019,  7:24pm; <NewLine> REPLY_DATE 4: November 27, 2019,  8:02pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58894,"Quantization aware training, extremely slow on GPU",2019-10-22T09:42:59.287Z,3,1246,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hey all,</p><NewLine><p>I’ve been experimenting with quantization aware training using pytorch 1.3.<br/><NewLine>I managed to adapt my model as demonstrated in the tutorial.<br/><NewLine>The documenation mentions that fake quantization is possible on GPU, however I notice that it is extremely slow.<br/><NewLine>Monitoring <code>nvidia-smi</code> shows that I only use 7% of the GPU, while it is close to 100% when using the non-qat adapted model.<br/><NewLine>Is this expected or am I doing something wrong?</p><NewLine></div>",https://discuss.pytorch.org/u/Mxbonn,(Maxim),Mxbonn,"October 22, 2019,  9:43am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>I would assume this is expected, since the <code>FakeQuantize</code> uses some additional operations on the tensor values to fake the quantization.</p><NewLine><blockquote><NewLine><p>PyTorch 1.3 doesn’t provide quantized operator implementations on CUDA yet - this is direction of future work. Move the model to CPU in order to test the quantized functionality.<br/><NewLine>Quantization-aware training (through <a href=""https://pytorch.org/docs/stable/quantization.html#torch.quantization.FakeQuantize"" rel=""nofollow noopener""> <code>FakeQuantize</code> </a>) supports both CPU and CUDA.</p><NewLine></blockquote><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>That should not be the case. fake_quantize is supported on the GPU. For more insight, can you compare the run-time per batch with and without quantization aware training on GPU?<br/><NewLine>Also, can you provide the quantization qconfig that you used for quantization aware training?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I believe the issue is due to the insertion of observers. I’m currently running MobileNetV2 with QAT, but only evaluating with calibration batches (no actual training), and then performing inference (after freezing the observers).</p><NewLine><p>When using observer=torch.quantization.MinMaxObserver, before running qat_model.apply(torch.quantization.disable_observer), the time per batch is 2.95s, and afterwards, it is 1.51s, on cpu. On a GPU, the numbers are 3.71s and 0.38s. When the observers are not enabled, there is a clear speedup on GPUs, but there’s actually a performance degradation with observers on CPU.</p><NewLine><p>When using the HistogramObserver, the performance on GPU (just for calibrating, not training) is 8x worse.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>It appears that the issue is in pytorch’s method for computing per_channel quantization parameters – instead of parallelizing the computation, it runs in a loop per layer. Since this is sequential, it bottlenecks the performance. For PerChannelMinMaxObserver (and the moving average version) it should be pretty easy to modify the code to run in parallel.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>If you want QAT training to be much faster, you can make the following changes:</p><NewLine><p>First, parallelize calculate_qparams for the PerChannel observers. Making the following change improved performance when calibrating (with observers enabled) by ~9x</p><NewLine><pre><code class=""lang-auto"">    def calculate_qparams(self):<NewLine>        min_val, max_val = self.min_vals, self.max_vals<NewLine>        if max_val is None or min_val is None:<NewLine>            warnings.warn(<NewLine>                ""must run observer before calling calculate_qparams.\<NewLine>                                    Returning default scale and zero point ""<NewLine>            )<NewLine>            return torch.tensor([1.0]), torch.tensor([0])<NewLine><NewLine>        assert torch.all(min_val &lt;= max_val), ""min {} should be less than max {}"".format(<NewLine>            min_val, max_val<NewLine>        )<NewLine><NewLine>        if self.dtype == torch.qint8:<NewLine>            if self.reduce_range:<NewLine>                qmin, qmax = -64, 63<NewLine>            else:<NewLine>                qmin, qmax = -128, 127<NewLine>        else:<NewLine>            if self.reduce_range:<NewLine>                qmin, qmax = 0, 127<NewLine>            else:<NewLine>                qmin, qmax = 0, 255<NewLine>        min_val = torch.clamp(min_val, max=0.0)<NewLine>        max_val = torch.clamp(max_val, min=0.0)<NewLine>        # The check of max_val == min_val is removed -- in that case, I prefer taking scale = eps than 1.<NewLine>        if self.qscheme == torch.per_tensor_symmetric or self.qscheme == torch.per_channel_symmetric:<NewLine>            max_val = torch.max(-min_val, max_val)<NewLine>            scale = max_val / ((qmax - qmin) / 2)<NewLine>            scale = torch.clamp(scale, min=self.eps)<NewLine>            zero_point = torch.ones_like(scale) * math.ceil((qmin + qmax) / 2)<NewLine>        else:<NewLine>            scale = (max_val - min_val) / float(qmax - qmin)<NewLine>            scale = torch.clamp(scale, min=self.eps)<NewLine>            zero_point = qmin - torch.round(min_val / scale)<NewLine>            zero_point = torch.clamp(zero_point, min=qmin, max=qmax)<NewLine><NewLine>        zero_point = zero_point.long()<NewLine><NewLine>        # Note: this code keeps scale, zero_point on GPU. Only use this if you parallelize the <NewLine>        # implementation in fake quantize instead of using fake_quantize_per_channel_affine<NewLine>        # by following the next code block. Otherwise CPU is faster<NewLine>        # return scale.cpu(), zero_point.cpu()<NewLine>        return scale, zero_point<NewLine></code></pre><NewLine><p>Second, use a parallelized version of fake quantization per channel (the C++ implementation of the operation iterates over every channel, which is slow). We can do this by changing FakeQuantize’s forward method to be the following. Note that you get almost identical results to the previous code, but values of weights that are close to a rounding boundary when quantizing (e.g. 67.5000) may be quantized to the other bin. Adding this decreases inference time by a factor of 2, and decreases calibration time by a factor of 1.5. Once making these two changes, inference time with the quantized model is the same as inference time with the baseline model.</p><NewLine><pre><code class=""lang-auto"">    def forward(self, X):<NewLine>        if self.observer_enabled:<NewLine>            self.observer(X.detach())<NewLine>            self.scale, self.zero_point = self.calculate_qparams()<NewLine>        if self.fake_quant_enabled:<NewLine>            if self.qscheme == torch.per_channel_symmetric or self.qscheme == torch.per_channel_affine:<NewLine>                new_shape = [1] * len(X.shape)<NewLine>                new_shape[self.ch_axis] = -1<NewLine>                self.scale = self.scale.view(new_shape)<NewLine>                self.zero_point = self.zero_point.view(new_shape)<NewLine>                X = X / self.scale + self.zero_point<NewLine>                X = torch.fake_quantize_per_tensor_affine(X, float(1.0),<NewLine>                                                          int(0), self.quant_min,<NewLine>                                                          self.quant_max)<NewLine>                X = (X - self.zero_point) * self.scale<NewLine>            else:<NewLine>                X = torch.fake_quantize_per_tensor_affine(X, float(self.scale),<NewLine>                                                          int(self.zero_point), self.quant_min,<NewLine>                                                          self.quant_max)<NewLine>        return X<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>Good obserations. Two issues have been created in github to track the suggestions here:<br/><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30348"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30348</a> -&gt; Speed up calc q params in observers.<br/><NewLine><a href=""https://github.com/pytorch/pytorch/issues/30349"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/issues/30349</a> -&gt; Speed up per channel fake-quant</p><NewLine><p>For fake-quant it will be better to speed up the C++ implementation to parallelize it.</p><NewLine></div>; <NewLine> REPLY 7: <div class=""post"" itemprop=""articleBody""><NewLine><p>For fake-quant, parallelization speed up is dependent on the tensor size, with slow downs of 30-40% for smaller kernels. Since per-channel quant is typically done for weights, it is not desirable to have the slow-down for smaller tensors.</p><NewLine></div>; <NewLine> REPLY 8: <div class=""post"" itemprop=""articleBody""><NewLine><p>PR has been put up for speeding up calculation of q params in observers: <a href=""https://github.com/pytorch/pytorch/pull/30485"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/30485</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/sahaj; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/sahaj; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/sahaj; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 7: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 8: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: October 22, 2019, 10:14am; <NewLine> REPLY_DATE 2: October 30, 2019, 11:51pm; <NewLine> REPLY_DATE 3: November 12, 2019, 11:18pm; <NewLine> REPLY_DATE 4: November 15, 2019,  3:08am; <NewLine> REPLY_DATE 5: November 15, 2019,  5:48am; <NewLine> REPLY_DATE 6: November 22, 2019, 11:30pm; <NewLine> REPLY_DATE 7: November 26, 2019,  1:08am; <NewLine> REPLY_DATE 8: November 27, 2019,  6:47am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: 1 Like; <NewLine> REPLY 7 LIKES: ; <NewLine> REPLY 8 LIKES: ; <NewLine> 
60840,Scalar operation observers,2019-11-13T10:18:29.380Z,1,166,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Scalar op observers doesn’t seem to record statistics.</p><NewLine><pre><code class=""lang-auto"">class HardSigmoid(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.act = nn.ReLU6()<NewLine>        self.add = nn.quantized.FloatFunctional()<NewLine>        self.mul = nn.quantized.FloatFunctional()<NewLine>    <NewLine>    def forward(self, input):<NewLine>        output = self.add.add_scalar(input, 3)<NewLine>        output = self.act(output)<NewLine>        output = self.mul.mul_scalar(output, 1/6)<NewLine>        return output<NewLine><NewLine>class Residual(nn.Module):<NewLine>    def __init__(self, *layers):<NewLine>        super().__init__()<NewLine>        self.layers = nn.Sequential(*layers)<NewLine>        self.add = nn.quantized.FloatFunctional()<NewLine><NewLine>    def forward(self, input):<NewLine>        return self.add.add(input, self.layers(input))<NewLine><NewLine>model = Residual(HardSigmoid())<NewLine>model.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(model, inplace=True)<NewLine>model(torch.rand(4,3,2,1))<NewLine><NewLine></code></pre><NewLine><p><code>model.add.activation_post_process.min_val</code> has a value, but <code>model.layers[0].mul.activation_post_process.min_val</code> and <code>model.layers[0].add.activation_post_process.min_val</code> is None</p><NewLine></div>",https://discuss.pytorch.org/u/pshashk,,pshashk,"November 13, 2019, 10:18am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""60840""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/9d8465/40.png"" width=""20""/> pshashk:</div><NewLine><blockquote><NewLine><p>act</p><NewLine></blockquote><NewLine></aside><NewLine><p>add_scalar and mul_scalar actually do not need observer. It could get the quantized parameter from the input qtensor and also compute the quantized parameter for the output qtensor.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>I’m glad there is no issue and it works as designed. But still it’s a bit strange for me to use <code>FloatFunctional</code> if it doesn’t record any state. Why not simply overload scalar operations at convert time?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Eager mode quantization is based on module level replacement. Current flow didn’t do the function level swapping, though it should be feasible to replace it with quantized kernel.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/pshashk; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  7:27am; <NewLine> REPLY_DATE 2: November 22, 2019,  8:13am; <NewLine> REPLY_DATE 3: November 23, 2019,  2:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
60547,Per Tensor/Channel quantization equivalents in PyTorch/Caffe2,2019-11-10T16:05:24.024Z,0,147,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The weights obtained from PyTorch per tensor quantization of Conv2d can be used in Int8Conv from caffe2. But from Int8Conv definitions, I understood that it only accepts <code>scale</code> as a float and not an array.</p><NewLine><p>Is it possible to use the PerChannel quantization in Caffe2?</p><NewLine></div>",https://discuss.pytorch.org/u/Dorozhko-Anton,(Anton Dorozhko),Dorozhko-Anton,"November 10, 2019,  4:07pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Unfortunately, Caffe2 Int8Conv doesn’t support per-channel quantization. The DNNLOWP engine that uses FBGEMM backend does support group-wise quantization if that helps you. Please see <a href=""https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/conv_groupwise_dnnlowp_op_test.py</a> for example of using group-wise quantization.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Jongsoo_Park; <NewLine> ,"REPLY_DATE 1: December 3, 2019,  1:06pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
60542,Matrix quantization while training,2019-11-10T14:50:15.182Z,0,101,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am having a bit trouble navigating the new quantization lib. I’m trying to train while quantizing a tensor matrix which is multiplied with a float32 convolutional layer weight. What would be the best way going about it?</p><NewLine></div>",https://discuss.pytorch.org/u/Toshi,(Toshi ),Toshi,"November 10, 2019,  2:50pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi <a class=""mention"" href=""/u/toshi"">@Toshi</a>, can you provide more details? Ideally a small code snipped would help. Have you seen the tutorials under <a href=""https://pytorch.org/tutorials/#quantization-experimental"" rel=""nofollow noopener"">https://pytorch.org/tutorials/#quantization-experimental</a>?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  9:45pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
61727,Static quantization error,2019-11-21T11:36:59.700Z,0,305,"<div class=""post"" itemprop=""articleBody""><NewLine><p><strong>my model as following:</strong></p><NewLine><pre><code class=""lang-auto"">from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>class dehaze_net(nn.Module):<NewLine>    <NewLine>	def __init__(self):<NewLine>		super(dehaze_net, self).__init__()<NewLine><NewLine>		self.relu = nn.ReLU(inplace=False)<NewLine>	<NewLine>		self.e_conv1 = nn.Conv2d(3,3,1,1,0,bias=True) <NewLine>		self.e_conv2 = nn.Conv2d(3,3,3,1,1,bias=True) <NewLine>		self.e_conv3 = nn.Conv2d(6,3,5,1,2,bias=True) <NewLine>		self.e_conv4 = nn.Conv2d(6,3,7,1,3,bias=True) <NewLine>		self.e_conv5 = nn.Conv2d(12,3,3,1,1,bias=True)<NewLine>		self.skip_add = nn.quantized.FloatFunctional()<NewLine>		self.quant = QuantStub() <NewLine>		self.dequant = DeQuantStub() <NewLine>        # weight initialization<NewLine>		for m in self.modules():<NewLine>			if isinstance(m, nn.Conv2d):<NewLine>				nn.init.kaiming_normal_(m.weight, mode='fan_out')<NewLine>				if m.bias is not None:<NewLine>					nn.init.zeros_(m.bias)<NewLine>			elif isinstance(m, nn.BatchNorm2d):<NewLine>				nn.init.ones_(m.weight)<NewLine>				nn.init.zeros_(m.bias)<NewLine>			elif isinstance(m, nn.Linear):<NewLine>				nn.init.normal_(m.weight, 0, 0.01)<NewLine>				nn.init.zeros_(m.bias)<NewLine>		<NewLine>	def forward(self, x):<NewLine>		x = self.quant(x)<NewLine>		source = []<NewLine>		source.append(x)<NewLine><NewLine>		x1 = self.relu(self.e_conv1(x))<NewLine>		x2 = self.relu(self.e_conv2(x1))<NewLine><NewLine>		concat1 =self.skip_add.cat((x1,x2), 1)<NewLine>		x3 = self.relu(self.e_conv3(concat1))<NewLine><NewLine>		concat2 = self.skip_add.cat((x2, x3), 1)<NewLine>		x4 = self.relu(self.e_conv4(concat2))<NewLine><NewLine>		concat3 = self.skip_add.cat((x1,x2,x3,x4),1)<NewLine>		x5 = self.relu(self.e_conv5(concat3))<NewLine><NewLine>		#clean_image = self.relu(self.skip_add.add((x5 * x) - x5, 1)) <NewLine>		clean_image = self.skip_add.add_relu(self.skip_add.add(self.skip_add.mul(x5,x),-x5), 1)<NewLine><NewLine>		clean_image = self.dequant(clean_image)<NewLine>		return clean_image<NewLine></code></pre><NewLine><p><strong>i load the pretrained model ,and predict the image ,it is right .as follows</strong></p><NewLine><pre><code class=""lang-auto"">def load_model(model_file):<NewLine>    model = dehaze_net()<NewLine>    state_dict = torch.load(model_file)<NewLine>    model.load_state_dict(state_dict)<NewLine>    model.to('cpu')<NewLine>    return model<NewLine>myModel = load_model(float_model_file).to('cpu')<NewLine>myModel.eval()<NewLine><NewLine>myModel.qconfig = torch.quantization.default_qconfig<NewLine>print(myModel.qconfig)<NewLine>torch.quantization.prepare(myModel, inplace=True)<NewLine>validation(myModel, data_loader_test)<NewLine></code></pre><NewLine><p><strong>but when i excute the following codes ,it give some errors:</strong></p><NewLine><pre><code class=""lang-auto"">torch.quantization.convert(myModel, inplace=True)<NewLine>validation(myModel, data_loader_test)<NewLine></code></pre><NewLine><p><strong>errors:</strong></p><NewLine><pre><code class=""lang-auto"">&lt;ipython-input-44-674cc7c0f6b1&gt; in forward(self, x)<NewLine>     47 <NewLine>     48                 #clean_image = self.relu(self.skip_add.add((x5 * x) - x5, 1))<NewLine>---&gt; 49                 clean_image = self.skip_add.add_relu(self.skip_add.add(self.skip_add.mul(x5,x),-x5), 1)<NewLine>     50 <NewLine>     51                 clean_image = self.dequant(clean_image)<NewLine><NewLine>RuntimeError: No function is registered for schema aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, CUDATensorId, MkldnnCPUTensorId, SparseCPUTensorId, SparseCUDATensorId, VariableTensorId<NewLine></code></pre><NewLine><p><em><strong>thanks</strong></em></p><NewLine></div>",https://discuss.pytorch.org/u/aa12356jm,(Aa12356jm),aa12356jm,"November 21, 2019, 11:36am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""61727""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/user_avatar/discuss.pytorch.org/aa12356jm/40/18003_2.png"" width=""20""/> aa12356jm:</div><NewLine><blockquote><NewLine><p>self.skip_add.mul(x5,x)</p><NewLine></blockquote><NewLine></aside><NewLine><p>Thanks for pasting the issue. I think the issue is resulting from “-x5”, can you change it to mul_scalar(x5, -1)? Also add_relu’s second argument should be a tensor instead of int. We will look into the support of -x.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> ,"REPLY_DATE 1: November 22, 2019,  6:58am; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> 
60878,ConvBnReLU quantized performance,2019-11-13T16:56:25.970Z,2,504,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I was trying to quantize FBNet model in PyTorch.<br/><NewLine>Quantized version has several times bigger latency than fp32.<br/><NewLine>but on raspberry pi it gives some gain in latency but still slow.</p><NewLine><p>here is the result of small benchmark (just one Conv+Bn+ReLU)<br/><NewLine><img alt=""qperf_bug_1"" data-base62-sha1=""yDgc47KUhxppWFh0q9WhCCV6Ae8"" height=""415"" src=""https://discuss.pytorch.org/uploads/default/original/3X/f/2/f2ba1128ddba148bcdad381aad25b6a9db1eb19c.png"" width=""654""/></p><NewLine><p>Is this an expected behavior ?</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>import torch.nn.quantized as nnq<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>class ConvBNRelu(nn.Module):<NewLine>    def __init__(self, cfg):<NewLine>        super(ConvBNRelu, self).__init__()<NewLine>        self.conv = nn.Conv2d(in_channels=cfg['in_channels'],<NewLine>                              out_channels=cfg['out_channels'],<NewLine>                              kernel_size=cfg['kernel_size'],<NewLine>                              stride=cfg['stride'],<NewLine>                              padding=cfg['padding']) <NewLine>        self.bn = nn.BatchNorm2d(num_features=cfg['out_channels'])<NewLine>        self.relu = nn.ReLU()<NewLine>   <NewLine>    def forward(self, x):<NewLine>        x = self.conv(x)<NewLine>        x = self.bn(x)<NewLine>        x = self.relu(x)<NewLine>        return x<NewLine>        <NewLine>class Backbone(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(Backbone, self).__init__()<NewLine>        cfg = {<NewLine>            'stem' : {<NewLine>                'in_channels' : 3,<NewLine>                'out_channels' : 32,<NewLine>                'kernel_size' : (3, 3),<NewLine>                'stride' : (2, 2),<NewLine>                'padding' : (1, 1),<NewLine>            }<NewLine>        }<NewLine>        self.stem = ConvBNRelu(cfg['stem'])<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.stem(x)<NewLine>        x = self.dequant(x)<NewLine>        return x<NewLine>    <NewLine>    def fuse_model(self):<NewLine>        torch.quantization.fuse_modules(self.stem, ['conv', 'bn', 'relu'], inplace=True)<NewLine><NewLine>model = Backbone()<NewLine>torch.manual_seed(123)<NewLine>x = torch.randn([1, 3, 34, 320], dtype=torch.float)*10<NewLine><NewLine>print(model)<NewLine>%timeit model(x) <NewLine><NewLine>model.qconfig = torch.quantization.get_default_qconfig('qnnpack')<NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine>torch.quantization.prepare(model, inplace=True);<NewLine>with torch.no_grad():<NewLine>    for i in range(5):<NewLine>        model(x)    <NewLine>torch.quantization.convert(model, inplace=True);<NewLine><NewLine>print(model)<NewLine>%timeit model(x) <NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Dorozhko-Anton,(Anton Dorozhko),Dorozhko-Anton,"November 13, 2019,  4:57pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the post. It seems to be some perf issue of the int8 kernel implementation for this specific shape. We will be investigating it.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Are you just trying this shape (in_channels  = 3, out_channels=32, kernel_size=3, stride=2 and padding=1) or is this actually used in a network? The reason I ask this is because usually the first conv is in_channels  = 3, out_channels=64, kernel_size=7, stride=2 and padding=3 and we do have optimized fast path for this in fbgemm.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>This conv2 params are actually used in FBNet like architectures from maskrrcnn_benchmark in “first” block. (to be more precise Chamnet-Mobile like arch )</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Oh, this is a typo</p><NewLine><pre><code class=""lang-auto"">x = torch.randn([1, 3, 34, 320], dtype=torch.float)*10  <NewLine></code></pre><NewLine><p>It should be [1, 3, 320, 320], but this doesn’t change the ratios of time mesurements</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/lly-zero-one; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> ,"REPLY_DATE 1: November 19, 2019,  9:46pm; <NewLine> REPLY_DATE 2: November 19, 2019,  9:57pm; <NewLine> REPLY_DATE 3: November 19, 2019, 10:25pm; <NewLine> REPLY_DATE 4: November 19, 2019, 10:28pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> 
60557,Quantized Conv2d gives different result from the Caffe2&rsquo;s Int8Conv with the same weights,2019-11-10T20:36:57.508Z,3,476,"<div class=""post"" itemprop=""articleBody""><NewLine><ol><NewLine><li>Create module with 1 Conv2d (with constant weights), Add quant stubs</li><NewLine><li>Save results of inference in <code>init_res</code><NewLine></li><NewLine><li>Perform post-training static quantization</li><NewLine><li>Save results of quantized inference in <code>q_res</code><NewLine></li><NewLine></ol><NewLine><p><code>init_res</code> and <code>q_res</code> are 100% different</p><NewLine><ol start=""5""><NewLine><li>Build caffe2 int8 network with 1 Conv2d and weights from the quantized PyTorch module. (with necessary NCHW -&gt; NHWC transpositions)</li><NewLine><li>Save inference in <code>caffe_res</code><NewLine></li><NewLine></ol><NewLine><p><code>init_res</code> and <code>caffe_res</code> are similar with rtol=0.05</p><NewLine><p>Question: what am I doing wrong? why quantized PyTorch Conv gives so different results?</p><NewLine><p>P.S. in every inference and calibration the same tensor <code>x</code> is used.</p><NewLine><p>Code:</p><NewLine><pre><code class=""lang-auto"">import torch<NewLine>import torch.nn as nn<NewLine>from torch.quantization import QuantStub, DeQuantStub<NewLine><NewLine>print(torch.__version__)<NewLine>torch.manual_seed(123)<NewLine><NewLine>import numpy as np<NewLine>from caffe2.python import workspace, model_helper<NewLine><NewLine>class QuantizedConv(nn.Module):<NewLine>    def __init__(self):<NewLine>        super(QuantizedConv, self).__init__()<NewLine>        self.conv = nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))<NewLine>        self.quant = QuantStub()<NewLine>        self.dequant = DeQuantStub()<NewLine>        <NewLine>        m = self.conv<NewLine>        self.conv.weight.data.fill_(0.01)<NewLine>        nn.init.zeros_(m.bias)<NewLine>        <NewLine>    def forward(self, x):<NewLine>        x = self.quant(x)<NewLine>        x = self.conv(x)<NewLine>        return self.dequant(x)<NewLine>    <NewLine><NewLine>model = QuantizedConv()<NewLine>x = torch.randn(1, 3, 320, 320, dtype=torch.float)<NewLine><NewLine>model = QuantizedConv()<NewLine>model.eval()<NewLine><NewLine>init_res = model(x).detach().numpy()<NewLine><NewLine>model.qconfig = torch.quantization.get_default_qconfig(""qnnpack"")<NewLine>print(model.qconfig)<NewLine>model = torch.quantization.prepare(model, inplace=True);<NewLine>with torch.no_grad():<NewLine>    for i in range(10):<NewLine>        model(x)<NewLine>torch.quantization.convert(model, inplace=True);<NewLine><NewLine>q_res = model(x).detach().numpy()<NewLine># np.testing.assert_allclose( q_res, init_res, rtol=0.05, atol=0.04) <NewLine># &lt;- error<NewLine># Not equal to tolerance rtol=0.05, atol=0.04<NewLine># (mismatch 99.97626676159962%)<NewLine>#  x: array([-0.159745, -0.194942, -0.138084, ..., -0.138084, -0.17599 ,<NewLine>#       -0.140792], dtype=float32)<NewLine># y: array([-0.049151, -0.018199,  0.065673, ...,  0.056086, -0.027169,<NewLine>#       -0.023959], dtype=float32)<NewLine></code></pre><NewLine><p>Caffe2 part</p><NewLine><pre><code class=""lang-auto"">workspace.ResetWorkspace()<NewLine>workspace.FeedBlob(""x1"", x)<NewLine>net = model_helper.ModelHelper(name=""test"")<NewLine><NewLine>net.net.NCHW2NHWC(""x1"", ""x"") # Transpose input tensor<NewLine>net.net.Int8Quantize(""x"", <NewLine>                     ""x_int8"", <NewLine>                     Y_scale=model.quant.scale.numpy()[0], <NewLine>                     Y_zero_point=model.quant.zero_point.numpy()[0],<NewLine>                     )<NewLine><NewLine>W = model.conv.weight().int_repr().detach().numpy()<NewLine>W = np.transpose(W, [0, 2, 3, 1]) # Transpose conv weights tensor <NewLine>net.param_init_net.Int8GivenTensorFill(<NewLine>                                [],<NewLine>                                'conv_w',<NewLine>                                shape=(32, 5, 5, 3), #model.conv.weight().int_repr().detach().numpy().shape,<NewLine>                                values=W.tobytes(),<NewLine>                                Y_scale=model.conv.weight().q_scale(),<NewLine>                                Y_zero_point=0,<NewLine>)<NewLine>net.param_init_net.Int8GivenIntTensorFill(<NewLine>                            [],<NewLine>                            'conv_b',<NewLine>                            shape=[32, ], # model.conv.bias().detach().numpy().shape,<NewLine>                            values= np.zeros(32, dtype=np.int32), #model.conv.bias().detach().numpy(),<NewLine>                            Y_scale=1.,<NewLine>                            Y_zero_point=0<NewLine>)<NewLine><NewLine>kwargs = {<NewLine>    ""kernel"":5,<NewLine>    ""stride"":2,<NewLine>    ""pad"":1,<NewLine>    ""order"":""NHWC"",<NewLine>    ""Y_scale"":model.conv.scale,<NewLine>    ""Y_zero_point"":model.conv.zero_point,<NewLine>}<NewLine>net.net.Int8Conv([""x_int8"", ""conv_w"", 'conv_b'], ""conv_1_q"", **kwargs)<NewLine>net.net.Int8Dequantize(""conv_1_q"", <NewLine>                       ""res"",<NewLine>                       Y_scale=1., <NewLine>                       Y_zero_point=0,)<NewLine>workspace.RunNetOnce(net.param_init_net)<NewLine>workspace.CreateNet(net.net)<NewLine>workspace.RunNet(net.name)<NewLine><NewLine>caffe_res = workspace.FetchBlob(""res"")<NewLine>caffe_res = np.transpose(caffe_res, [0, 3, 1, 2]) # Transpose output tensor<NewLine><NewLine>np.testing.assert_allclose( caffe_res, init_res, rtol=0.05, atol=0.1)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Dorozhko-Anton,(Anton Dorozhko),Dorozhko-Anton,"November 10, 2019,  8:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>And I also noted that PyTorch results max value is 0</p><NewLine><pre><code class=""lang-auto"">print(f'min {init_res.min()}, max {init_res.max()}')<NewLine>print(f'min {q_res.min()}, max {q_res.max()}')<NewLine>print(f'min {caffe_res.min()}, max {caffe_res.max()}')<NewLine><NewLine># min -0.3654041886329651, max 0.36045965552330017<NewLine># min -0.36551710963249207, max 0.0<NewLine># min -0.36551710963249207, max 0.32490411400794983<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>with other initialization of conv2d</p><NewLine><pre><code class=""lang-auto"">self.conv.weight.data.uniform_(-0.05, 0.05)<NewLine></code></pre><NewLine><p>PyTorch quantized results become close enough to <code>init_res</code>, but caffe conv is very different.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is that default pytorch uses fbgemm as a backend for quantized operators. You can check this by</p><NewLine><pre><code class=""lang-auto"">print(torch.backends.quantized.engine)<NewLine></code></pre><NewLine><p>Caffe2 int8conv uses the qnnpack engine by default. To ensure that pytorch and c2 match, you will need to do the following:<br/><NewLine>Once I do that the outputs match.</p><NewLine><pre><code class=""lang-auto""><NewLine>model.qconfig = torch.quantization.get_default_qconfig(""qnnpack"")<NewLine>torch.backends.quantized.engine = 'qnnpack'<NewLine># Set engine to qnnpack<NewLine>print(model.qconfig)<NewLine>model = torch.quantization.prepare(model, inplace=True);<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Is it possible to call int8 Conv in caffe2 with signed weights ?</p><NewLine><pre><code class=""lang-auto"">self.conv.weight.data.uniform_(-0.05, 0.05)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>It should work, qnnpack can take in both signed and unsigned weights in floating point. Internally, the weights are represented as uint8 values with a scale and zero-point mapping from float to quantized values.</p><NewLine></div>; <NewLine> REPLY 6: <div class=""post"" itemprop=""articleBody""><NewLine><p>I wasn’t able to run c2 Int8Conv with signed weights<br/><NewLine>Should I use <code>Int8ConvPackWeight</code> to make it work ?</p><NewLine><p>I tried to look at test code (because I didn’t find any docs except for code)<br/><NewLine><aside class=""onebox githubblob""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/blob/e870a9a87042805cd52973e36534357f428a0748/caffe2/quantization/server/utils.py#L437"" rel=""nofollow noopener"" target=""_blank"">github.com</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><h4><a href=""https://github.com/pytorch/pytorch/blob/e870a9a87042805cd52973e36534357f428a0748/caffe2/quantization/server/utils.py#L437"" rel=""nofollow noopener"" target=""_blank"">pytorch/pytorch/blob/e870a9a87042805cd52973e36534357f428a0748/caffe2/quantization/server/utils.py#L437</a></h4><NewLine><pre class=""onebox""><code class=""lang-py""><ol class=""start lines"" start=""427"" style=""counter-reset: li-counter 426 ;""><NewLine><li>def add_quantization_param_args(op, tensor, preserve_sparsity=False):</li><NewLine><li>    tensor_min = 0 if tensor.size == 0 else tensor.min()</li><NewLine><li>    tensor_max = 0 if tensor.size == 0 else tensor.max()</li><NewLine><li><NewLine></li><NewLine><li>    q_param = choose_quantization_params(tensor_min, tensor_max, preserve_sparsity)</li><NewLine><li><NewLine></li><NewLine><li>    add_quantization_param_args_(op, q_param)</li><NewLine><li>    return q_param</li><NewLine><li><NewLine></li><NewLine><li><NewLine></li><NewLine><li class=""selected"">def create_int8_given_tensor_fill(tensor, out_blob_name, preserve_sparsity=False):</li><NewLine><li>    """"""</li><NewLine><li>    Create Int8GivenTensorFill op that quantizes the given tensor and outputs</li><NewLine><li>    an Int8Tensor with out_blob_name.</li><NewLine><li>    """"""</li><NewLine><li>    op = core.CreateOperator(""Int8GivenTensorFill"", [], out_blob_name)</li><NewLine><li>    q_param = add_quantization_param_args(op, tensor, preserve_sparsity)</li><NewLine><li>    quantized_tensor = (</li><NewLine><li>        np.around(tensor / q_param.scale).astype(np.int32) + q_param.zero_point</li><NewLine><li>    )</li><NewLine><li>    quantized_tensor = np.maximum(0, np.minimum(quantized_tensor, 255))</li><NewLine></ol></code></pre><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine><p>And before weight packaging they used <code>uint8</code> .</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 6: https://discuss.pytorch.org/u/Dorozhko-Anton; <NewLine> ,"REPLY_DATE 1: November 10, 2019,  8:50pm; <NewLine> REPLY_DATE 2: November 11, 2019, 11:14pm; <NewLine> REPLY_DATE 3: November 12, 2019,  2:07am; <NewLine> REPLY_DATE 4: November 12, 2019,  8:42am; <NewLine> REPLY_DATE 5: November 12, 2019,  6:58pm; <NewLine> REPLY_DATE 6: November 13, 2019, 11:15am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> REPLY 6 LIKES: ; <NewLine> 
60119,Test_adaptive_avg_pool2d_nhwc deadline in test_quantized.py,2019-11-05T23:44:49.560Z,0,117,"<div class=""post"" itemprop=""articleBody""><NewLine><p>When I build pytorch (successfully) and execute the tests, things go well until encountering test <code>test_adaptive_avg_pool2d_nhwc</code> which is consistently giving a “DeadlineExceeded” error … it usually takes from 230-260 ms, exceeding the 200ms deadline, e.g.:</p><NewLine><p><code>hypothesis.errors.DeadlineExceeded: Test took 256.44ms, which exceeds the deadline of 200.00ms</code></p><NewLine><p>What is the approximate time magnitude that this test should take?  I’m wondering if the 200ms is intended as a generous time buffer for something that should take significantly less, or is it perhaps not generous enough given that the test does otherwise complete?  Many tests specify “<span class=""mention"">@no_deadline</span>” but not this one; should we consider adding it?</p><NewLine><p>Note this is a <code>ppc64le</code> build/test.</p><NewLine></div>",https://discuss.pytorch.org/u/dncliss,(David Clissold),dncliss,"November 5, 2019, 11:44pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>So on my laptop (4x i7-5600U CPU @ 2.60GHz in cpuinfo), the test seems to run in ~95ms (with the most lazy, inaccurate measurement and running it once or 11x to get some handle on the invocation overhead).</p><NewLine><pre><code class=""lang-auto"">tv@aComp:~/python/pytorch/pytorch$ python3 test/test_quantized.py TestQuantizedOps.test_adaptive_avg_pool2d_nhwc<NewLine>.<NewLine>----------------------------------------------------------------------<NewLine>Ran 1 test in 0.115s<NewLine><NewLine>OK<NewLine>$ python3 test/test_quantized.py TestQuantizedOps.test_adaptive_avg_pool2d_nhwc{,,,,,,,,,,}<NewLine>...........<NewLine>----------------------------------------------------------------------<NewLine>Ran 11 tests in 1.050s<NewLine><NewLine>OK<NewLine></code></pre><NewLine><p>I don’t know what factor you would suppose to exist between my laptop and your computer.<br/><NewLine>We would likely want to know if the test suddenly regressed 2x in performance, so turning off the deadline might not be the best thing, but maybe you could have a decorator turning it off or making it more generous for your specific architectures. (These are just some thoughts, I don’t know about any “official policy” or anything.)</p><NewLine><p>Best regards</p><NewLine><p>Thomas</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thomas, Thanks for this comparison point.  If you’re getting ~95ms on a laptop, that tells me the 200ms deadline is not unreasonable and executing above that level does flag a possible performance concern.  Now in this case, the execution environment is in a conda env within a Ubuntu docker container on the OSUOSL lab on a shared server, so it’s not an environment optimized for performance.<br/><NewLine>In fact, this is literally the exact ppc64le CI which is directly linked to from the pytorch github home page README.md labelled as 3.6 and “Linux (ppc64le) GPU”, which points to the CI at: <a href=""https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/"" rel=""nofollow noopener"">https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/</a>  (in spite of misleading URL name, it’s on cuda10; we’ll fix that URL once it is running cleanly).</p><NewLine><p>Anyhow, this testcase appears new as of pytorch 1.3 and I’m going to try to get a build running and tested on our own local systems to see how the execution of this test compares.  (Of course, we’d then like to get the public CI version showing a successful test completion too.)</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/tom; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/dncliss; <NewLine> ,"REPLY_DATE 1: November 6, 2019,  7:23am; <NewLine> REPLY_DATE 2: November 6, 2019,  5:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
60000,Very Bad Bounding Boxes After Quantization of Model,2019-11-04T23:38:05.104Z,1,218,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Folks,<br/><NewLine>I have been trying to perform static quantization of a model for scene text detection and reduce the precision of the model from float32 to integer8.</p><NewLine><p>I followed the steps mentioned in the official tutorial - fused the conv,batch norm and relu layers ;  converted to int ; replaced concat with nn.quantized.Floattensor().<br/><NewLine>The original model works well, but the prediction from quantized model is really bad -</p><NewLine><p>Prediction from FP32 model -<br/><NewLine><img alt=""image"" data-base62-sha1=""srf1d3fMvsfeNSXdiQC5HJJ28QV"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/c/7/c751371bae97b455047d1077fe8f926bcb825a51.jpeg"" width=""450""/></p><NewLine><p>Prediction from INT8 model -<br/><NewLine><img alt=""image"" data-base62-sha1=""c3Tm7enUCpVGGk3aDJVw7qRwKBV"" height=""375"" src=""https://discuss.pytorch.org/uploads/default/original/3X/5/4/548abd74691d9488395a8a4e8307a4ed987c37b7.jpeg"" width=""450""/></p><NewLine><p>Not sure where I’m going wrong <img alt="":confused:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title="":confused:""/></p><NewLine><p>Would appreciate on any suggestions where I’m going wrong during quantization.</p><NewLine><p>The code is available here -  <a href=""https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py"" rel=""nofollow noopener"">https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py</a></p><NewLine></div>",https://discuss.pytorch.org/u/Raghav_Gurbaxani,(Raghav Gurbaxani),Raghav_Gurbaxani,"November 4, 2019, 11:40pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Not sure how it might help you, but you can try using Intel’s <code>Openvino</code> for <a href=""http://docs.openvinotoolkit.org/latest/_docs_Workbench_DG_Int_8_Quantization.html"" rel=""nofollow noopener"">quantisation</a>.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>yeah pytorch quantization didn’t yield very good results. Might give OpenVino or Tensortt a try.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Seems it’s the same topic as another thread where we are helping on debugging:<br/><NewLine><aside class=""quote quote-modified"" data-post=""15"" data-topic=""59966""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""/user_avatar/discuss.pytorch.org/raghav_gurbaxani/40/17041_2.png"" width=""20""/><NewLine><a href=""https://discuss.pytorch.org/t/quantization-error-during-concat-runtimeerror-didnt-find-kernel-to-dispatch-to-for-operator-aten-cat/59966/15"">Quantization Error During Concat -- RuntimeError: Didn't find kernel to dispatch to for operator 'aten::_cat'</a> <a class=""badge-wrapper bullet"" href=""/c/quantization""><span class=""badge-category-bg"" style=""background-color: #0088CC;""></span><span class=""badge-category clear-badge"" data-drop-close=""true"" style="""" title=""This category is for questions, discussion and issues related to PyTorch’s quantization feature."">quantization</span></a><NewLine></div><NewLine><blockquote><NewLine>    thanks <a class=""mention"" href=""/u/hx89"">@hx89</a> , if you could post that example for compare module level quantization error - It would be great <img alt=""smiley"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/smiley.png?v=9"" title=""smiley""/> <NewLine>In the meantime, I tried the histogram observer and the result is still pretty bad  <img alt=""confused"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/confused.png?v=9"" title=""confused""/> <NewLine>[image] <NewLine>any other suggestions ?<NewLine>  </blockquote><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/paganpasta; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/hx89; <NewLine> ,"REPLY_DATE 1: November 5, 2019,  5:14am; <NewLine> REPLY_DATE 2: November 5, 2019, 11:16pm; <NewLine> REPLY_DATE 3: November 5, 2019, 10:20pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59519,"RuntimeError: No function is registered for schema aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor?",2019-10-29T23:29:09.108Z,2,1286,"<div class=""post"" itemprop=""articleBody""><NewLine><p>After static . quantization, I receive the following error-</p><NewLine><p>RuntimeError: No function is registered for schema aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -&gt; (Tensor output, Tensor finput, Tensor fgrad_input) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, VariableTensorId</p><NewLine><p>raised by traced_script_module = torch.jit.trace(quantized, q_example, check_trace=False)</p><NewLine><p>full code here - <a href=""https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py"" rel=""nofollow noopener"">https://github.com/raghavgurbaxani/experiments/blob/master/try_static_quant.py</a></p><NewLine></div>",https://discuss.pytorch.org/u/Raghav_Gurbaxani,(Raghav Gurbaxani),Raghav_Gurbaxani,"October 29, 2019, 11:29pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This is saying conv2d_forward is given a quantized tensor, which means some float Conv is not swapped as quantized Conv, can you print the quantized model and see where this happens?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am not sure this is the same problem or not.<br/><NewLine>But for my example, when I forget to add “model.eval()”, there is such error.<br/><NewLine>When I add “model.eval()”, it seems fine.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I only get this error when I load the model with GPU:</p><NewLine><pre><code class=""lang-auto"">use_cuda = torch.cuda.is_available()<NewLine>device = torch.device(""cuda:0"" if use_cuda else ""cpu"")<NewLine><NewLine>model = ResNet3D().to(device)<NewLine>model.apply(weights_init)<NewLine><NewLine>input = torch.rand([2,1,86,110,78])<NewLine>output = model(input) <NewLine>print(input.shape)<NewLine>print(output.shape)<NewLine></code></pre><NewLine><p>Then comes the error: <code>RuntimeError: No function is registered for schema aten::thnn_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -&gt; (Tensor output, Tensor finput, Tensor fgrad_input) on tensor type CUDATensorId; available functions are CPUTensorId, VariableTensorId</code></p><NewLine><p>if I load the model in cpu:</p><NewLine><pre><code class=""lang-auto"">model = ResNet3D()<NewLine>model.apply(weights_init)<NewLine>output = model(input)<NewLine>print(input.shape)<NewLine>print(output.shape)<NewLine></code></pre><NewLine><p>gives output nicely:</p><NewLine><pre><code class=""lang-auto"">torch.Size([2, 1, 86, 110, 78])<NewLine>torch.Size([2, 256])<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>solution: I will correct myself. I loaded the input as following:</p><NewLine><pre><code class=""lang-auto"">input = torch.rand([2,1,86,110,78]).float().to(device)<NewLine></code></pre><NewLine><p>which solved the issue of loading model with GPUs.<br/><NewLine>Thanks</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/liwei46; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/banikr; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/banikr; <NewLine> ,"REPLY_DATE 1: October 30, 2019, 12:06am; <NewLine> REPLY_DATE 2: November 4, 2019,  7:01am; <NewLine> REPLY_DATE 3: November 5, 2019,  5:51pm; <NewLine> REPLY_DATE 4: November 5, 2019,  6:10pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: 2 Likes; <NewLine> 
59622,Static quantization returning torch.float32 parameters,2019-10-30T22:09:09.450Z,0,309,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Hello,</p><NewLine><p>I am currently following <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">the official PyTorch static quantization tutorial</a> for quantizing my model. The model uses a ResNet backbone and implements other things as a Generalized Mean Pooling, the complete code can be accessed <a href=""https://gist.github.com/paulomann/60003d4a7f54c8b92e467e1f0a2d7208"" rel=""nofollow noopener"">here in this gist</a>.</p><NewLine><p>I could follow the tutorial steps and run the <code>torch.quantization.convert(myModel, inplace=True)</code> line to finally quantize my model, but when I check the size of the model, it’s almost the same size as before quantizing (186mb to 174mb).</p><NewLine><p>The main code is this (complementary to the gist):</p><NewLine><pre><code class=""lang-auto""># My own function to load the checkpoint<NewLine>ckpt = common.load_checkpoint(settings.CKPT_MODEL, False)<NewLine># The definition of QuantizableRMAC is in the gist<NewLine># But it is similar to how is done in the tutorial<NewLine>qnet = QuantizableRMAC(**ckpt[""model_options""])<NewLine>qnet.fuse_model() # Fuse model here!!!!<NewLine>qnet.eval()<NewLine>qnet.qconfig = torch.quantization.default_qconfig<NewLine>torch.quantization.prepare(qnet, inplace=True)<NewLine>data_loader = quantizable_dataloader(""path/to/images"")<NewLine><NewLine>def evaluate(model, data_loader):<NewLine>    cnt = 0<NewLine>    with torch.no_grad():<NewLine>        for image, _ in data_loader:<NewLine>            model(image)<NewLine>            <NewLine>evaluate(qnet, data_loader, 1)<NewLine>quantized_net = torch.quantization.convert(qnet)<NewLine><NewLine>def print_size_of_model(model):<NewLine>    torch.save(model.state_dict(), ""temp.p"")<NewLine>    print('Size (MB):', os.path.getsize(""temp.p"")/1e6)<NewLine>    os.remove('temp.p')<NewLine><NewLine>print_size_of_model(quantized_net)<NewLine><NewLine>for p in quantized_net.parameters():<NewLine>    print(p.dtype)<NewLine></code></pre><NewLine><p>The last line prints only <code>torch.float32</code>.</p><NewLine></div>",https://discuss.pytorch.org/u/Paulo_Mann,(Paulo Mann),Paulo_Mann,"November 1, 2019,  6:46pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi Paulo,<br/><NewLine>I am not able to access the gist. Can you fix that? The main code looks ok, except that you are missing calling torch.quantization.fuse_model() which is required for quantization to work.  Quantization does not support unfused batch norms currently.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hello <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a>, thanks for your reply!</p><NewLine><p>I think you can now access the gist. In fact, I call the <code>fuse_model()</code> function of the <code>QuantizableRMAC</code> class (in the gist), which in turn calls the <code>ResNet</code> <code>fuse_model()</code> function, in the same way you wrote the original code <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py"" rel=""nofollow noopener"">here</a>. I just missed this part of the code when transitioning to here. I am going to fix this in the first post to match the way it is done.</p><NewLine><p>However, when I inspect the model by printing its modules names after calling the <code>fuse_model()</code> function, I see that I successfully fused all conv, bn and relus, that’s why I am not sure about what is happening.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I made a minimum working example here, I use the code from (I copied the code to a jupyter notebook) <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py</a></p><NewLine><p><img alt=""image"" data-base62-sha1=""yaVFBLS51e9WxnCO8guhGdX8qHx"" height=""176"" src=""https://discuss.pytorch.org/uploads/default/original/3X/e/f/ef8637341566664b4bddc176c45d98e40dc336c3.png"" width=""552""/></p><NewLine><p>However, it does not work, it raises this error:</p><NewLine><p><img alt=""image"" data-base62-sha1=""tJVQUPxJWJhwCEeL21OVFw0srTH"" height=""129"" src=""https://discuss.pytorch.org/uploads/default/original/3X/d/0/d07045644f99760b18a496bf960e49baa37e7fc9.png"" width=""670""/></p><NewLine><p>And when removing the <code>model.eval()</code> code, it does not raises any error, but does not work as expected:</p><NewLine><p><img alt=""image"" data-base62-sha1=""lWfeOw4DeZIW5Wwytu3xUnWt19f"" height=""194"" src=""https://discuss.pytorch.org/uploads/default/original/3X/9/9/99c32fd79bb89e827bb4d1aa9987f297261d1c99.png"" width=""551""/></p><NewLine><p>This is using the same ResNet, QuantizableResNet, and QuantizableBottleneck code used in the tutorials. These pictures were taken with <a href=""https://nbviewer.jupyter.org/github/paulomann/samples/blob/master/minimum_reproducible_example.ipynb"" rel=""nofollow noopener"">this minimum reproducible code</a><br/><NewLine>Thoughts? <a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> <a class=""mention"" href=""/u/jerryzh168"">@jerryzh168</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Paulo_Mann; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Paulo_Mann; <NewLine> ,"REPLY_DATE 1: October 30, 2019, 11:49pm; <NewLine> REPLY_DATE 2: October 31, 2019,  1:27am; <NewLine> REPLY_DATE 3: November 1, 2019,  7:04pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59114,"RuntimeError: lhs of assignment must be a variable, subscript, or starred expression:",2019-10-24T11:16:38.067Z,3,286,"<div class=""post"" itemprop=""articleBody""><NewLine><p>after i QAT my model,  i tried to save it with torch.jit.save<br/><NewLine>but it did not work:</p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/6017a36f06a889f59a605d49f9d243a32b53c5dc"" href=""https://discuss.pytorch.org/uploads/default/original/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc.png"" title=""21.png""><img alt=""21"" data-base62-sha1=""dI4tTGHrvgPWrip3uakF2HzQXA8"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc_2_10x10.png"" height=""374"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc_2_690x374.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc_2_690x374.png, https://discuss.pytorch.org/uploads/default/optimized/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc_2_1035x561.png 1.5x, https://discuss.pytorch.org/uploads/default/original/3X/6/0/6017a36f06a889f59a605d49f9d243a32b53c5dc.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">21.png</span><span class=""informations"">1163×631 139 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine><p><div class=""lightbox-wrapper""><a class=""lightbox"" data-download-href=""https://discuss.pytorch.org/uploads/default/161a7dd9faaeafb40851a996d73c2c221c8853e1"" href=""https://discuss.pytorch.org/uploads/default/original/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1.png"" title=""22.png""><img alt=""22"" data-base62-sha1=""39xgguZ6Bi7zTYUCrRWeR6gPRpn"" data-small-upload=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1_2_10x10.png"" height=""405"" src=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1_2_690x405.png"" srcset=""https://discuss.pytorch.org/uploads/default/optimized/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1_2_690x405.png, https://discuss.pytorch.org/uploads/default/optimized/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1_2_1035x607.png 1.5x, https://discuss.pytorch.org/uploads/default/optimized/3X/1/6/161a7dd9faaeafb40851a996d73c2c221c8853e1_2_1380x810.png 2x"" width=""690""/><div class=""meta""><NewLine><svg aria-hidden=""true"" class=""fa d-icon d-icon-far-image svg-icon""><use xlink:href=""#far-image""></use></svg><span class=""filename"">22.png</span><span class=""informations"">1562×918 128 KB</span><svg aria-hidden=""true"" class=""fa d-icon d-icon-discourse-expand svg-icon""><use xlink:href=""#discourse-expand""></use></svg><NewLine></div></a></div></p><NewLine></div>",https://discuss.pytorch.org/u/jiacheng1gujiaxin,(Jiacheng1gujiaxin),jiacheng1gujiaxin,"October 24, 2019, 11:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi,</p><NewLine><p>Would you have a small code snippet that reproduces the crash please?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><blockquote><NewLine><p>if args.qat:<br/><NewLine>configure(args.log_dir, flush_secs=5)<br/><NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig(‘fbgemm’)<br/><NewLine>torch.quantization.prepare_qat(model, inplace=True)<br/><NewLine># print(qat_model.parameters())</p><NewLine><pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)<NewLine>best_loss = 100<NewLine>for nepoch in range(args.epochs):<NewLine>    model.to(device)<NewLine>    train_one_epoch(nepoch, model, criterion, optimizer, data_loader, device)<NewLine>    if nepoch &gt; 90:<NewLine>        # Freeze quantizer parameters<NewLine>        model.apply(torch.quantization.disable_observer)<NewLine>    if nepoch &gt; 95:<NewLine>        # Freeze batch norm mean and variance estimates<NewLine>        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>    # Check the accuracy after each epoch<NewLine>    model.to('cpu')<NewLine>    quantized_model = torch.quantization.convert(model.eval(), inplace=False)<NewLine>    quantized_model.eval()<NewLine>    size = get_size_of_model(quantized_model)<NewLine>    top1, top5, loss = evaluate(quantized_model, data_loader_test)<NewLine>    if loss &lt; best_loss:<NewLine>        best_loss = loss<NewLine>        # torch.jit.save(torch.jit.script(quantized_model), 'quantizated_best.pth')<NewLine>        torch.save(quantized_model.state_dict(), 'quantizated_best.pkl')<NewLine>    print(<NewLine>        f'Epoch {nepoch} | top1: {top1} | top5: {top5} | loss {loss} | size {size}')<NewLine><NewLine>    log_value('Validating/Accuracy', top1, nepoch)<NewLine>    log_value('Validating/Loss', loss, nepoch)<NewLine>    torch.jit.save(torch.jit.script(quantized_model), 'quantizated_final.pth')<NewLine></code></pre><NewLine></blockquote><NewLine><p>everything is ok except the process of ‘torch.jit.save()’</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>I already solve this problem, thank you !!!</p><NewLine><p>there is empty nn.Squentional() in my module, which torch.quantized.convert can`t remove (observer) from it,</p><NewLine><p>so I remove it manually, and it works</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>btw, we have a fix for empty sequential as well: <a href=""https://github.com/pytorch/pytorch/pull/28384"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/28384</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/albanD; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jiacheng1gujiaxin; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jiacheng1gujiaxin; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: October 24, 2019,  7:38pm; <NewLine> REPLY_DATE 2: October 25, 2019,  6:07am; <NewLine> REPLY_DATE 3: October 25, 2019,  7:25am; <NewLine> REPLY_DATE 4: November 1, 2019,  6:07pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 5 Likes; <NewLine> REPLY 4 LIKES: 1 Like; <NewLine> 
58880,[Quantization] (Error): No function is registered for schema aten,2019-10-22T07:16:49.241Z,2,713,"<div class=""post"" itemprop=""articleBody""><NewLine><pre><code class=""lang-auto"">I tried to quantizate my own shuffle model through 'STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH'.<NewLine>But when it comes to forward propagation, the time to assess model losses, I failed<NewLine></code></pre><NewLine><p>RuntimeError: No function is registered for schema aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, CUDATensorId, MkldnnCPUTensorId, VariableTensorId<br/><NewLine>The above operation failed in interpreter, with the following stack trace:<br/><NewLine>at code/ <strong>torch</strong> /shuff_slim/___torch_mangle_297.py:316:13</p><NewLine></div>",https://discuss.pytorch.org/u/jiacheng1gujiaxin,(Jiacheng1gujiaxin),jiacheng1gujiaxin,"October 22, 2019,  7:16am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Please fold batchnorms with preceding convs manually. Look for the APIs to do this for you in the Resnext-101 tutorial.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/dskhudia"">@dskhudia</a> could you share the link for the Resnext 101 tutorial?</p><NewLine><p>Or could you provide an example ? That would be very helpful</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>You can find tutorial for post training static quantization here: <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"" rel=""nofollow noopener"">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/dskhudia; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Raghav_Gurbaxani; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  5:56am; <NewLine> REPLY_DATE 2: October 25, 2019,  9:26pm; <NewLine> REPLY_DATE 3: November 1, 2019,  6:05pm; <NewLine> ",REPLY 1 LIKES: 1 Like; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> 
59527,Quantize conv layer with bias,2019-10-30T01:58:45.061Z,0,272,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have a resnet-based network. Trying to convert it to INT8 but met accuracy loss using static quant.<br/><NewLine>Tried QAT but met with assert error “Only support fusing Conv2d that does not have bias”</p><NewLine><p>Look into the model structure and found all the conv layers have bias:<br/><NewLine>nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, bias=True)</p><NewLine><p>why should conv without bias? is it possible to use fake quant to train it?</p><NewLine><p>can I only modify the fusing code in qat.conv_fuse.ConvBn2d to do fuse conv bias ?</p><NewLine></div>",https://discuss.pytorch.org/u/dtcrong,(Bigrong),dtcrong,"October 30, 2019,  2:10am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>When you are fusing a conv with a batch norm, there is no separate bias term in the conv. This is because, batch norm already has a trainable bias parameter which serves the same purpose. The reference resnet implementation at: <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L24"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L24</a> does not have bias as part of the conv.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/raghuramank100; <NewLine> ,"REPLY_DATE 1: October 31, 2019, 12:01am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
59021,Error when converting quantization aware trained model for evaluation,2019-10-23T11:57:55.928Z,2,490,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I get the error:<br/><NewLine><code>Didn't find kernel to dispatch to for operator 'aten::fake_quantize_per_tensor_affine'.  Tried to look up kernel for dispatch key 'QuantizedCPUTensorId'. Registered dispatch keys are: [CUDATensorId, CPUTensorId, VariableTensorId].</code><br/><NewLine>when trying to evaluate the model after QAT.<br/><NewLine>My workflow is the same as in the tutorial/documentation:</p><NewLine><pre><code class=""lang-python"">model.train()<NewLine>prepare_qat(model, inplace=True)<NewLine>train(model, run_args)<NewLine>eval_model = convert(model, inplace=False)<NewLine>eval(eval_model)<NewLine></code></pre><NewLine></div>",https://discuss.pytorch.org/u/Mxbonn,(Maxim),Mxbonn,"October 23, 2019, 11:57am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>This means you are feeding a quantized tensor to fake_quantize op, which only supports unquantized tensor, probably because the op with FakeQuantize is not swapped out somehow after the convert, can you paste the model?</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>My original model looks like:</p><NewLine><pre><code class=""lang-auto"">ResNetCifar10(<NewLine>  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>  (relu): ReLU()<NewLine>  (layer1): Sequential(<NewLine>    (0): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>    (1): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>    (2): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>  )<NewLine>  (layer2): Sequential(<NewLine>    (0): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): LambdaLayer()<NewLine>    )<NewLine>    (1): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>    (2): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>  )<NewLine>  (layer3): Sequential(<NewLine>    (0): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): LambdaLayer()<NewLine>    )<NewLine>    (1): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>    (2): BasicBlockCifar10(<NewLine>      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu1): ReLU()<NewLine>      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<NewLine>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<NewLine>      (relu2): ReLU()<NewLine>      (shortcut): Sequential()<NewLine>    )<NewLine>  )<NewLine>  (linear): Linear(in_features=64, out_features=10, bias=True)<NewLine>)<NewLine></code></pre><NewLine><p>I then convert it to a model that can be trained using QAT:<br/><NewLine>(quant and dequant, fuse layers and torch.quantization.prepare_qat)</p><NewLine><pre><code class=""lang-auto"">QuantWrapper(<NewLine>  (quant): QuantStub(<NewLine>    (observer): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>      (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>    )<NewLine>  )<NewLine>  (dequant): DeQuantStub(<NewLine>    (observer): FakeQuantize(<NewLine>      fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>      (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>    )<NewLine>  )<NewLine>  (module): ResNetCifar10(<NewLine>    (conv1): ConvBnReLU2d(<NewLine>      3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>      (observer): FakeQuantize(<NewLine>        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>        (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>      )<NewLine>      (weight_fake_quant): FakeQuantize(<NewLine>        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>        (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>      )<NewLine>    )<NewLine>    (bn1): Identity()<NewLine>    (relu): Identity()<NewLine>    (layer1): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (layer2): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): LambdaLayer()<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (layer3): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): LambdaLayer()<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): ConvBnReLU2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): ConvBn2d(<NewLine>          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>          (weight_fake_quant): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (bn2): Identity()<NewLine>        (relu2): ReLU(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (linear): Linear(<NewLine>      in_features=64, out_features=10, bias=True<NewLine>      (observer): FakeQuantize(<NewLine>        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>        (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>      )<NewLine>      (weight_fake_quant): FakeQuantize(<NewLine>        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None<NewLine>        (observer): MovingAverageMinMaxObserver(min_val=None, max_val=None)<NewLine>      )<NewLine>    )<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>Which can be trained, but I then try to convert it to a evaluation model using<br/><NewLine><code>torch.quantization.convert</code> like in the tutorial:<br/><NewLine>The converted model looks like:</p><NewLine><pre><code class=""lang-auto"">QuantWrapper(<NewLine>  (quant): Quantize(scale=tensor([0.0203]), zero_point=tensor([120]), dtype=torch.quint8)<NewLine>  (dequant): DeQuantize()<NewLine>  (module): ResNetCifar10(<NewLine>    (conv1): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.00903782807290554, zero_point=0, padding=(1, 1))<NewLine>    (bn1): Identity()<NewLine>    (relu): Identity()<NewLine>    (layer1): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.009634326212108135, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.018731188029050827, zero_point=130, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0086]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.1906778812408447)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.010925675742328167, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.01969429850578308, zero_point=141, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0112]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.8511974811553955)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.007963746786117554, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.016999518498778343, zero_point=136, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0144]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.661449670791626)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (layer2): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.009311596862971783, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.02033022604882717, zero_point=115, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): LambdaLayer()<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.006638957187533379, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.018797850236296654, zero_point=136, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0147]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.7611701488494873)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.007839899510145187, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.026490747928619385, zero_point=85, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0144]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.678955316543579)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (layer3): Sequential(<NewLine>      (0): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.009626154787838459, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.02358696237206459, zero_point=109, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): LambdaLayer()<NewLine>      )<NewLine>      (1): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0076929363422095776, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.02710540033876896, zero_point=124, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0241]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.156522750854492)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>      (2): BasicBlockCifar10(<NewLine>        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008539369329810143, zero_point=0, padding=(1, 1))<NewLine>        (bn1): Identity()<NewLine>        (relu1): Identity()<NewLine>        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06625250726938248, zero_point=92, padding=(1, 1))<NewLine>        (bn2): Identity()<NewLine>        (relu2): QuantizedReLU()<NewLine>        (shortcut): Sequential(<NewLine>          (observer): FakeQuantize(<NewLine>            fake_quant_enabled=True, observer_enabled=True,            scale=tensor([0.0236]), zero_point=tensor([0])<NewLine>            (observer): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.006941795349121)<NewLine>          )<NewLine>        )<NewLine>      )<NewLine>    )<NewLine>    (linear): QuantizedLinear(in_features=64, out_features=10, scale=0.10857795923948288, zero_point=69)<NewLine>  )<NewLine>)<NewLine></code></pre><NewLine><p>It appears that some fakequantization nodes remain after converting.<br/><NewLine>shortcut is an empty <code>nn.Sequential()</code> layer (depending on the network arguments this gets overwritten to handle different strides)</p><NewLine><p>But it running the model results in an error:</p><NewLine><pre><code class=""lang-auto"">RuntimeError: Didn't find kernel to dispatch to for operator 'aten::sub'. Tried to look up kernel for dispatch key 'QuantizedCPUTensorId'. Registered dispatch keys are: [CUDATensorId, SparseCPUTensorId, VariableTensorId, CPUTensorId, SparseCUDATensorId]<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>Just found the related pytorch issue, so I assume its a known bug?<br/><NewLine><aside class=""onebox githubissue""><NewLine><header class=""source""><NewLine><a href=""https://github.com/pytorch/pytorch/issues/28375"" rel=""nofollow noopener"" target=""_blank"">github.com/pytorch/pytorch</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><div class=""github-row""><NewLine><div class=""github-icon-container"" title=""Issue""><NewLine><svg aria-hidden=""true"" class=""github-icon"" height=""60"" viewbox=""0 0 14 16"" width=""60""><path d=""M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z""></path></svg><NewLine></div><NewLine><div class=""github-info-container""><NewLine><h4><NewLine><a href=""https://github.com/pytorch/pytorch/issues/28375"" rel=""nofollow noopener"" target=""_blank"">torch.quantization.convert() doesn't remove observer from empty Sequential() module, causing error when scripting</a><NewLine></h4><NewLine><div class=""github-info""><NewLine><div class=""date""><NewLine>        opened <span class=""discourse-local-date"" data-date=""2019-10-21"" data-format=""ll"" data-time=""19:17:42"" data-timezone=""UTC"">07:17PM - 21 Oct 19 UTC</span><NewLine></div><NewLine><div class=""user""><NewLine><a href=""https://github.com/hx89"" rel=""nofollow noopener"" target=""_blank""><NewLine><img alt=""hx89"" class=""onebox-avatar-inline"" height=""20"" src=""https://avatars1.githubusercontent.com/u/43588773?v=4"" width=""20""/><NewLine>          hx89<NewLine>        </a><NewLine></div><NewLine></div><NewLine></div><NewLine></div><NewLine><div class=""github-row""><NewLine><p class=""github-content"">🐛 Bug<NewLine>torch.quantization.convert() doesn't remove observer from empty Sequential() module, causing errors when scripting the model since observers are not scriptable.<NewLine>Steps to...</p><NewLine></div><NewLine><div class=""labels""><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">jit</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">quantization</span><NewLine><span style=""display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;"">triaged</span><NewLine></div><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>The issue is that there is a dangling <code>Sequential</code> module. When seeing an empty <code>Sequential</code>, the logic is  (erroneously) inserting the observer in there, which causes it to have the error you are seeing. This is fixed in <a href=""https://github.com/pytorch/pytorch/pull/28384"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/pull/28384</a> – if you rebase to <code>master</code>, it will fix it. Alternatively, you can modify the model to drop the shortcuts.</p><NewLine><p>If you are still seeing the error after rebasing, please, let us know <img alt="":slight_smile:"" class=""emoji"" src=""https://discuss.pytorch.org/images/emoji/apple/slight_smile.png?v=9"" title="":slight_smile:""/></p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Mxbonn; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Mxbonn; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Zafar; <NewLine> ,"REPLY_DATE 1: October 24, 2019, 12:57am; <NewLine> REPLY_DATE 2: October 28, 2019,  1:22pm; <NewLine> REPLY_DATE 3: October 28, 2019,  1:25pm; <NewLine> REPLY_DATE 4: October 28, 2019,  5:58pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
58766,Where is the Dataset and model file,2019-10-21T02:06:03.529Z,1,217,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’m trying to run the codes in <a href=""https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#experimental-static-quantization-with-eager-mode-in-pytorch"" rel=""nofollow noopener"">(EXPERIMENTAL) STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH</a>, but there are no dataset and model files available, such as <strong>imagenet_1k, mobilenet_quantization.pth</strong> and so on.<br/><NewLine>So anyone can provide the address of the necessary files and dataset in this tutorial?</p><NewLine></div>",https://discuss.pytorch.org/u/Aspirinkb,(Frank),Aspirinkb,"October 21, 2019,  2:06am",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Also, static quantification of google colab is not available</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> can you take a look?</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/raghuramank100"">@raghuramank100</a> Need your help.</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Hi frank,  to run the tutorial, you will need to download imagenet dataset yourself, as we cannot upload imagenet images. The dataset can be obtained by using: <a href=""https://pytorch.org/docs/stable/_modules/torchvision/datasets/imagenet.html#ImageNet"" rel=""nofollow noopener"">https://pytorch.org/docs/stable/_modules/torchvision/datasets/imagenet.html#ImageNet</a></p><NewLine><p>For the tutorial, you only need the floating point models as the tutorial walks through the process of creating a quantized model from a floating point model. The floating point mobilenet model can be found at: <a href=""https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py#L9"" rel=""nofollow noopener"">https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py#L9</a>.</p><NewLine><p>We will add  more documentation to the tutorial so that these steps are clearer.</p><NewLine></div>; <NewLine> REPLY 5: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for your replay. I find the download urls are in the Makefile in github pytorch/tutorial.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/bintonto; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/jerryzh168; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/Aspirinkb; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/raghuramank100; <NewLine> REPLIER 5: https://discuss.pytorch.org/u/Aspirinkb; <NewLine> ,"REPLY_DATE 1: October 21, 2019,  1:52pm; <NewLine> REPLY_DATE 2: October 24, 2019,  1:03am; <NewLine> REPLY_DATE 3: October 25, 2019,  6:17am; <NewLine> REPLY_DATE 4: October 25, 2019,  9:39pm; <NewLine> REPLY_DATE 5: October 27, 2019,  4:54am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> REPLY 3 LIKES: ; <NewLine> REPLY 4 LIKES: ; <NewLine> REPLY 5 LIKES: ; <NewLine> 
59013,Quantized hard sigmoid,2019-10-23T09:54:09.784Z,0,743,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I’ve tried to implement hard sigmoid activation in a way suitable for quantization aware training:</p><NewLine><pre><code class=""lang-auto"">from torch import nn<NewLine><NewLine>class HardSigmoid(nn.Module):<NewLine>    def __init__(self):<NewLine>        super().__init__()<NewLine>        self.act = nn.ReLU6()<NewLine>        self.add = nn.quantized.FloatFunctional()<NewLine>        self.mul = nn.quantized.FloatFunctional()<NewLine>    <NewLine>    def forward(self, input):<NewLine>        # relu6(input + 3) / 6<NewLine>        output = self.add.add_scalar(input, 3)<NewLine>        output = self.act(output)<NewLine>        output = self.mul.mul_scalar(output, 1/6)<NewLine>        return output<NewLine></code></pre><NewLine><p>The backward pass and conversion works fine:</p><NewLine><pre><code class=""lang-auto"">import torch.quantization as tq<NewLine>from torch.nn.intrinsic import ConvBn2d<NewLine>import torch<NewLine><NewLine>model = nn.Sequential(<NewLine>    tq.QuantStub(), <NewLine>    ConvBn2d(<NewLine>        nn.Conv2d(3, 16, kernel_size=3, padding = 1, bias = False),<NewLine>        nn.BatchNorm2d(16)<NewLine>    ),<NewLine>    HardSigmoid(),<NewLine>    tq.DeQuantStub()<NewLine>)<NewLine><NewLine>optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)<NewLine>model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')<NewLine>torch.quantization.prepare_qat(model, inplace = True)<NewLine><NewLine>x = torch.rand(16, 3, 16, 16)<NewLine>y = model(x)<NewLine><NewLine>y.sum().backward()<NewLine>optimizer.step()<NewLine><NewLine>y = model(x)<NewLine><NewLine>model.apply(torch.quantization.disable_observer)<NewLine>model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)<NewLine><NewLine>torch.quantization.convert(model.eval(), inplace = True)<NewLine></code></pre><NewLine><p>But the forward pass (<code>model(x)</code>) fails with the following error:</p><NewLine><pre><code class=""lang-auto"">  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward<NewLine>    input = module(input)<NewLine>  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""&lt;stdin&gt;"", line 10, in forward<NewLine>  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__<NewLine>    result = self.forward(*input, **kwargs)<NewLine>  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/modules/activation.py"", line 209, in forward<NewLine>    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)<NewLine>  File ""/usr/local/lib64/python3.7/site-packages/torch/nn/functional.py"", line 960, in hardtanh<NewLine>    result = torch._C._nn.hardtanh(input, min_val, max_val)<NewLine>RuntimeError: Didn't find kernel to dispatch to for operator 'aten::hardtanh'. Tried to look up kernel for dispatch key 'QuantizedCPUTensorId'. Registered dispatch keys are: [CUDATensorId, CPUTensorId, VariableTensorId]<NewLine></code></pre><NewLine><p>What is the correct way to implement hard sigmoid activation?</p><NewLine></div>",https://discuss.pytorch.org/u/pshashk,,pshashk,"October 23, 2019,  9:54am",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><aside class=""quote no-group"" data-post=""1"" data-topic=""59013""><NewLine><div class=""title""><NewLine><div class=""quote-controls""></div><NewLine><img alt="""" class=""avatar"" height=""20"" src=""https://discuss.pytorch.org/letter_avatar_proxy/v4/letter/p/9d8465/40.png"" width=""20""/> pshashk:</div><NewLine><blockquote><NewLine><p>tq.DeQuantStub()</p><NewLine></blockquote><NewLine></aside><NewLine><p>We actually don’t have relu6 in the mapping right now: <a href=""https://github.com/pytorch/pytorch/blob/master/torch/quantization/default_mappings.py#L14"" rel=""nofollow noopener"">https://github.com/pytorch/pytorch/blob/master/torch/quantization/default_mappings.py#L14</a>. adding an entry here should fix the problem. Thanks for reporting.</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/jerryzh168; <NewLine> ,"REPLY_DATE 1: October 23, 2019,  4:56pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
