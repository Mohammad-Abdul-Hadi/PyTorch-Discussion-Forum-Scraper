id,title,created_at,reply_count,views,description,creator_link,creator_name,creator_alias,post_date,post_likes,replies,repliers_links,reply_dates,reply_likes
51871,About the hackathon category,2019-07-29T03:19:50.840Z,0,306,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Use this category to discuss ideas about the PyTorch Global and local Hackathons.</p><NewLine><p>The current ongoing hackathon is the Global SummerHack with a deadline of September 16th, 2019.</p><NewLine><p>Read more at <a href=""https://pytorch.devpost.com/"" rel=""nofollow noopener"">https://pytorch.devpost.com/</a></p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"August 10, 2019,  3:29pm",1 Like,,,,
91538,Tips to creating a sequential GAN image generator,2020-08-04T01:41:02.326Z,0,44,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I would like to train a GANs model that can generate 1 output image from a set of 3 images.</p><NewLine><ul><NewLine><li><NewLine><p>Do you have any tips on this?</p><NewLine></li><NewLine><li><NewLine><p>Will the GPU capacity needed to train this be really high? I just requested 1 P optimized instance on AWS to use a p2.xlarge instance. Will that be enough or do I need a p3.2xlarge ?</p><NewLine></li><NewLine></ul><NewLine></div>",https://discuss.pytorch.org/u/santi22,(Santiago Norena),santi22,"August 4, 2020,  1:41am",,,,,
55834,"Anyone working with a data pipeline of CPU -&gt; GPU? I am developing a library of methods for faster transfer to GPU. In some cases, 370x faster than used Pytorch&rsquo;s Pinned CPU Tensors",2019-09-12T22:53:33.371Z,1,323,"<div class=""post"" itemprop=""articleBody""><NewLine><p>Let me know what your pipeline is and I’ll try to add methods for it. Just show me your code.</p><NewLine><p>I am developing methods for fast transfer from CPU and GPU, and currently coding the methods for it. Show me your code (A Colab notebook would be really helpful) and I’ll see how to incorporate the library into it, for faster data transfer.</p><NewLine></div>",https://discuss.pytorch.org/u/Santosh-Gupta,(Santosh Gupta),Santosh-Gupta,"September 12, 2019, 10:53pm",,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/santosh-gupta"">@Santosh-Gupta</a> Hi, thanks for your work and proposal. Actually I am very interested on tools to speed up CPU-GPU-CPU data transfers. I have written the details of my application in <a href=""https://discuss.pytorch.org/t/cpu-x10-faster-than-gpu-recommendations-for-gpu-implementation-speed-up/54980"">this post</a>. There you will find details on what I tried and access to the code.</p><NewLine><p>Basically, im trying to do real-time control <span class=""mention"">@100hz</span> on constrained devices (Jetson Nano) using pytorch for computation of the control laws on GPU. I am sure there are several bottlenecks but one of them is the CPU-GPU and GPU-CPU data transfers.</p><NewLine><p>I am starting to rewrite the code to use torch JIT, but I am not quite sure about the speed up improvements. I will appreciate your support on trying out SpeedTorch (of which I read about on <a href=""https://discuss.pytorch.org/t/introducing-speedtorch-4x-speed-cpu-gpu-transfer-110x-gpu-cpu-transfer/56147"">your post</a>) and hopefully get some nice performance increase.</p><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p><a class=""mention"" href=""/u/juanmed"">@juanmed</a> it’s out</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/Santosh-Gupta/SpeedTorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/5524261?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/Santosh-Gupta/SpeedTorch"" rel=""nofollow noopener"" target=""_blank"">Santosh-Gupta/SpeedTorch</a></h3><NewLine><p>Library for fastest pinned CPU &lt;-&gt; GPU transfer . Contribute to Santosh-Gupta/SpeedTorch development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/juanmed; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/Santosh-Gupta; <NewLine> ,"REPLY_DATE 1: September 20, 2019,  4:03pm; <NewLine> REPLY_DATE 2: September 21, 2019,  1:24am; <NewLine> ",REPLY 1 LIKES: ; <NewLine> REPLY 2 LIKES: ; <NewLine> 
56147,"Introducing SpeedTorch: 4x speed CPU-&gt;GPU transfer, 110x GPU-&gt;CPU transfer",2019-09-17T12:20:43.105Z,2,2371,"<div class=""post"" itemprop=""articleBody""><NewLine><p> <a class=""onebox"" href=""https://i.imgur.com/wr4VaUV.png"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""500"" src=""https://i.imgur.com/wr4VaUV.png"" width=""589""/><NewLine></a><NewLine></p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/Santosh-Gupta/SpeedTorch"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/5524261?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/Santosh-Gupta/SpeedTorch"" rel=""nofollow noopener"" target=""_blank"">Santosh-Gupta/SpeedTorch</a></h3><NewLine><p>Library for fastest pinned CPU &lt;-&gt; GPU transfer . Contribute to Santosh-Gupta/SpeedTorch development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>This is my submission to the Pytorch Hackathon. It’s a library I made for Pytorch, for fast transfer between pinned CPU tensors and GPU pytorch variables. The inspiration came from needed to train large number of embeddings, which don’t all fit on GPU ram at a desired embedding size, so I needed a faster CPU &lt;-&gt; GPU transfer method. This also allows using any optimizer for sparse training, since every embedding contained in the Pytorch embedding variable receives an update, previously only Pytorch’s SGD, Adagrad, and SparseAdam were suitable for such training.</p><NewLine><p>In addition to augmenting parameter sizes, you can use to increase the speed of which data on your CPU is transferred to Pytorch Cuda variables.</p><NewLine><p>Also, SpeedTorch’s GPU tensors are also overall faster then Pytorch cuda tensors, when taking into account both transferring two and from (overall 2.6x faster). For just transfering to a Pytorch Cuda, Pytorch is still faster, but significantly slower when transfering from a Pytorch Cuda variable.</p><NewLine><p>I have personally used this to nearly double the embedding size of embeddings in two other projects, by holding half the parameters on CPU. The training speed is decent thanks to the fast CPU&lt;-&gt;GPU exchange.</p><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/Santosh-Gupta/Research2Vec2"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/5524261?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/Santosh-Gupta/Research2Vec2"" rel=""nofollow noopener"" target=""_blank"">Santosh-Gupta/Research2Vec2</a></h3><NewLine><p>Updated from the original Research2vec project. Contribute to Santosh-Gupta/Research2Vec2 development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><aside class=""onebox whitelistedgeneric""><NewLine><header class=""source""><NewLine><img class=""site-icon"" height=""16"" src=""https://github.githubassets.com/favicon.ico"" width=""16""/><NewLine><a href=""https://github.com/Santosh-Gupta/lit2vec2"" rel=""nofollow noopener"" target=""_blank"">GitHub</a><NewLine></header><NewLine><article class=""onebox-body""><NewLine><img class=""thumbnail onebox-avatar"" height=""60"" src=""https://avatars2.githubusercontent.com/u/5524261?s=400&amp;v=4"" width=""60""/><NewLine><h3><a href=""https://github.com/Santosh-Gupta/lit2vec2"" rel=""nofollow noopener"" target=""_blank"">Santosh-Gupta/Lit2Vec2</a></h3><NewLine><p>Contribute to Santosh-Gupta/Lit2Vec2 development by creating an account on GitHub.</p><NewLine></article><NewLine><div class=""onebox-metadata""><NewLine></div><NewLine><div style=""clear: both""></div><NewLine></aside><NewLine><p>There’s a bit of a learning curve for the very first time getting started with it, so as soon as you run into any sort of friction, feel free to ask a question on the project gitter</p><NewLine><p><a class=""onebox"" href=""https://gitter.im/SpeedTorch"" rel=""nofollow noopener"" target=""_blank"">https://gitter.im/SpeedTorch</a></p><NewLine><p>And I’ll answer them.</p><NewLine><p> <a class=""onebox"" href=""https://i.imgur.com/6o8C1BP.gif"" rel=""nofollow noopener"" target=""_blank""><NewLine><img height=""498"" src=""https://i.imgur.com/6o8C1BP.gif"" width=""498""/><NewLine></a><NewLine></p><NewLine></div>",https://discuss.pytorch.org/u/Santosh-Gupta,(Santosh Gupta),Santosh-Gupta,"September 17, 2019, 12:20pm",2 Likes,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine><p>Interesting work. This can speed-up operations like:</p><NewLine><pre><code class=""lang-auto"">cpu_tensor[idx] = gpu_tensor.cpu()<NewLine></code></pre><NewLine><p>and</p><NewLine><pre><code class=""lang-auto"">gpu_tensor = cpu_tensor[idx].cuda()<NewLine></code></pre><NewLine><p>The library does this by masquerading the pinned-memory CPU tensor as a cupy GPU tensor and using the cupy GPU indexing kernels. It’s not really speeding up the CPU-GPU copy part, it’s avoiding the overhead of a separate CPU indexing operation. (The GitHub page mentions “memmaps”, but it’s not using memory mapping at all).</p><NewLine><p>For CPU &lt;-&gt; GPU copies, the relative speed-up depends on how slow the PyTorch CPU indexing operation is. For the (131072, 128) size, this can be as fast as 2 ms on a multi-core machine or ~25 ms on collab notebook with only one or two virtual CPUs.  For comparison, the CPU-GPU copy from pinned memory is consistent at ~5-6 ms. (Again, it’s not the CPU-GPU copy that’s sped up; it’s the separate CPU indexing operation that’s avoided)</p><NewLine><p>There’s also a significant indexing performance bug in PyTorch 1.1 and 1.2 (fixed in the nightly builds) that makes the indexing much slower. The apparent GPU &lt;-&gt; GPU indexing speed-ups are entirely due to this bug. You could either use the nightly builds or use <code>index_select</code> / <code>index_copy_</code> instead of a[idx] notation in 1.1/1.2 to avoid that slow down in vanilla PyTorch.  (Bug is described in <a href=""https://github.com/pytorch/pytorch/pull/24083"" rel=""nofollow noopener"">#24083</a> for reference)</p><NewLine><p>The library is missing some synchronization. Particularly, when copying from GPU to pinned memory (masquerading as GPU via cupy), you need to synchronize before accessing the CPU data; otherwise it may not be consistent.</p><NewLine><p>There’s a few bugs in the benchmark code, mostly minor:</p><NewLine><ol><NewLine><li><NewLine><p><code>sampl = np.random.uniform(low=-1.0, high=1.0, size=(1000000, 128))</code>. This is float64; it should be float32 to match everything else:<br/><NewLine><code>sampl = np.random.uniform(low=-1.0, high=1.0, size=(1000000, 128)).astype(np.float32)</code></p><NewLine></li><NewLine><li><NewLine><p>The timing code should have a torch.cuda.synchronize() at the end. Instead of:</p><NewLine></li><NewLine></ol><NewLine><pre><code class=""lang-auto"">torch.cuda.synchronize()<NewLine>cupy.cuda.Device().synchronize()<NewLine><NewLine>runningTime=0<NewLine>for i in range(numSamples):<NewLine>    start = time.time()<NewLine><NewLine>    with torch.no_grad():<NewLine><NewLine>        gadgetCPU.insertData(u_embeddings.weight.data, indexess )<NewLine><NewLine>    end = time.time()<NewLine>    runningTime = runningTime + end - start<NewLine><NewLine>print('set corpus. cupy live pinned')<NewLine>print(runningTime/numSamples)<NewLine></code></pre><NewLine><p>It should be written as:</p><NewLine><pre><code class=""lang-auto"">torch.cuda.synchronize()<NewLine>cupy.cuda.Device().synchronize()<NewLine><NewLine>start = time.time()<NewLine>for i in range(numSamples):<NewLine>    with torch.no_grad():<NewLine>        gadgetCPU.insertData(u_embeddings.weight.data, indexess )<NewLine><NewLine>torch.cuda.synchronize()<NewLine>end = time.time()<NewLine>runningTime = end - start<NewLine><NewLine>print('set corpus. cupy live pinned')<NewLine>print(runningTime/numSamples)<NewLine></code></pre><NewLine></div>; <NewLine> REPLY 2: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the detailed analysis.<br/><NewLine>I had a quick look at the implementations and besides the missing synchronizations, I’m still trying to figure out, if the CUPY arrays can hold any CPU data, or if it’s restricted to the device only?<br/><NewLine>E.g. while this line of code</p><NewLine><pre><code class=""lang-python"">gadgetCPU.insertData(u_embeddings.weight.data,  indexess )<NewLine></code></pre><NewLine><p>seems to copy the CUDATensor from <code>u_embeddings</code> onto the CPU, it seems as if the underlying data never left the GPU:</p><NewLine><pre><code class=""lang-python"">gadgetCPU.CUPYcorpus.device<NewLine>Out[4]: &lt;CUDA Device 0&gt;<NewLine></code></pre><NewLine><p>I’m not really experienced in CUPY so I might miss something.</p><NewLine></div>; <NewLine> REPLY 3: <div class=""post"" itemprop=""articleBody""><NewLine><p>As far as I can tell, CuPy is only intended to hold CUDA data, but in this case it’s actually holding CPU data (pinned memory). You can check with something like:</p><NewLine><pre><code class=""lang-auto"">cupy.cuda.runtime.pointerGetAttributes(gadgetCPU.CUPYcorpus.data.ptr).memoryType<NewLine></code></pre><NewLine><p>This will print 1 (= cudaMemoryTypeHost). On gadgetGPU it’ll print 2 (=cudaMemoryTypeDevice).  (<a href=""https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g13de56a8fe75569530ecc3a3106e9b6d"" rel=""nofollow noopener"">cudaMemoryType  reference</a>)</p><NewLine><p>You can do something similar in PyTorch  from the C++ API using torch::from_blob. Here’s an <a href=""https://gist.github.com/colesbury/11b3308539f5162b178cc229a2aac9c5"" rel=""nofollow noopener"">example</a>. Note there’s a <a href=""https://github.com/pytorch/pytorch/blob/caed485873a750fa698c402e7d64f9b1c386cca9/aten/src/ATen/templates/Functions.h#L32-L38"" rel=""nofollow noopener"">check</a> in from_blob that tries to prevent this sort of thing, but the check is broken (what luck!).</p><NewLine></div>; <NewLine> REPLY 4: <div class=""post"" itemprop=""articleBody""><NewLine><p>Thanks for the detailed analysis! I was very curious about what was going on.</p><NewLine><blockquote><NewLine><p>The GitHub page mentions “memmaps”, but it’s not using memory mapping at all</p><NewLine></blockquote><NewLine><p>Ahhh that’s a vestige from my initial approach to using memmaps, I overlooked changing that, it’s fixed now.</p><NewLine><p>I updated the benchmarking code, and I’ll link this analysis to the ‘how it works’ section.</p><NewLine><p>I’ll also include a section for when to use SpeedTorch.</p><NewLine><blockquote><NewLine><p>you need to synchronize before accessing the CPU data; otherwise it may not be consistent.</p><NewLine></blockquote><NewLine><p>So adding <code>torch.cuda.synchronize()</code> and <code>cupy.cuda.Device().synchronize()</code> before the transfer? By consistency, do you mean in terms transfer times?</p><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/colesbury; <NewLine> REPLIER 2: https://discuss.pytorch.org/u/ptrblck; <NewLine> REPLIER 3: https://discuss.pytorch.org/u/colesbury; <NewLine> REPLIER 4: https://discuss.pytorch.org/u/Santosh-Gupta; <NewLine> ,"REPLY_DATE 1: September 19, 2019,  6:51pm; <NewLine> REPLY_DATE 2: September 19, 2019, 10:27pm; <NewLine> REPLY_DATE 3: September 20, 2019, 12:30am; <NewLine> REPLY_DATE 4: September 21, 2019,  1:06am; <NewLine> ",REPLY 1 LIKES: 2 Likes; <NewLine> REPLY 2 LIKES: 1 Like; <NewLine> REPLY 3 LIKES: 1 Like; <NewLine> REPLY 4 LIKES: ; <NewLine> 
55483,Any tips in creating libraries/frameworks which are compatible with both pytorch and numpy arrays?,2019-09-08T23:31:48.816Z,0,132,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I am creating a pytorch library. I am trying to make it as friendly as possible for inputs in both pytorch tensors and numpy arrays.</p><NewLine><p>If there are are there any particular methods and tips that are best for this, I would love to hear them!</p><NewLine></div>",https://discuss.pytorch.org/u/Santosh-Gupta,(Santosh Gupta),Santosh-Gupta,"September 8, 2019, 11:31pm",,,,,
53142,"Need teammate for Pytorch Hackathon, creating Sparse Library",2019-08-12T06:33:19.026Z,0,228,"<div class=""post"" itemprop=""articleBody""><NewLine><p>I have an idea for a sparse training library for pytorch.</p><NewLine><p>A lot of the work will involve speed/performance optimization, particularly parallelizing IO memmap operations so that it doesn’t slow down training.</p><NewLine><p>The library will then be used to train embeddings for ~40 million research papers.</p><NewLine><p>If you haven’t heard of the Facebook Pytorch Hackathon, Google ‘Facebook Pytorch Hackathon’</p><NewLine></div>",https://discuss.pytorch.org/u/Santosh-Gupta,(Santosh Gupta),Santosh-Gupta,"August 12, 2019,  6:33am",,,,,
53071,PyTorch Global SummerHack 2019 is Open,2019-08-10T15:34:04.798Z,0,579,"<div class=""post"" itemprop=""articleBody""><NewLine><p>The Global PyTorch Summer Hackathon is here!</p><NewLine><p>Calling developers and researchers to hack together packages, demos and applications on top of PyTorch and submit online.</p><NewLine><p>We’ll help you reach the community, amplify your vision and have $60K in prizes!</p><NewLine><p>Read more at <a href=""https://pytorch.devpost.com"" rel=""nofollow noopener"">https://pytorch.devpost.com</a></p><NewLine></div>",https://discuss.pytorch.org/u/smth,,smth,"August 10, 2019,  3:34pm",1 Like,"REPLY 1: <div class=""post"" itemprop=""articleBody""><NewLine></div>; <NewLine> ",REPLIER 1: https://discuss.pytorch.org/u/smth; <NewLine> ,"REPLY_DATE 1: October 24, 2019,  9:27pm; <NewLine> ",REPLY 1 LIKES: ; <NewLine> 
