{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV9siJvBvoDQ"
      },
      "source": [
        "!pip install pyqt5\n",
        "!pip install webdriver_manager\n",
        "!pip install PyQtWebEngine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFs7vulsydQ"
      },
      "source": [
        "import math, shutil, sys, os, csv, time, pprint, json, csv, requests\n",
        "import urllib.request\n",
        "import pandas as pd  \n",
        "from PyQt5.QtWidgets import QApplication\n",
        "from PyQt5.QtCore import QUrl\n",
        "from PyQt5.QtWebEngineWidgets import QWebEnginePage\n",
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmI6XN4GVr58"
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxa8IyP1s7-U"
      },
      "source": [
        "def get_keys(dict): \n",
        "    list = [] \n",
        "    for key in dict.keys(): \n",
        "        list.append(key)       \n",
        "    return list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdK1kntEYFS8"
      },
      "source": [
        "def create_list(d, keys):\n",
        "    keys_d = get_keys(d)\n",
        "\n",
        "    if set(keys_d) == set(keys):\n",
        "        return list(d.values())\n",
        "    else:\n",
        "        dictionary = {}\n",
        "        for key in keys:\n",
        "            dictionary[key] = d[key] if key in d else None\n",
        "        return list(dictionary.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXatBwmgwUCF"
      },
      "source": [
        "categories = {'uncategorized': [1, 21654], 'vision': [5,5825], 'autograd': [7,3036], 'nlp': [8,1282], 'c': [11,1073], 'distributed': [12,560], 'jit': [13,374], 'reinforcement-learning': [6,333], 'deployment': [14,227], 'quantization': [17,203], 'mobile': [18,124], 'audio': [9,109], 'glow': [10,68], 'site-feedback': [3,64], 'ignite': [15,34], 'memory-format': [23,23], 'mixed-precision': [27,15], 'complex': [22,14], 'windows': [26,13], 'captum': [21,10], 'xla': [25,7], 'hackathon': [16,6], 'jobs': [24,2], 'tensorboard': [28,0]}\n",
        "# cats = get_keys(categories)\n",
        "# categories[cats[0]]\n",
        "# categories['tensorboard'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLMHVF8kwmzN"
      },
      "source": [
        "# FETCH TITLES AND META-DATA\n",
        "cats = get_keys(categories)\n",
        "topics_per_page = 30\n",
        "\n",
        "for cat in cats:\n",
        "    topics_info = []\n",
        "    keys = []\n",
        "    num_pages = categories[cat][1]/topics_per_page\n",
        "    for i in range(0, math.ceil(num_pages)): \n",
        "        time.sleep(0.5)\n",
        "        my_url = 'https://discuss.pytorch.org/c/' + cat + '/' + str(categories[cat][0]) + '.json?page='+ str(i)\n",
        "        # my_url\n",
        "\n",
        "        uClient = uReq(my_url)\n",
        "        page_html = uClient.read()\n",
        "        uClient.close()\n",
        "        print(my_url+' fetched...')\n",
        "\n",
        "        encoding = 'utf-8'\n",
        "        filename = '/'+cat+'/'+cat+'_'+str(i)+'.json'\n",
        "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "        f = open(filename, \"w\")\n",
        "        f.write(page_html.decode(encoding))\n",
        "        f.close()\n",
        "        print(filename+ ' file created')\n",
        "\n",
        "        with open(filename) as json_file: \n",
        "            lst = []    \n",
        "            dict = json.load(json_file) \n",
        "            # str(dict['users'])\n",
        "            # str(dict['topic_list'])\n",
        "            topic_list = dict['topic_list']\n",
        "            topics = topic_list['topics'] # list of dictionary\n",
        "            print('topics have been fetched from '+ filename)\n",
        "\n",
        "            if i == 0:\n",
        "                keys = get_keys(topics[0])\n",
        "            \n",
        "            for d in topics:\n",
        "                row = create_list(d, keys)\n",
        "                topics_info.append(row)\n",
        "            print('Values/Rows added for ' + str(i) + '-th page')\n",
        "\n",
        "    # print(topics_info)\n",
        "    df = pd.DataFrame(topics_info)\n",
        "    df.columns = keys\n",
        "    csv_data = df.to_csv(index=False)\n",
        "    df.to_csv('topics_info_'+cat+'.csv', index=False)\n",
        "    print('CSV file created. End of '+cat+ ' category\\n\\n')\n",
        "\n",
        "    time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hn8B2hNRGvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85c3b15-1b47-41c0-a923-1352e33550c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa6fuiFVSd5k"
      },
      "source": [
        "for cat in cats:\n",
        "    shutil.move(\"/content/topics_info_\"+cat+\".csv\", \"/content/drive/My Drive/AREA_51.2/Research/PyTorch Discussion Forum Data/Titles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvFiIKQzfgv-"
      },
      "source": [
        "# Fetch Description if all titles\n",
        "\n",
        "keys = ['id', 'title', 'created_at', 'reply_count', 'views', 'description', 'creator_link', 'creator_name', 'creator_alias', 'post_date', 'post_likes', 'replies', 'repliers_links', 'reply_dates', 'reply_likes']\n",
        "cats = get_keys(categories) # commented for debugging \n",
        "# cats = ['site-feedback'] # added for debugging \n",
        "\n",
        "for cat in cats:\n",
        "    csv_list = []\n",
        "    filename = \"/content/drive/My Drive/AREA_51.2/Research/PyTorch Discussion Forum Data/Titles/topics_info_\"+cat+\".csv\"\n",
        "    if categories[cat][1] != 0:\n",
        "        data = pd.read_csv(filename)\n",
        "        for i in range(0, data.shape[0]):\n",
        "            time.sleep(0.5)\n",
        "            temp_lst = []\n",
        "            topic_url = \"https://discuss.pytorch.org/t/\"+data.at[i,'slug']+\"/\"+str(data.at[i,'id'])\n",
        "\n",
        "            while True:\n",
        "                try:\n",
        "                    uClient = uReq(topic_url)\n",
        "                    page_html = uClient.read()\n",
        "                    if uClient.getcode() == 200:\n",
        "                        uClient.close()\n",
        "                        break\n",
        "                except Exception as inst:\n",
        "                    print(inst)\n",
        "            \n",
        "            page_soup = soup(page_html, \"html.parser\")\n",
        "            \n",
        "            temp_lst.append(data.at[i,'id'])\n",
        "            temp_lst.append(data.at[i,'fancy_title'])\n",
        "            temp_lst.append(data.at[i,'created_at'])\n",
        "            temp_lst.append(data.at[i,'reply_count'])\n",
        "            temp_lst.append(data.at[i,'views'])\n",
        "\n",
        "            all_posts = page_soup.findAll(\"div\",{\"class\":\"post\"}) # commented for debugging\n",
        "            temp_lst.append(str(all_posts[0]).replace(\"\\n\", \"<NewLine>\")) # description # commented for debugging\n",
        "\n",
        "            all_creators = page_soup.findAll(\"span\",{\"class\":\"creator\"})\n",
        "            temp_lst.append(all_creators[0].a['href']) # creator_link\n",
        "            temp_lst.append(\" \".join(all_creators[0].text.split()[1:])) # creator_name\n",
        "            temp_lst.append(all_creators[0].span.text) # creator_alias\n",
        "\n",
        "            all_reply_dates = page_soup.findAll(\"span\",{\"class\":\"crawler-post-infos\"})\n",
        "            temp_lst.append(all_reply_dates[0].time.text.strip()) # post_date\n",
        "\n",
        "            all_reply_likes = page_soup.findAll(\"span\",{\"class\":\"post-likes\"})\n",
        "            temp_lst.append(all_reply_likes[0].text) # post_like\n",
        "\n",
        "            replies = \"\"\n",
        "            for p in range(1, len(all_posts)):\n",
        "                replies += \"REPLY \"+str(p)+\": \"+ str(all_posts[p]).replace(\"\\n\", \"<NewLine>\")+\"; <NewLine> \"\n",
        "            temp_lst.append(replies)\n",
        "\n",
        "            repliers_links = \"\"\n",
        "            for c in range(1, len(all_creators)):\n",
        "                repliers_links += \"REPLIER \"+str(c)+\": \"+all_creators[c].a['href']+\"; <NewLine> \"\n",
        "            temp_lst.append(repliers_links)\n",
        "\n",
        "            reply_dates = \"\"\n",
        "            for d in range(1, len(all_reply_dates)):\n",
        "                reply_dates += \"REPLY_DATE \"+str(d)+\": \"+all_reply_dates[d].time.text.strip()+\"; <NewLine> \"\n",
        "            temp_lst.append(reply_dates)\n",
        "\n",
        "            reply_likes = \"\"\n",
        "            for l in range(1, len(all_reply_likes)):\n",
        "                reply_likes += \"REPLY \"+str(l)+\" LIKES: \"+all_reply_likes[l].text+\"; <NewLine> \"\n",
        "            temp_lst.append(reply_likes)        \n",
        "            \n",
        "            # print(description, \"\\n\\n\", replies, \"\\n\\n\", creator_link, creator_alias, creator_name, \"\\n\\n\", repliers_links, \"\\n\\n\", post_date, \"\\n\\n\", reply_dates, \"\\n\\n\", post_like, \"\\n\\n\", reply_likes)\n",
        "            \n",
        "            csv_list.append(temp_lst)\n",
        "            if i%50 == 0:\n",
        "                print(\"Topic Details for # \"+str(i)+\" from Category \"+cat+\" has been added to csv_list\\t\"+str(data.shape[0]-i)+\" number of topics left\\t Estimated Remaining time for this category: \"+str(int(((data.shape[0]-i)*0.6)/60)) + \" minutes\")\n",
        "    \n",
        "    if categories[cat][1] != 0:\n",
        "        df = pd.DataFrame(csv_list)\n",
        "        df.columns = keys\n",
        "        df.to_csv('topics_details_'+cat+'.csv', index=False)\n",
        "        print('\\nCSV file created. End of '+cat+ ' category\\n\\n')\n",
        "    else:\n",
        "        print('No Topic exists in '+cat+' category')\n",
        "    time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohm94bX_ll5n"
      },
      "source": [
        "# Run this in the console to prevent Google Colab from disconnecting\n",
        "function ClickConnect(){\n",
        "    console.log(\"Working\"); \n",
        "    document.querySelector(\"colab-toolbar-button\").click() \n",
        "}setInterval(ClickConnect,3600000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_d907rRZO-"
      },
      "source": [
        "### User Meta Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yuOvWNGTo99",
        "outputId": "05962c19-399f-4f76-9284-c380f0abc222"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcYUOh1jRfOt"
      },
      "source": [
        "for i in range(0, 711): # as of 23rd Nov 2020, there are 35520 Users in Pytorch Discussion forum\n",
        "    time.sleep(0.5)\n",
        "    my_url = 'https://discuss.pytorch.org/directory_items.json?order=likes_received&page='+str(i)+'&period=all'\n",
        "    # my_url\n",
        "\n",
        "    uClient = uReq(my_url)\n",
        "    page_html = uClient.read()\n",
        "    uClient.close()\n",
        "    print(my_url+' fetched...')\n",
        "\n",
        "    encoding = 'utf-8'\n",
        "    with open('/content/drive/MyDrive/Colab Data/PyTorch User Metadata/User Metadata Page '+str(i)+'.json', 'w') as f: \n",
        "        f.write(page_html.decode(encoding))\n",
        "    print('Page '+str(i)+'.json'+' file created')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmPbAwRMScJV"
      },
      "source": [
        "lst = []\n",
        "dic = {}\n",
        "for i in range(0, 711):\n",
        "    filename = '/content/drive/MyDrive/Colab Data/PyTorch User Metadata/User Metadata Page '+str(i)+'.json'\n",
        "\n",
        "    with open(filename) as json_file:\n",
        "        dict = json.load(json_file) \n",
        "        # print(str(dict['directory_items']))\n",
        "        # print(str(dict['meta']))\n",
        "        lst = lst + dict['directory_items']\n",
        "        print('Dictionary Items from page '+str(i)+' has been integrated into <lst>')\n",
        "        # dic.update(dict['meta'])\n",
        "        # print('Meta from page '+str(i)+' has been integrated into <dic>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iblkfqPkjOQZ"
      },
      "source": [
        "for i in range(0, len(lst)): # len(lst) = 35520, lst[0 was handled in test]\n",
        "    tmp_dict = lst[i]\n",
        "    tmp_dict['user_avatar_template'] = lst[i]['user']['avatar_template']\n",
        "    tmp_dict['user_id'] = lst[i]['user']['id']\n",
        "    tmp_dict['user_name'] = lst[i]['user']['name']\n",
        "    tmp_dict['user_title'] = lst[i]['user']['title']\n",
        "    tmp_dict['user_name'] = lst[i]['user']['username']\n",
        "    del tmp_dict['user']\n",
        "\n",
        "df = pd.DataFrame(lst)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxV9x_TWcZMa"
      },
      "source": [
        "csv_data = df.to_csv(index=False)\n",
        "df.to_csv('/content/drive/MyDrive/Colab Data/PyTorch User Metadata/ALL USER METADATA.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmUJUXneobCY"
      },
      "source": [
        "### Collecting Summary and Badges for all Users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ8qktilCoyP",
        "outputId": "c0d2299e-c1f9-49d3-df66-e190c32a9ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seen = set()\n",
        "new_lst = []\n",
        "for d in lst:\n",
        "    t = tuple(d.items())\n",
        "    if t not in seen:\n",
        "        seen.add(t)\n",
        "        new_lst.append(d)\n",
        "\n",
        "print(len(new_lst))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGurcQnsogxw"
      },
      "source": [
        "count = 0\n",
        "for i in range(0, len(lst)):\n",
        "    my_url = 'https://discuss.pytorch.org/u/'+str(lst[i]['user_name'])+'/summary.json'\n",
        "    try:\n",
        "        uClient = uReq(my_url)\n",
        "        page_html = uClient.read()\n",
        "        if uClient.getcode() == 200:\n",
        "            uClient.close()\n",
        "    except Exception as inst:\n",
        "        print(inst, str(lst[i]['user_name'])+'/summary.json Not Found!!!')\n",
        "        continue\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Data/PyTorch User Metadata/User Summaries/'+str(lst[i]['user_name'])+'.json', 'w') as f: \n",
        "        f.write(page_html.decode('utf-8'))\n",
        "    if i % 355 == 0:\n",
        "        print(str(count)+'% file created')\n",
        "        count = count + 1\n",
        "\n",
        "# dict = json.loads(page_html.decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeshX0qx-KWb"
      },
      "source": [
        "for i in range(0, len(new_lst)):\n",
        "    old_dict = new_lst[i]\n",
        "    print(i)\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/Colab Data/PyTorch User Metadata/User Summaries/'+ str(old_dict['user_name']) + '.json') as json_file:\n",
        "            dict = json.load(json_file)\n",
        "            if 'likes_given' in dict['user_summary']:\n",
        "                old_dict['likes_given'] = dict['user_summary']['likes_given']\n",
        "            else:\n",
        "                old_dict['likes_given'] = 0\n",
        "            if 'likes_received' in dict['user_summary']:\n",
        "                old_dict['likes_received'] = dict['user_summary']['likes_received']\n",
        "            else:\n",
        "                old_dict['likes_received'] = 0\n",
        "            if 'topics_entered' in dict['user_summary']:\n",
        "                old_dict['topics_entered'] = dict['user_summary']['topics_entered']\n",
        "            else:\n",
        "                old_dict['topics_entered'] = 0\n",
        "            if 'posts_read_count' in dict['user_summary']:\n",
        "                old_dict['posts_read_count'] = dict['user_summary']['posts_read_count']\n",
        "            else:\n",
        "                old_dict['posts_read_count'] = 0\n",
        "            if 'days_visited' in dict['user_summary']:\n",
        "                old_dict['days_visited'] = dict['user_summary']['days_visited']\n",
        "            else:\n",
        "                old_dict['days_visited'] = 0\n",
        "            if 'topic_count' in dict['user_summary']:\n",
        "                old_dict['topic_count'] = dict['user_summary']['topic_count']\n",
        "            else:\n",
        "                old_dict['topic_count'] = 0\n",
        "            if 'post_count' in dict['user_summary']:\n",
        "                old_dict['post_count'] = dict['user_summary']['post_count']\n",
        "            else:\n",
        "                old_dict['post_count'] = 0\n",
        "            if 'time_read' in dict['user_summary']:\n",
        "                old_dict['time_read'] = dict['user_summary']['time_read']\n",
        "            else:\n",
        "                old_dict['time_read'] = 0\n",
        "            if 'recent_time_read' in dict['user_summary']:\n",
        "                old_dict['recent_time_read'] = dict['user_summary']['recent_time_read']\n",
        "            else:\n",
        "                old_dict['recent_time_read'] = 0\n",
        "            if 'can_see_summary_stats' in dict['user_summary']:\n",
        "                old_dict['can_see_summary_stats'] = dict['user_summary']['can_see_summary_stats']\n",
        "            else:\n",
        "                old_dict['likes_given'] = 0\n",
        "            if 'solved_count' in dict['user_summary']:\n",
        "                old_dict['solved_count'] = dict['user_summary']['solved_count']\n",
        "            else:\n",
        "                old_dict['can_see_summary_stats'] = 'NAN'\n",
        "            if 'topic_ids' in dict['user_summary']:\n",
        "                old_dict['topic_ids'] = str(dict['user_summary']['topic_ids'])\n",
        "            else:\n",
        "                old_dict['topic_ids'] = ''\n",
        "            if 'most_liked_users' in dict['user_summary']:\n",
        "                usernames = ''\n",
        "                for j in range(0, len(dict['user_summary']['most_liked_users'])):\n",
        "                    usernames = usernames + dict['user_summary']['most_liked_users'][j]['username'] + '; '\n",
        "                old_dict['most_liked_users'] = usernames.strip()\n",
        "            else:\n",
        "                old_dict['most_liked_users'] = ''\n",
        "            if 'most_liked_by_users' in dict['user_summary']:\n",
        "                usernames = ''\n",
        "                for j in range(0, len(dict['user_summary']['most_liked_by_users'])):\n",
        "                    usernames = usernames + dict['user_summary']['most_liked_by_users'][j]['username'] + '; '\n",
        "                old_dict['most_liked_by_users'] = usernames.strip()\n",
        "            else:\n",
        "                old_dict['most_liked_by_users'] = ''\n",
        "\n",
        "            if 'badge_types' in dict and len(dict['badge_types']) > 0:\n",
        "                badge_types = ''\n",
        "                for j in range(0, len(dict['badge_types'])):\n",
        "                    badge_types = badge_types + dict['badge_types'][j]['name'] + '; '\n",
        "                old_dict['badge_types'] = badge_types.strip()\n",
        "            else:\n",
        "                old_dict['badge_types'] = ''\n",
        "                \n",
        "            if 'badges' in dict and len(dict['badges']) > 0:\n",
        "                old_dict['total_badges'] = len(dict['badges'])\n",
        "                badges = ''\n",
        "                for j in range(0, len(dict['badges'])):\n",
        "                    badges = badges + dict['badges'][j]['name'] + ' (Description: ' + dict['badges'][j]['description'] + '; Grant Count: ' + str(dict['badges'][j]['grant_count']) + '); '\n",
        "                old_dict['badges'] = badges.strip()\n",
        "            else:\n",
        "                old_dict['total_badges'] = 0\n",
        "                old_dict['badges'] = ''\n",
        "                \n",
        "            if 'topics' in dict and len(dict['topics']) > 0:\n",
        "                old_dict['total_topics'] = len(dict['topics'])\n",
        "                topics = ''\n",
        "                for j in range(0, len(dict['topics'])):\n",
        "                    topics = topics + str(dict['topics'][j]['id']) + '; '\n",
        "                old_dict['topic_ids'] = topics.strip()\n",
        "            else:\n",
        "                old_dict['total_topics'] = 0\n",
        "                old_dict['topic_ids'] = ''\n",
        "    except Exception as inst:\n",
        "        print(inst, str(old_dict['user_name']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfYlDmUHehOS"
      },
      "source": [
        "df = pd.DataFrame(new_lst)\n",
        "csv_data = df.to_csv(index=False)\n",
        "df.to_csv('/content/drive/MyDrive/Colab Data/PyTorch User Metadata/USERDATA-FINAL.csv', index=False)"
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}